{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RsX6trIHR_-8"
   },
   "source": [
    "# Using a library for topic modeling with Latent Dirichlet Allocation\n",
    "\n",
    "This self-study notebook shows how to use a machine learning library to train a Latent Dirichlet Allocation model for a small corpus and then inspect what the model has learned. As you recall from the description of this type of model, there are two things we will get from an LDA model:\n",
    "- a set of **topics**: groups of words relating to a common topic;\n",
    "- for a given document: a **topic distribution** that shows how we analyze this document in terms of topics.\n",
    "\n",
    "**PLEASE NOTE** that what you will do in Assignment 2 is different. In the assignment, your goal is to implement the learning algorithm on your own. In this notebook, we are just going to use a library.\n",
    "\n",
    "There is also an important technical difference: in the assignment, you are going to use an [Gibbs sampling](https://en.wikipedia.org/wiki/Gibbs_sampling) to train the model, while the library uses a completely different approach called [variational inference](https://en.wikipedia.org/wiki/Variational_Bayesian_methods). To make a long story very short, the key differences between these two approaches to training Bayesian models are:\n",
    "- **MCMC/Gibbs sampling** is based on repeated sampling. It generally gives higher-quality estimates but is more time-consuming.\n",
    "- **Variational inference** optimizes a quantity that approximates the model likelihood. It is typically faster but the estimates may be less reliable.\n",
    "\n",
    "### Libraries that include an LDA implementation\n",
    "\n",
    "You can find LDA implementation in many libraries. Some of the most well-known are:\n",
    "\n",
    "- [scikit-learn](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.LatentDirichletAllocation.html): uses variational inference.\n",
    "- [gensim](https://radimrehurek.com/gensim/): also variational inference.\n",
    "- [mallet](http://mallet.cs.umass.edu/topics.php): uses Gibbs sampling.\n",
    "\n",
    "In this notebook, we are going to use the implementation from scikit-learn, for the simple reason that most of you probably already have this library installed. (This library as well as gensim are also pre-installed on Colab.) Its implementation uses training algorithm described in [this paper](https://papers.nips.cc/paper/2010/hash/71f6278d140af599e06ad9bf1ba03cb0-Abstract.html).\n",
    "\n",
    "**Acknowledgement**: this notebook is based on [the following example](https://scikit-learn.org/stable/auto_examples/applications/plot_topics_extraction_with_nmf_lda.html) from the scikit-learn documentation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T9y6gqs-R__D"
   },
   "source": [
    "### Reading the documents\n",
    "\n",
    "The zip file [here](http://www.cse.chalmers.se/~richajo/dat450/data/dredze_amazon_reviews.zip) contains a processed version of a corpus originally described in the paper [*Biographies, Bollywood, Boom-boxes and Blenders: Domain Adaptation for Sentiment Classification*](https://aclanthology.org/P07-1056.pdf) by Blitzer et al. (2007). (You may have encountered the same dataset in other courses Richard taught...)\n",
    "\n",
    "The corpus consists of about 12,000 product reviews scraped from Amazon. The reviews are selected from six product categories (books, cameras, DVDs, music, health and software). The reviews are also tagged with sentiment polarity values (positive or negative). Each line in the file consists of one review. For instance, in the following line\n",
    "```\n",
    "music neg 544.txt i was misled and thought i was buying the entire cd and it contains one song \n",
    "```\n",
    "we have a music review with negative sentiment polarity. The third column is an ID we won't consider, and the rest of the line contains the words in the review text. The text has already been tokenized and lowercased.\n",
    "\n",
    "We first read the corpus. For the notebook, we are actually just going to use the text, but we also store the product type labels which may become useful when we look at the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bPGpk50NSKod"
   },
   "outputs": [],
   "source": [
    "!wget http://www.cse.chalmers.se/~richajo/dat450/data/dredze_amazon_reviews.zip\n",
    "!unzip dredze_amazon_reviews.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BQQKQa8nR__E"
   },
   "outputs": [],
   "source": [
    "docs = []\n",
    "prod_lbls = []\n",
    "sent_lbls = []\n",
    "\n",
    "with open('dredze_amazon_reviews.txt') as f:\n",
    "    for l in f:\n",
    "        prod, sent, _, doc = l.strip().split(maxsplit=3)\n",
    "        docs.append(doc)\n",
    "        prod_lbls.append(prod)\n",
    "        sent_lbls.append(sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1pnfN2Z-R__H"
   },
   "source": [
    "To take one example, here is a book review with positive polarity:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DAbDeCtJR__I"
   },
   "outputs": [],
   "source": [
    "doc_index = 3\n",
    "docs[doc_index], prod_lbls[doc_index], sent_lbls[doc_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aWtid024R__K"
   },
   "source": [
    "### Preprocessing documents\n",
    "\n",
    "We convert the documents into a [document-term matrix](https://en.wikipedia.org/wiki/Document-term_matrix) (that is: a matrix contain word frequencies for each document) by applying the [`CountVectorizer`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) from scikit-learn. We exclude [stop words](https://en.wikipedia.org/wiki/Stop_word): general grammatical words that we think will not be useful when finding the topics. (It is common practice when using LDA to remove the stop words, but there are [some arguments](https://aclanthology.org/E17-2069/) about whether this is useful or not.) We include the 1,000 most frequent words in the matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0aWDNBu8R__L"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "tf_vectorizer = CountVectorizer(\n",
    "    max_features=1000, stop_words=\"english\"\n",
    ")\n",
    "doc_matrix = tf_vectorizer.fit_transform(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fDFO1jcMR__M"
   },
   "source": [
    "This matrix has 11,914 rows and 1,000 columns; make sure you understand why."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fz3JX66qR__M"
   },
   "outputs": [],
   "source": [
    "doc_matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rnQQ7TC1R__O"
   },
   "source": [
    "### Training an LDA model\n",
    "\n",
    "When we have the matrix representing the word frequencies in each document, it is a simple matter of applying scikit-learn's LDA implementation. Here is an example of how this can be done. The model should train in a few seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dl51nDPSR__O"
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from time import time\n",
    "\n",
    "print(\"Training the LDA model...\")\n",
    "lda = LatentDirichletAllocation(\n",
    "    n_components=10,           # number of topics\n",
    "    learning_method=\"online\",  # faster training by using the \"online\" algorithm\n",
    "    max_iter=5,                # number of epochs\n",
    "    random_state=0,            # random seed for reproducibility\n",
    "    n_jobs=-1,                 # uses all available CPU cores\n",
    "    verbose=1                  # shows some output during training\n",
    ")\n",
    "t0 = time()\n",
    "lda.fit(doc_matrix)\n",
    "print(f\"Trained in {time()-t0:.3f} seconds.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cmx2D2tgR__Q"
   },
   "source": [
    "As you see, there are some different hyperparameters we can play with.\n",
    "\n",
    "- `n_components`: Probably, the most interesting hyperparameter is the number of topics. In practice, the rule of thumb is that the more fine-grained model we want to train, the more training data we need. So it's probably a good idea to use a rather small set of topics for this small set of documents.\n",
    "- `learning_method`: When we use the `online` algorithm, we apply the method described in [this paper](https://papers.nips.cc/paper/2010/hash/71f6278d140af599e06ad9bf1ba03cb0-Abstract.html). The key idea is similar to stochastic gradient descent: we update the model in small increments, after processing a small number of documents. The alternative is `batch`: in this case, we update the model after processing all documents. `batch` might be mathematically somewhat more stable but `online` typically requires fewer iterations.\n",
    "- `max_iter`: the maximal number of iterations used in the training algorithm. \n",
    "- `random_state`: the random seed. If you change this, your topics will probably be different.\n",
    "\n",
    "### Showing the learned topics\n",
    "\n",
    "Now, let's see what the model has learned. We define a function that will let us visualize the topics. For each topic, it will draw a bar for the subset of words with the highest occurrence probabilities. (This code has been adapted from [this example](https://papers.nips.cc/paper/2010/hash/71f6278d140af599e06ad9bf1ba03cb0-Abstract.html).)\n",
    "\n",
    "The function needs a trained LDA model, a list of words in the vocabulary, and the number of words to show for each topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kLo55Hu0R__Q"
   },
   "outputs": [],
   "source": [
    "from math import ceil\n",
    "import matplotlib.pyplot as plt\n",
    "%config InlineBackend.figure_format = 'svg' \n",
    "plt.style.use('seaborn')\n",
    "\n",
    "def plot_top_words(model, feature_names, n_top_words):\n",
    "    n_topics = len(model.components_)\n",
    "    n_cols = 5\n",
    "    n_rows = ceil(n_topics/n_cols)\n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(10, 5), sharex=True)\n",
    "    axes = axes.flatten()\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        top_features_ind = topic.argsort()[: -n_top_words - 1 : -1]\n",
    "        top_features = [feature_names[i] for i in top_features_ind]\n",
    "        weights = topic[top_features_ind]\n",
    "\n",
    "        ax = axes[topic_idx]\n",
    "        ax.barh(top_features, weights, height=0.7)\n",
    "        ax.set_title(f\"Topic {topic_idx +1}\", fontdict={\"fontsize\": 14})\n",
    "        ax.invert_yaxis()\n",
    "        ax.tick_params(axis=\"both\", which=\"major\", labelsize=10)\n",
    "        for i in \"top right left\".split():\n",
    "            ax.spines[i].set_visible(False)\n",
    "\n",
    "    plt.subplots_adjust(top=0.90, bottom=0.05, wspace=0.90, hspace=0.3)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4o6utJL7R__R"
   },
   "source": [
    "Here is the result of applying the visualization function. We get the vocabulary from the `CountVectorizer` we used to build the word count matrix.\n",
    "\n",
    "**Reflection**: how do the topics relate to the content of our corpus? Can you interpret the topics? Do you think we have found everything there is to find?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wmaeAmyJR__R"
   },
   "outputs": [],
   "source": [
    "plot_top_words(lda, tf_vectorizer.get_feature_names(), 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0Vs8hqAnR__S"
   },
   "source": [
    "We also define a function that simply prints the top words in each topic as text. This might be more useful than the visualization if you are working with an LDA model with a large number of topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "huHu4ukZR__S"
   },
   "outputs": [],
   "source": [
    "def print_top_words(model, feature_names, n_top_words):\n",
    "    for i, topic in enumerate(model.components_, 1):\n",
    "        top_words = sorted(zip(topic, feature_names), reverse=True)[:n_top_words]\n",
    "        print(f'{i:2d}', ' '.join(w for _, w in top_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "npyDczg7R__T"
   },
   "source": [
    "### Predicting the topics for a new document\n",
    "\n",
    "If you have a new document, you can use the trained model to estimate the topic proportions for it. This is done in two steps: first, the document is converted into a matrix, and then the inference is carried out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fjWBnvvxR__T"
   },
   "outputs": [],
   "source": [
    "test_doc = 'this book describes windows software'\n",
    "#test_doc = 'i love my new mac , it is wonderful !'\n",
    "#test_doc = 'i hate my new razor , it is horrible !'\n",
    "\n",
    "# build a word frequency matrix for this document\n",
    "test_doc_encoded = tf_vectorizer.transform([test_doc])\n",
    "\n",
    "# apply the LDA model to get a topic distribution for this document\n",
    "# the result is a vector of predicted topic proportions\n",
    "test_doc_topics = lda.transform(test_doc_encoded)[0]\n",
    "\n",
    "# now, let's draw a bar plot\n",
    "names = [f'T{i+1}' for i in range(len(test_doc_topics))]\n",
    "plt.bar(names, test_doc_topics);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nFyUf6CiR__U"
   },
   "source": [
    "The result shows predicted topic distribution. In most cases, there will be one or more dominant topics, and small probabilities for the rest of the topics.\n",
    "\n",
    "For instance, for the document `this book describes Windows software`, we will typically get a result that this document is a mix of a book-related topic and a software-related topic. (Compare to the topic list you got above.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6mZ2-EAGR__U"
   },
   "source": [
    "### Reflection\n",
    "\n",
    "- Do you see a problem from a **scientific point of view** with the approach you just used? Is it a good approach scientifically to let the model output something and then \"read the tea leaves\" and try to come up with a reason for why you got this result?\n",
    "- What do you think of the effect of **randomness** on the results? (That is, what happens if you adjust `random_state`.)\n",
    "- How do you think we should **evaluate** a topic model? What does it mean for topics to be good, or the topic assignment for a document to be good? As for all unsupervised learning, topic modeling evaluation is a tricky subject; [this paper](https://papers.nips.cc/paper/2009/hash/f92586a25bb3145facd64ab20fd554ff-Abstract.html) may be a good starting point.\n",
    "\n",
    "### Optional example: topic modeling in software engineering\n",
    "\n",
    "[This file](http://www.cse.chalmers.se/~richajo/dat450/data/mozilla_bugs.zip) contains a set of 25,000 bug reports from the Mozilla projects (involving e.g. the Firefox browser and Thunderbird email client etc). These bug reports were collected as part of a research project by [Leif Jonsson](https://github.com/lejon) of Linköping University and Ericsson. See [his paper](https://ieeexplore.ieee.org/document/7589822) for more details about the data.\n",
    "\n",
    "Train an LDA model with a suitable number of topics and reflect about the results. What uses do you see for LDA models in software engineering?"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Self-study notebook W2 Topic models.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
