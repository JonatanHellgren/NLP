{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_qbrFXDUg-Mv"
   },
   "source": [
    "# Self-study notebook for week 1: Introduction to PyTorch\n",
    "\n",
    "This notebook shows how we can build simple models for text classification using the [PyTorch](https://pytorch.org) library. We first show an example using a \"standard\" PyTorch solution, and then show how to do the same thing with [PyTorch Lightning](https://pytorch-lightning.readthedocs.io), which provides some high-level functionality to simplify some chores.\n",
    "\n",
    "The main purpose here is for you to see the basic structure of PyTorch-based solutions. You will be able to reuse this basic design pattern in later parts of the course. In terms of neural network architectures, we will be keeping it to the basics at this point. Later, we will see several types of NN architectures for working with text that you can \"plug\" into this solution.\n",
    "\n",
    "We will *not* cover the very basics of how PyTorch works. There are plenty of tutorials available, for instance [on the PyTorch website](https://pytorch.org/tutorials/). There is also a notebook available [here](http://www.cse.chalmers.se/~richajo/dit866/backup_2019/lectures/l5/PyTorch%20linear%20regression%20demo.ipynb)  (or [here](http://www.cse.chalmers.se/~richajo/dit866/backup_2019/lectures/l5/PyTorch%20linear%20regression%20demo.html) in html), developed for another Chalmers course, that covers the most important fundamental PyTorch concepts such as tensors and optimizers.\n",
    "\n",
    "Throughout the code, there are some pieces marked `YOUR_CODE_HERE` where the implementation is missing. The idea is that you should fill in these parts. We are keeping it simple at this point and it should be stressed that in almost all cases, a single line of code should be enough – so if you are writing several lines, your solution is likely to be overly complicated.\n",
    "\n",
    "### Prerequisites\n",
    "\n",
    "To run this notebook, you will need to have PyTorch installed on the machine where you are running, as well as the Scikit-learn library for some basic preprocessing. See [here](https://pytorch.org/get-started/locally/) for PyTorch installation instructions and [here](https://scikit-learn.org/stable/install.html) for Scikit-learn. Generally, we will not be using scikit-learn much in this course, except for practical utilities such as training/test dataset splitting, but in this introductory example this library will simplify our work a bit.\n",
    "\n",
    "If you are using Colab, these libraries are already installed. (If you want to run the PyTorch Lightning part, you will have to install this library as well, and this is not available by default on Colab. See instructions below.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-qy74ECug-M4"
   },
   "source": [
    "# Case study: language classification\n",
    "\n",
    "The example we are going to consider is the task of *language classification*: given a written text, try to determine what language the text is written in.\n",
    "\n",
    "Download [this zip file](http://www.cse.chalmers.se/~richajo/dat450/selfstudy_notebooks/eulangs.zip) and uncompress it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7aedztiyiDV6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2021-11-02 16:49:36--  http://www.cse.chalmers.se/~richajo/dat450/selfstudy_notebooks/eulangs.zip\n",
      "Resolving www.cse.chalmers.se (www.cse.chalmers.se)... 129.16.221.33\n",
      "Connecting to www.cse.chalmers.se (www.cse.chalmers.se)|129.16.221.33|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 8009713 (7,6M) [application/zip]\n",
      "Saving to: ‘eulangs.zip’\n",
      "\n",
      "eulangs.zip          12%[=>                  ]   1005K  64,4KB/s    eta 82s    "
     ]
    }
   ],
   "source": [
    "!wget http://www.cse.chalmers.se/~richajo/dat450/selfstudy_notebooks/eulangs.zip\n",
    "!unzip eulangs.zip\n",
    "!ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "im0385u0g-M6"
   },
   "source": [
    "This file contains 100,000 short texts from European Parliament proceedings, written in 22 of the languages of the European Union. (The EU has [24 official languages](https://en.wikipedia.org/wiki/Languages_of_the_European_Union), but this dataset does not include any texts in Irish or Croatian.) This collection has been extracted from the [Europarl](https://www.statmt.org/europarl/) dataset, often used for the development of machine translation systems.\n",
    "\n",
    "Here is an example of how the file is structured:\n",
    "\n",
    "```\n",
    "fr:Intitulé: Primes brutes émises au titre de l'assurance directe, primes de contrat ...\n",
    "da:For at der skal kunne ydes tilskud efter artikel 108 i finansforordningen, skal der ...\n",
    "el:Μέλος της Επιτροπής\n",
    "```\n",
    "\n",
    "In the file, each row contains an example. Each row starts with a two-letter language code in the [ISO 639-1](https://en.wikipedia.org/wiki/ISO_639-1) format, and then the text follows after a colon separator.\n",
    "\n",
    "**Common-sense sanity test**: which languages are probably going to be the easiest to recognize?\n",
    "\n",
    "**Your work:** read the dataset and store it as a list of documents `X` and a list of labels `Y`. (This task is an exception to what we wrote above that `YOUR_CODE_HERE` should be replaced by a single line of code. You might be able to solve this with a one-liner, but the code will probably be more readable if you use more lines.) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fzHR6WS2g-M9"
   },
   "outputs": [],
   "source": [
    "def read_texts(corpus_file):\n",
    "    X = []\n",
    "    Y = []\n",
    "    YOUR_CODE_HERE\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HPbT9WVLg-M_"
   },
   "outputs": [],
   "source": [
    "X, Y = read_texts('eulangs_100k.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B6L8cxQjg-NA"
   },
   "source": [
    "We then use a utility in the scikit-learn library to randomly split the dataset into training and test sections. (Later on, we will use the same utility to set aside a validation part.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i5_wSCmNg-NB"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "Xtrain, Xtest, Ytrain, Ytest = train_test_split(X, Y, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rXx75A3Bg-NC"
   },
   "source": [
    "You can now inspect some of the instances. For instance, the first document in the training set is in [Estonian](https://en.wikipedia.org/wiki/Estonian_language)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y5lpx4fmg-NC"
   },
   "outputs": [],
   "source": [
    "Xtrain[0], Ytrain[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V-ncfMX5g-NE"
   },
   "source": [
    "... and the first document in the test set is in English."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Nd0Z9Y7Eg-NF"
   },
   "outputs": [],
   "source": [
    "Xtest[0], Ytest[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QKmyAiv_g-NG"
   },
   "source": [
    "# Representing documents\n",
    "\n",
    "The idea of *representation* is arguably the most crucial concept of this course. The idea is that the object that we are working with (in this case a text) should be represented in a way so that its useful information is \"obvious\" to the machine learning system. Concretely, this typically means that each document will be stored as a vector in some high-dimensional vector space, and in a good representation for a classification task (in our case, determining the language of the text), the different classes will be located in different regions of this space.\n",
    "\n",
    "In the course, we will consider many different approaches to representing words and documents. Generally, these representations will be *learned* from data.\n",
    "\n",
    "### Using scikit-learn to build a count-based document representation\n",
    "\n",
    "However, right now we are going to keep things simple and use a straightforward *count-based* representation. You have probably seen this approach used in previous machine learning courses.\n",
    "\n",
    "We define a vector space where each dimension represents how many times we observed a certain character. For instance, given the text *kreuzschlitzschraubenzieher*, one of the dimensions will represent the character `z` and its value will be 3 in this case.\n",
    "\n",
    "The scikit-learn library provides some useful utilities to compute such count-based representations. The [CountVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) will give us the desired result. We will also apply a [Normalizer](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.Normalizer.html) that normalizes each example to unit norm (that is, its Euclidean length is 1). The reason is that we want the representation to be stable even if there is much variation in the length of the documents. Finally, we also import [make_pipeline](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.make_pipeline.html), which is simply a utility to combine several steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L2eu9vplg-NH"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "document_representation = make_pipeline(CountVectorizer(analyzer='char'), Normalizer())\n",
    "document_representation.fit(Xtrain, Ytrain)\n",
    "Xtrain_encoded = document_representation.transform(Xtrain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IUcTDXECg-NI"
   },
   "source": [
    "**Self-check:** The shape of this output is 80000 x 252. What does this mean? Make sure you understand what the rows and columns represent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YZQ4RCpfg-NI"
   },
   "outputs": [],
   "source": [
    "Xtrain_encoded.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "igzcHogHg-NJ"
   },
   "source": [
    "The output of a CountVectorizer is a [*sparse* matrix](https://en.wikipedia.org/wiki/Sparse_matrix#Storing_a_sparse_matrix). That is, only nonzero elements are stored explicitly, together with their positions. For instance, if we print the representation of the first document, we get"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0hzGeiFkg-NJ"
   },
   "outputs": [],
   "source": [
    "print(Xtrain_encoded[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p9lKy0-0g-NK"
   },
   "source": [
    "This sparse format does not work well with neural network libraries such as PyTorch or TensorFlow, so we need to convert it into a straightforward NumPy matrix that also stores the zeroes. This is done simply by calling `.toarray()` on the sparse matrix.\n",
    "\n",
    "There is another technicality we need to keep in mind here. The matrix we have here uses [double-precision](https://en.wikipedia.org/wiki/Double-precision_floating-point_format) floating point numbers (each number uses 64 bits). However, it is common practice when working with neural networks to use a lower precision: 32 or even 16 bits. To convert into [single precision](https://en.wikipedia.org/wiki/Single-precision_floating-point_format) (32 bits), we can call `.astype(np.float32)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vBLMDYDVg-NL"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "Xtrain_matrix = Xtrain_encoded.toarray().astype(np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BDDKI-s0g-NL"
   },
   "source": [
    "Repeat the preprocessing steps for the test set as well. ***Note:*** Don't call `fit` once again! We want the training and test sets to live in the same vector space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LwIxoexMg-NL"
   },
   "outputs": [],
   "source": [
    "YOUR_CODE_HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "luqbSfPIg-NN"
   },
   "source": [
    "### Encoding the class labels\n",
    "\n",
    "Now that we have taken care of the *input* part of the dataset (the documents), let's deal with the *output* (the language codes). This is a bit simpler. We just have to map each language code (such as `en` for English) to an integer.\n",
    "\n",
    "Again, we will use a utility from scikit-learn here. A [LabelEncoder](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html) creates a mapping between class labels and their corresponding integer indices. As we saw for the document encoding above, we first need to `fit` the label encoder and then call `transform` to get the output for the training and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ayIoC942g-NN"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "lblenc = LabelEncoder()\n",
    "lblenc.fit(Ytrain)\n",
    "Ytrain_enc = lblenc.transform(Ytrain)\n",
    "Ytest_enc = lblenc.transform(Ytest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9jLZjW52g-NO"
   },
   "source": [
    "# Building a text classifier in PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wp00TKc_g-NO"
   },
   "source": [
    "Let's first import the PyTorch library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N3htPCkKg-NO"
   },
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dnqzq2__g-NP"
   },
   "source": [
    "We can check whether a GPU is available. The models we are going to work with in this example are so simple that a GPU won't make much of a difference, but in future examples and assignments, using a GPU will give us a massive speed improvement.\n",
    "\n",
    "If you are working on Colab and you see `False` here, you have probably forgotten to set your runtime type to GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ngf8U-drg-NP"
   },
   "outputs": [],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T-7geZw8g-NP"
   },
   "source": [
    "Before we implement the text classifier, we will define an object that will store different types of information that defines the classifier's behavior, including [hyperparameters](https://en.wikipedia.org/wiki/Hyperparameter_%28machine_learning%29) controlling the model and the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y4wTM2xPg-NP"
   },
   "outputs": [],
   "source": [
    "class ClassifierParameters:\n",
    "    \"\"\"Container class to store the hyperparameters that control the training process.\"\"\"\n",
    "\n",
    "    # Proportion of data set aside for validation.\n",
    "    val_size = 0.2\n",
    "    \n",
    "    # Computation device: 'cuda' or 'cpu'\n",
    "    device = 'cuda'\n",
    "\n",
    "    # Number of hidden units in the neural network.\n",
    "    n_hidden_units = 64\n",
    "    \n",
    "    # Number of training epochs.\n",
    "    n_epochs = 25\n",
    "\n",
    "    # Size of batches: how many documents to process in parallel.\n",
    "    batch_size = 512\n",
    "\n",
    "    # Learning rate in the optimizer.\n",
    "    learning_rate = 5e-3\n",
    "\n",
    "    # Weight decay (L2 regularization) in the optimizer (if necessary).\n",
    "    decay = 0\n",
    "    \n",
    "    # Dropout probability (if necessary).\n",
    "    dropout = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VwXxF0WUg-NQ"
   },
   "source": [
    "### Implementing the classifier\n",
    "\n",
    "Now, let's do the heavy lifting!\n",
    "\n",
    "We create a class `NNClassifier` that contains everything we need to train the model and run it on new examples. The centerpiece is the training loop, but there are also some steps of initialization, etc. Most of the structure is in place, but there are some crucial pieces missing that you will have to provide.\n",
    "\n",
    "Note that this code has been written with a modular design so that it is agnostic about what kind of neural network model we are using. The user will have to provide a [factory function](https://en.wikipedia.org/wiki/Factory_%28object-oriented_programming%29) that creates the neural network. We will define this later.\n",
    "\n",
    "#### Preprocessing steps: `preprocess`\n",
    "\n",
    "In the preprocessing steps, we will typically do various types of encoding of the datasets we are using. In this case, we have already preprocessed the datasets and stored them as NumPy matrices, but in later examples and assignments, this step will typically also do vocabulary management, etc.\n",
    "\n",
    "The other thing we will do here is to prepare the *data loaders*. You might have read about datasets and data loaders in a [tutorial](https://pytorch.org/tutorials/beginner/basics/data_tutorial.html), or you can read the official documentation [here](https://pytorch.org/docs/stable/data.html). To summarize, the main purpose of a data loader is to go through a dataset and create [*minibatches*](https://machinelearningmastery.com/gentle-introduction-mini-batch-gradient-descent-configure-batch-size/) (or more commonly, just \"batches\").\n",
    "\n",
    "To create the batches, a data loader needs a *dataset*. There can be different types of datasets, some of which might be backed by a file system or a database. However, to make a long story short, for our purposes a dataset is simply an object that can be indexed like a regular list to give an input/output pair for training or validation.\n",
    "\n",
    "**Your work**: \n",
    "- Set up two data loaders: one for the training set and one for the validation set.\n",
    "\n",
    "#### Main training/validation loop: `fit`\n",
    "\n",
    "This is the main method of our text classifier and it will create a new neural network model and train it. We first carry out some initial steps:\n",
    "- Call `preprocess` discussed above.\n",
    "- Create the neural network model, using the factory function.\n",
    "- Move the model onto the GPU (if you are using a GPU).\n",
    "- Create a [*loss function*](https://en.wikipedia.org/wiki/Loss_functions_for_classification); for a classification task, this will often be the [cross-entropy loss](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html).\n",
    "- Create an *optimizer*. In PyTorch, an optimizer updates a model, depending on the gradients of the model's weights with respect to the loss. Typical optimizers include [SGD](https://pytorch.org/docs/stable/generated/torch.optim.SGD.html) and [Adam](https://pytorch.org/docs/stable/generated/torch.optim.Adam.html).\n",
    "\n",
    "We then enter the training loop, which will be run for a number of *epochs* (iterations). In each epoch, we will call the utility function `epoch`, discussed below. This will be done separately for the training set and validation set.\n",
    "\n",
    "**Your work**: \n",
    "- Move the model to the appropriate device, \n",
    "- Create a loss function. \n",
    "- Create an optimizer.\n",
    "\n",
    "#### Running the model for one training epoch: `epoch`\n",
    "\n",
    "The `epoch` method is where we see most of the machinery. Here, we will go through the batches in the training or validation set, compute the model outputs and the loss function. We will also keep track of some statistics: the overall loss and the prediction accuracy. If we are processing the training set (that is: not the validation set), we will also update the model here after each batch.\n",
    "\n",
    "**Your work**:\n",
    "- Compute the outputs from the model.\n",
    "- Compute the loss. \n",
    "- If we are training: compute the weight gradients and then update the model.\n",
    "\n",
    "#### Making predictions for a new dataset: `predict`\n",
    "\n",
    "After a model has been trained, we can apply it to new instances. This code will go through the new data in a similar fashion as we did in `epoch`, except that there are no gold-standard outputs Y here. In the end, we return all the predicted outputs.\n",
    "\n",
    "**Your work**:\n",
    "- Create a data loader for the new instances. Keep in mind that there is no Y part here.\n",
    "- For a batch, move the tensor to the GPU if necessary.\n",
    "- Compute the model output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0Xo3zjOvg-NQ"
   },
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "\n",
    "class NNClassifier:\n",
    "    \"\"\"A classifier based on a neural network.\"\"\"\n",
    "    \n",
    "    def __init__(self, params, model_factory):\n",
    "        self.params = params\n",
    "        self.model_factory = model_factory\n",
    "        \n",
    "    def preprocess(self, X, Y):\n",
    "        \"\"\"Carry out the document preprocessing, then build `DataLoader`s for the \n",
    "           training and validation sets.\"\"\"\n",
    "\n",
    "        # Split X and Y into training and validation sets. We\n",
    "        # apply the utility function we used previously.\n",
    "        Xtrain, Xval, Ytrain, Yval = train_test_split(X, Y, test_size=self.params.val_size, random_state=0)        \n",
    "        \n",
    "        # As discussed above, a dataset simply needs to behave like a list: it should\n",
    "        # be aware of its length and be able to index:\n",
    "        # dataset[position] should give an x,y pair (input and output).\n",
    "        # This means that we can simply use lists here!\n",
    "        train_dataset = list(zip(Xtrain, Ytrain))\n",
    "        val_dataset = list(zip(Xval, Yval))\n",
    "        \n",
    "        # Now, create the data loaders. The user parameters specify the batch size.\n",
    "        self.train_loader = YOUR_CODE_HERE\n",
    "        self.val_loader = YOUR_CODE_HERE\n",
    "        \n",
    "        \n",
    "    def fit(self, X, Y):\n",
    "        \"\"\"Train the model. We assume that a dataset and a model have already been provided.\"\"\"\n",
    "        par = self.params\n",
    "        \n",
    "        self.preprocess(X, Y)\n",
    "\n",
    "        self.input_size = X.shape[1]\n",
    "        self.n_classes = len(set(Y))        \n",
    "        \n",
    "        # Create a new model using the previously provided factory.\n",
    "        # The assumption is that this is a PyTorch neural network that\n",
    "        # takes inputs and makes outputs of the right sizes.\n",
    "        self.model = self.model_factory(self)\n",
    "        \n",
    "        # If we're using a GPU, put the model there.\n",
    "        YOUR_CODE_HERE\n",
    "    \n",
    "        # Declare a loss function, in this case the cross-entropy.\n",
    "        self.loss_func = YOUR_CODE_HERE\n",
    "\n",
    "        # An optimizer for updating the neural network. We use the Adam optimizer.\n",
    "        optimizer = YOUR_CODE_HERE\n",
    "\n",
    "        # We'll log the loss and accuracy scores encountered during training.\n",
    "        self.history = defaultdict(list)\n",
    "        \n",
    "        # Use the tqdm library to get a progress bar. The progress bar object can be used like\n",
    "        # any generator-like object.\n",
    "        progress = tqdm(range(par.n_epochs), 'Epochs')\n",
    "        \n",
    "        # Go through the dataset for a given number of epochs.\n",
    "        for epoch in progress:\n",
    "\n",
    "            t0 = time.time()\n",
    "            \n",
    "            # Set the model in training mode. This affects some components that \n",
    "            # behave differently at training and evaluation time, such as dropout\n",
    "            # and various types of normalization (e.g. batch normalization). It is good\n",
    "            # practice to include this even if you don't use any dropout or normalization.\n",
    "            self.model.train()\n",
    "            \n",
    "            # Run the model on the training data. The model will be updated after each batch.\n",
    "            # See below for the implementation of `epoch`.\n",
    "            train_loss, train_acc = self.epoch(self.train_loader, optimizer)\n",
    "                        \n",
    "            # Set the model in evaluation mode, again affecting dropout and normalization modules.\n",
    "            self.model.eval()\n",
    "\n",
    "            # Run the model on the validation data. For somewhat improved efficiency, we disable \n",
    "            # gradient computation now since we are not going to update the model.\n",
    "            with torch.no_grad():\n",
    "                val_loss, val_acc = self.epoch(self.val_loader)\n",
    "            \n",
    "            t1 = time.time()\n",
    "\n",
    "            # Store some evaluation metrics in the history object.\n",
    "            self.history['train_loss'].append(train_loss)\n",
    "            self.history['train_acc'].append(train_acc)\n",
    "            self.history['val_loss'].append(val_loss)\n",
    "            self.history['val_acc'].append(val_acc)\n",
    "            self.history['time'].append(t1-t0)\n",
    "            \n",
    "            # Show validation-set metrics on the progress bar.\n",
    "            progress.set_postfix({'val_loss': f'{val_loss:.2f}', 'val_acc': f'{val_acc:.2f}'})\n",
    "        \n",
    "        \n",
    "    def epoch(self, batches, optimizer=None):\n",
    "        \"\"\"Runs the neural network for one epoch, using the given batches.\n",
    "        If an optimizer is provided, this is training data and we will update the model\n",
    "        after each batch. Otherwise, this is assumed to be validation data.\n",
    "        \n",
    "        Returns the loss and accuracy over the epoch.\"\"\"\n",
    "        n_correct = 0\n",
    "        n_instances = 0\n",
    "        total_loss = 0\n",
    "        \n",
    "        # We iterate through the batches (typically from a data loader).\n",
    "        # This will give us X, Y pairs, containing the input and output parts\n",
    "        # of this batch, respectively.\n",
    "        for Xbatch, Ybatch in batches:\n",
    "\n",
    "            # Xbatch is a tensor of shape (batch_size, input_size).\n",
    "            # Ybatch has the shape (batch_size).\n",
    "                        \n",
    "            # If we're using the GPU, move the batch there.\n",
    "            Xbatch = Xbatch.to(self.params.device)\n",
    "            Ybatch = Ybatch.to(self.params.device)\n",
    "            \n",
    "            # Compute the predictions for this batch.\n",
    "            scores = YOUR_CODE_HERE\n",
    "            \n",
    "            # If the previous step was implemented correctly, your scores\n",
    "            # tensor should have the shape (batch_size, n_classes).\n",
    "            \n",
    "            # Compute the loss for this batch. Note: various loss functions \n",
    "            # behave differently, depending on whether they aggregate or not \n",
    "            # (that is, by summing or averaging). \n",
    "            # In the end, the loss value needs to be a single number so\n",
    "            # that we can compute gradients and update our model later.\n",
    "            loss = YOUR_CODE_HERE\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # Compute the number of correct predictions, for the accuracy.\n",
    "            guesses = scores.argmax(dim=1)\n",
    "            n_correct += (guesses == Ybatch).sum().item()\n",
    "            n_instances += Ybatch.shape[0]\n",
    "\n",
    "            # If this is training data, update the model.\n",
    "            if optimizer:\n",
    "                # Reset the gradients.\n",
    "                YOUR_CODE_HERE\n",
    "                # Run the backprop algorithm to compute the new gradients.\n",
    "                YOUR_CODE_HERE\n",
    "                # Update the model based on the gradients.\n",
    "                YOUR_CODE_HERE\n",
    "           \n",
    "        return total_loss/len(batches), n_correct/n_instances\n",
    "        \n",
    "        \n",
    "    def predict(self, X):\n",
    "        \"\"\"Run a trained classifier on a set of instances and return the predictions.\"\"\"\n",
    "        \n",
    "        # Build a DataLoader to generate the batches, as above except that now we don't have Y.\n",
    "        loader = YOUR_CODE_HERE\n",
    "\n",
    "        # Apply the model to all the batches and aggregate the predictions.\n",
    "        self.model.eval()\n",
    "        outputs = []\n",
    "        with torch.no_grad():\n",
    "            for Xbatch in loader:\n",
    "                # Move the batch onto the GPU if we are using one.\n",
    "                YOUR_CODE_HERE\n",
    "                \n",
    "                # Compute the output scores.\n",
    "                scores = YOUR_CODE_HERE\n",
    "                # scores should have the shape (batch_size, n_classes).\n",
    "                \n",
    "                # For each row, find the position of the highest score. This represents\n",
    "                # the model's guess for this instance.\n",
    "                # The output will have the shape (batch_size).\n",
    "                guesses = scores.argmax(dim=1)\n",
    "\n",
    "                # Move the result back onto the CPU and convert into a NumPy array,\n",
    "                # and keep the result for later.\n",
    "                outputs.append(guesses.cpu().numpy())\n",
    "                \n",
    "            # Finally, concatenate all output arrays.\n",
    "            return np.hstack(outputs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m3lRYHuTg-NR"
   },
   "source": [
    "In the implementation above, we left the definition of the neural network architecture unspecified. Now, let's write the factory function that creates a NN model.\n",
    "\n",
    "This can be any kind of PyTorch neural network: you just need to make sure that you use the right input and output sizes. The input size is equal to the number of columns in the input matrix: on our case, the number of different characters we have observed. The output size is equal to the number of classes: the number of languages.\n",
    "\n",
    "Probably, the simplest option to start with is a single linear layer ([`nn.Linear`](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html)). Then you can try something more complex: the most natural solution will then probably be a [`nn.Sequential`](https://pytorch.org/docs/stable/generated/torch.nn.Sequential.html), which will allow you to stack several layers on top of each other.\n",
    "\n",
    "Note that our feature representation is rather primitive, and it might be the case that a complex deep model will not give better results than a linear model. The point is simply to understand how things fit together, so it can still be useful to play around with a variety of models.\n",
    "\n",
    "If you need anything from the user-specified parameters, you can access them via `clf.param`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "grxDkWfRg-NR"
   },
   "outputs": [],
   "source": [
    "def make_model(clf):\n",
    "    input_size = clf.input_size\n",
    "    output_size = clf.n_classes\n",
    "    model = YOUR_CODE_HERE\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aQxQ5qsdg-NS"
   },
   "source": [
    "### Training the classifier\n",
    "Now, we have all the pieces we need and can finally train the text classifier!\n",
    "\n",
    "Note that we first call `torch.random.manual_seed` with a fixed number. This is to initialize random number generators and the reason we do this is for reproducibility: we will get the same result each time we run the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ALsThN1Dg-NS"
   },
   "outputs": [],
   "source": [
    "# Initialize the random number generator.\n",
    "torch.random.manual_seed(0)\n",
    "\n",
    "# Create the classifier.\n",
    "clf = NNClassifier(ClassifierParameters(), make_model)\n",
    "\n",
    "# Train the classifier.\n",
    "clf.fit(Xtrain_matrix, Ytrain_enc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NdCbpMgSg-NS"
   },
   "source": [
    "We stored the values of the evaluation metrics after each epoch and we can now plot these metrics. We use standard Python plotting functionality. Optionally, read up on how to use [TensorBoard](https://pytorch.org/tutorials/intermediate/tensorboard_tutorial.html) with PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bCDrYX-ig-NS"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%config InlineBackend.figure_format = 'retina' \n",
    "plt.style.use('seaborn')\n",
    "\n",
    "x = range(len(clf.history['train_loss']))\n",
    "fig, ax = plt.subplots(1, 2, figsize=(2*6,1*6))\n",
    "ax[0].plot(x, clf.history['train_loss'], x, clf.history['val_loss']);\n",
    "ax[0].legend(['train loss', 'val loss']);\n",
    "ax[1].plot(x, clf.history['train_acc'], x, clf.history['val_acc']);\n",
    "ax[1].legend(['train acc', 'val acc']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MxJ17r5ug-NT"
   },
   "source": [
    "### Optional tasks\n",
    "\n",
    "Add some code to monitor the model's performance while you are training, and each time you see an improvement, save the current state of the model to a file. Also do *early stopping*: if we fail to get an improvement for a given number of epochs, terminate training.\n",
    "\n",
    "### Evaluation and analysis\n",
    "\n",
    "We predict the outputs for the test set and compute the [accuracy](https://en.wikipedia.org/wiki/Evaluation_of_binary_classifiers) metric to evaluate the quality of the predictions. This is simply the proportion of correct answers and we use the function [`accuracy_score`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html) from scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jiM15h0tg-NT"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "pred = clf.predict(Xtest_matrix)\n",
    "accuracy_score(Ytest_enc, pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D5gMaPuCg-NT"
   },
   "source": [
    "Here is some code to classify a single text. Keep in mind that the classifier works on batches, not single instances, so we need to put the text into a list for input and the output will also be a list. We use the `LabelEncoder`'s `inverse_transform` to convert the output integer index back into a language label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MEDZ8FjJg-NU"
   },
   "outputs": [],
   "source": [
    "testtext = 'as vitrines são vitrines'\n",
    "#testtext = 'kreuzschlitzschraubenzieher'\n",
    "#testtext = 'På Norra älvstranden (Hisingens södra älvstrand) pågår ett omfattande planarbete'\n",
    "\n",
    "X_one = document_representation.transform([testtext]).toarray().astype(np.float32)\n",
    "Y_one = clf.predict(X_one)\n",
    "lblenc.inverse_transform(Y_one)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ip-O3al0g-NV"
   },
   "source": [
    "Play around with different input texts and see how well the classifier works for them. You may also look at individual instances in the validation set or test set.\n",
    "\n",
    "**Reflection.** Can you think of properties that make a text easy to classify?\n",
    "\n",
    "**More reflection.** If you think of the way that we prepared the data for this task, how do you think that our approach is limited? Specifically, think of how we converted the texts into a matrix. How could this be improved to compute the matrix in a smarter way to improve performance? What are the technical challenges with doing that?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a2PuUFKRg-NV"
   },
   "source": [
    "# Working with PyTorch Lightning\n",
    "\n",
    "**If you are happy with the code above, this part can be skipped. All examples in the course will be given in straightforward PyTorch code and while PyTorch Lightning can simplify your code, it is not required for understanding anything in the course.** \n",
    "\n",
    "While Pytorch offers the user detailed control over all aspects of the training process, you might find the implementation above somewhat tedious. Why do we need to write boilerplate code to do trivial things like running the training loop, moving tensors back and forth, or going through the batches?\n",
    "\n",
    "[PyTorch Lightning](https://www.pytorchlightning.ai/) is a library that works on a somewhat higher level of abstraction compared to regular PyTorch, and the purpose is exactly to get rid of \"trivial\" parts such as training loops, so that you can focus on the interesting parts that are specific for the problem you are trying to solve and the models you are working with."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wwTuN1x-jQZ-"
   },
   "source": [
    "**Note for Colab users:** In Colab, this library has to be installed manually. Keep in mind that libraries will have to be reinstalled each time you restart a Colab notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Dj5EbwVwjrCD"
   },
   "outputs": [],
   "source": [
    "!pip install pytorch-lightning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J0hmbxsjg-NV"
   },
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7zva4n8Ag-NW"
   },
   "source": [
    "### Main concepts in PyTorch Lightning\n",
    "\n",
    "The most important building block used in PyTorch Lightning is the [`LightningModule`](https://pytorch-lightning.readthedocs.io/en/latest/common/lightning_module.html). This is the component that stores the neural network model and defines how it should be applied. It can be seen as an extension of the regular PyTorch [`Module`](https://pytorch.org/docs/stable/generated/torch.nn.Module.html).\n",
    "\n",
    "To be able to train and evaluate, we need to provide some data as input. PyTorch Lightning wraps data processing in a component called a [`LightningDataModule`](https://pytorch-lightning.readthedocs.io/en/latest/extensions/datamodules.html). The data module will carry out the necessary data preprocessing and create data loaders that will help us with minibatching (as in our implementation above).\n",
    "\n",
    "Finally, to do the actual work and train the model, we use a [`Trainer`](https://pytorch-lightning.readthedocs.io/en/latest/common/trainer.html), which carries out the initialization and run the training loops. This includes the parts we referred to as \"boilerplate\" above and does not have to be implemented by the user.\n",
    "\n",
    "### Implementing the `LightningDataModule`\n",
    "\n",
    "Let's start with the data module. We don't need to do any preprocessing here, since we did the necessary groundwork previously, so we just need to split the dataset and create the data loaders.\n",
    "\n",
    "**Your work.**\n",
    "- Split the provided data into training and validation parts.\n",
    "- Then implement two methods `train_dataloader` and `val_dataloader` that return data loaders for the training and validation parts, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nEooxlVIg-NW"
   },
   "outputs": [],
   "source": [
    "class ExampleDataModule(pl.LightningDataModule):\n",
    "\n",
    "    def __init__(self, X, Y, param):\n",
    "        super().__init__()\n",
    "        self.param = param\n",
    "        YOUR_CODE_HERE       \n",
    "\n",
    "    def train_dataloader(self):\n",
    "        YOUR_CODE_HERE\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        YOUR_CODE_HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "um4vpQpgg-NW"
   },
   "source": [
    "### Implementing the `LightningModule`\n",
    "\n",
    "Now, let's implement the `LightningModule`. To have a place to store hyperparameters and other configuration \n",
    "settings, we use the same `ClassifierParameters` as above.\n",
    "\n",
    "**Your work.**\n",
    "- Initialize the model structure, the loss function, and the optimizer.\n",
    "- Carry out the computations for training and validation steps.\n",
    "- Predict the outputs at test time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EzK86Agug-NX"
   },
   "outputs": [],
   "source": [
    "class ExampleModule(pl.LightningModule):\n",
    "\n",
    "    def __init__(self, data, params):\n",
    "        super().__init__()\n",
    "\n",
    "        self.params = params        \n",
    "\n",
    "        n_inputs = data.Xtrain.shape[1]\n",
    "        n_outputs = len(set(data.Ytrain))\n",
    "        \n",
    "        # Build the model architecture. If you want, you can use a factory function here as above,\n",
    "        # or you can keep it simple and create the model directly.\n",
    "        self.model = YOUR_CODE_HERE\n",
    "\n",
    "        # Loss function.\n",
    "        self.loss_func = YOUR_CODE_HERE\n",
    "                 \n",
    "    def configure_optimizers(self):\n",
    "        \"\"\"Set up the optimizers used to tune the model weights.\"\"\"\n",
    "        optimizer = YOUR_CODE_HERE\n",
    "        return optimizer                        \n",
    "            \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        \"\"\"Apply the model to one training batch and compute the loss.\"\"\"\n",
    "        # This method needs to return the loss, so that we can compute gradients and update the model.\n",
    "        x, y = batch\n",
    "        model_output = YOUR_CODE_HERE\n",
    "        # As in our previous implementation, model_output should be of \n",
    "        # the shape (batch_size, n_outputs) here.\n",
    "        loss = YOUR_CODE_HERE\n",
    "        self.log('train_loss', loss)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        \"\"\"Apply the model to one validation batch.\"\"\"\n",
    "        # This method does not need to return anything, but you will probably want to\n",
    "        # log some evaluation metric here.\n",
    "        x, y = batch\n",
    "        model_output = YOUR_CODE_HERE\n",
    "        loss = YOUR_CODE_HERE\n",
    "        self.log('val_loss', loss)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Predict the outputs for the given batch `x`.\"\"\"\n",
    "        model_output = YOUR_CODE_HERE\n",
    "        # Again, model_output should be (batch_size, n_outputs). We\n",
    "        # aggregate over the second dimension to get the output.\n",
    "        return model_output.argmax(dim=1)        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w-_c-s_7g-NX"
   },
   "source": [
    "### Running the `Trainer`\n",
    "\n",
    "We now have the building blocks to train our classifier with PyTorch Lightning.\n",
    "\n",
    "The main things we need to do is just to initialize the `LightningDataModule` and `LightningModule` we implemented above, and then run a `Trainer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zHyMg1d6g-NY"
   },
   "outputs": [],
   "source": [
    "def train_with_lightning():\n",
    "    # Like before, we create an object to hold our parameters.\n",
    "    param = ClassifierParameters()\n",
    "\n",
    "    # Initialize the random seed for reproducibility.\n",
    "    torch.random.manual_seed(0)\n",
    "\n",
    "    # Create the data module you defined above.\n",
    "    data_module = YOUR_CODE_HERE\n",
    "\n",
    "    # Create the module.    \n",
    "    module = YOUR_CODE_HERE\n",
    "\n",
    "    # Create a callback for early stopping\n",
    "    es = pl.callbacks.EarlyStopping(monitor='val_loss')\n",
    "    \n",
    "    # We can specify how many GPUs we want to use.\n",
    "    # Alternatively, provide identifiers of which GPUS to use.\n",
    "    gpus = None if param.device == 'cpu' else 1\n",
    "    \n",
    "    # Create the trainer object that will run the training loop.\n",
    "    # Set progress_bar_refresh_rate=0 if you find the progress bar annoying.\n",
    "    # Set weights_summary=None if you want to get rid of the architecture summary.\n",
    "    trainer = pl.Trainer(gpus=gpus, callbacks=[es], max_epochs=param.n_epochs)\n",
    "    \n",
    "    # Finally, call the training method!\n",
    "    trainer.fit(module, data_module)\n",
    "    \n",
    "    # To make predictions later, we need to keep the trainer and the module.\n",
    "    return trainer, module\n",
    "    \n",
    "ptl_trainer, ptl_module = train_with_lightning()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pe_AUmCWg-NZ"
   },
   "source": [
    "### Predicting outputs\n",
    "\n",
    "Now that we have trained the model, let's just see how to make predictions for new instances.\n",
    "\n",
    "We first do this for a single instance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6RBDn7WVg-Na"
   },
   "outputs": [],
   "source": [
    "testtext = \"La sfida dell'inclusione raccontata agli studenti da due gemelle di 19 anni affette da una grave disabilità.\"\n",
    "\n",
    "# Create a data loader to hold this single instance.\n",
    "dl = DataLoader(list(document_representation.transform([testtext]).toarray().astype(np.float32)))\n",
    "\n",
    "# Predict the output.\n",
    "Y_one = ptl_trainer.predict(ptl_module, dl)[0].cpu().numpy()\n",
    "\n",
    "# Convert the integer index back into a string.\n",
    "lblenc.inverse_transform(Y_one)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V73oJqP3g-Na"
   },
   "source": [
    "To predict the outputs for the test set as a whole, we will do basically the same thing. There's only one small technicality here: the `predict` step will return a list of PyTorch tensors, one for each batch. This may be enough in some circumstances, but we will combine them into a single list. (This can be done using PyTorch or NumPy.)\n",
    "\n",
    "Note that it is not necessary to map the integers back into strings here, since we are just interested in whether the predictions are correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E8Mgzq8Eg-Nb"
   },
   "outputs": [],
   "source": [
    "# Create a dataloader for the whole test set.\n",
    "test_dl = DataLoader(list(Xtest_matrix), batch_size=ptl_module.params.batch_size)\n",
    "\n",
    "# Compute the outputs for all batches.\n",
    "batch_outputs = ptl_trainer.predict(ptl_module, test_dl)\n",
    "\n",
    "# Glue together all the batch outputs.\n",
    "outputs = torch.hstack(batch_outputs).cpu().numpy()\n",
    "\n",
    "# Compute the classification accuracy.\n",
    "accuracy_score(Ytest_enc, outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uoKHir2Zg-Nb"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Self-study notebook week 1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
