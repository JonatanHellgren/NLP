{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bXYgzOHIXQ1m",
    "outputId": "1d2ee20a-4903-4d78-af51-1917f7260829"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package gutenberg to /home/jona/nltk_data...\n",
      "[nltk_data]   Package gutenberg is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/jona/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/jona/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import gutenberg\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "\n",
    "nltk.download('gutenberg')\n",
    "nltk.download('stopwords')\n",
    "nltk.download(\"punkt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zryrpc1Md182"
   },
   "source": [
    "#Code for dowloading corpus preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "UeE8nji5XBAp"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "preprocessing step of the document. First we remove the stop words and change all\n",
    "the words to lower case. Then we remove the uncommon words, given a specified threshold.\n",
    "\"\"\"\n",
    "def preprocess(doc, threshold):\n",
    "    docs = sort_chapters(doc)\n",
    "    docs = filter_doc(docs)\n",
    "    docs = removing_uncommon(docs, threshold)\n",
    "\n",
    "    return docs\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "A function for removing the stop words in a document and changing all the words\n",
    "to lower case.\n",
    "\"\"\"\n",
    "def filter_doc(doc):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "\n",
    "    filtered_doc = []\n",
    "    for chapter in doc:\n",
    "        filtered_chapter = [w.lower() for w in chapter if not w.lower() in stop_words and w.isalnum()]\n",
    "        filtered_doc.append(filtered_chapter)\n",
    "\n",
    "    return filtered_doc\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "A function for removing the less common words, i.e. the words with a higher\n",
    "frequency in the document over a specified threshold.\n",
    "\"\"\"\n",
    "def removing_uncommon(doc, threshold):\n",
    "    freq = get_counter(doc)\n",
    "    \n",
    "    filtered_doc = []\n",
    "    for chapter in doc:\n",
    "        filtered_chapter = [w for w in chapter if freq[w] >= threshold]\n",
    "        filtered_doc.append(filtered_chapter)\n",
    "\n",
    "    return filtered_doc\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "A helper function to get a frequency counter for the document.\n",
    "\"\"\"\n",
    "def get_counter(doc):\n",
    "    freq = Counter()\n",
    "    for sentance in doc:\n",
    "        for word in sentance:\n",
    "            freq[word] += 1\n",
    "\n",
    "    return freq\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Start by assigning all words to the unassigned class -1, just to constuct\n",
    "a data structure that can hold the categories in the future\n",
    "\"\"\"\n",
    "def get_categories(docs):\n",
    "    cats = []\n",
    "    for chapter in docs:\n",
    "        chap = []\n",
    "        for _ in chapter:\n",
    "            chap.append(-1)\n",
    "        cats.append(np.array(chap))\n",
    "    return cats\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Divide the book up into chapters, the chapters will be though of as documents\n",
    "\"\"\"\n",
    "def sort_chapters(doc):\n",
    "    docs = []\n",
    "    chapter = []\n",
    "    for word in doc:\n",
    "        if word != 'CHAPTER':\n",
    "            chapter.append(word)\n",
    "        else:\n",
    "            docs.append(chapter)\n",
    "            chapter = []\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_IonwPNHgQ_c"
   },
   "source": [
    "#LDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-ql-gbkvdwpz"
   },
   "source": [
    "##Defining LDA model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "f3as7pPhX36f"
   },
   "outputs": [],
   "source": [
    "class LDA:\n",
    "\n",
    "    def __init__(self, docs, cats, n_classes, voc, alpha, beta):\n",
    "        # The document we are analysing\n",
    "        self.docs = docs\n",
    "        # All the tokens stored in a array of lists\n",
    "        self.w = np.array(docs)\n",
    "        # The category for each word\n",
    "        self.z = np.array(cats)\n",
    "        # Number of categories we are looking for\n",
    "        self.K = n_classes\n",
    "        # The words in our corpus\n",
    "        self.voc = voc\n",
    "        # Number of words in our corpus\n",
    "        self.V = len(voc)\n",
    "        # Hyper parameters used for Gibb's sampling\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "\n",
    "        \"\"\"\n",
    "        Instead of counting these we just store the value in a matrix, this\n",
    "        makes to code a bit more optimized since we do not have to allocate or\n",
    "        do this computation over and over agian.\n",
    "        \"\"\"\n",
    "        # Category distribution for each doc (chapter)\n",
    "        self.n = np.zeros( [len(docs), n_classes] )\n",
    "        # Category distribution for each word\n",
    "        self.m = np.zeros( [len(voc), n_classes] )\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    This function fits our model to the data by iterating a collapsed\n",
    "    Gibb's sampling n times\n",
    "    \"\"\"\n",
    "    def fit(self, n_iteration):\n",
    "        \n",
    "        print(\"Fitting the model\")\n",
    "        for it in range(n_iteration):\n",
    "            print(\"Iteration %i\" %(it+1))\n",
    "            self.iterate()\n",
    "\n",
    "    \"\"\"\n",
    "    This function performs a single iteration in our algorithm\n",
    "    \"\"\"\n",
    "    def iterate(self):\n",
    "        \n",
    "        # Allocating these now to optimize the code\n",
    "        q = np.zeros(self.K)\n",
    "        p = np.zeros(self.K)\n",
    "\n",
    "        for d, chapter in enumerate(self.docs):\n",
    "            for j, word in enumerate(chapter):\n",
    "                word_ind = self.voc.index(word)\n",
    "\n",
    "                # Reomve the category for our current word and updating m & n acordingly\n",
    "                if self.z[d][j] != -1:\n",
    "                    k = self.z[d][j]\n",
    "                    self.n[d, k] -= 1\n",
    "                    self.m[word_ind, k] -= 1\n",
    "                    self.z[d][j] = -1 \n",
    "\n",
    "                # computes the posterior distribution for the category distribution\n",
    "                for k in range(self.K):\n",
    "                    sum_m_k = np.sum(self.m[:, k]) \n",
    "                    q[k] = ( self.alpha + self.n[d, k] ) * \\\n",
    "                           ( self.beta + self.m[word_ind, k] ) / \\\n",
    "                           (self.V * self.beta + sum_m_k ) \n",
    "                # Normalizing\n",
    "                p = q / np.sum(q)\n",
    "\n",
    "                # Assigning a new category\n",
    "                assigned_category = np.random.choice(self.K, 1, p = p)[0]\n",
    "                \n",
    "                # Updating the model object accordingly\n",
    "                self.z[d][j] = assigned_category\n",
    "                self.n[d, assigned_category] += 1\n",
    "                self.m[word_ind, assigned_category] += 1\n",
    "\n",
    "    \"\"\"\n",
    "    To evaluate our model we take a look at the highest values in the either the\n",
    "    relative frequency or the total frequency.\n",
    "    This function returns thoose counters.\n",
    "    \"\"\"\n",
    "    def evaluate(self, relative=True):\n",
    "        counters = self.count_occurances()\n",
    "\n",
    "        if relative:\n",
    "            for key in self.voc:\n",
    "                tot_freq = sum([counters[it][key] for it in range(self.K)])\n",
    "                \n",
    "                for jt in range(self.K):\n",
    "                    counters[jt][key] /= tot_freq\n",
    "\n",
    "        return counters\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    This is a helper function for our evaluation function. Here we count the frequencies \n",
    "    each appeared in each category and save it in a list of counter objects.\n",
    "    \"\"\"\n",
    "    def count_occurances(self):\n",
    "        counters = [Counter() for _ in range(self.K)]\n",
    "\n",
    "        for d, chapter in enumerate(self.docs):\n",
    "            for j, word in enumerate(chapter):\n",
    "                counters[self.z[d][j]][word] += 1\n",
    "\n",
    "        return counters\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "jUZiGaqNX7pm"
   },
   "outputs": [],
   "source": [
    "def train_model(alpha, beta, n_iteration, K):\n",
    "    # Moby Dick - Herman Melville\n",
    "    print(\"Loading text document and formatting\")\n",
    "    moby_dick = gutenberg.words('melville-moby_dick.txt')\n",
    "\n",
    "    threshold = 10 \n",
    "    docs = preprocess(moby_dick, threshold)\n",
    "    freq = get_counter(docs)\n",
    "    cats = get_categories(docs)\n",
    "    voc = list(freq.keys())\n",
    "\n",
    "    model = LDA(docs, cats, K, voc, alpha, beta)\n",
    "    model.fit(n_iteration)\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HML5fnuIeAsV"
   },
   "source": [
    "##LDA 1: α = β = 0.1,  100 iterations, 10 categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GOvr2pUSia2l"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading text document and formatting\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_208441/2081198903.py:7: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  self.w = np.array(docs)\n",
      "/tmp/ipykernel_208441/2081198903.py:9: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  self.z = np.array(cats)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting the model\n",
      "Iteration 1\n",
      "Iteration 2\n",
      "Iteration 3\n",
      "Iteration 4\n",
      "Iteration 5\n",
      "Iteration 6\n",
      "Iteration 7\n",
      "Iteration 8\n",
      "Iteration 9\n",
      "Iteration 10\n",
      "Iteration 11\n",
      "Iteration 12\n",
      "Iteration 13\n",
      "Iteration 14\n",
      "Iteration 15\n",
      "Iteration 16\n",
      "Iteration 17\n",
      "Iteration 18\n",
      "Iteration 19\n",
      "Iteration 20\n",
      "Iteration 21\n",
      "Iteration 22\n",
      "Iteration 23\n",
      "Iteration 24\n",
      "Iteration 25\n",
      "Iteration 26\n",
      "Iteration 27\n",
      "Iteration 28\n",
      "Iteration 29\n",
      "Iteration 30\n",
      "Iteration 31\n",
      "Iteration 32\n",
      "Iteration 33\n",
      "Iteration 34\n",
      "Iteration 35\n",
      "Iteration 36\n",
      "Iteration 37\n",
      "Iteration 38\n",
      "Iteration 39\n",
      "Iteration 40\n",
      "Iteration 41\n",
      "Iteration 42\n",
      "Iteration 43\n",
      "Iteration 44\n",
      "Iteration 45\n",
      "Iteration 46\n",
      "Iteration 47\n",
      "Iteration 48\n",
      "Iteration 49\n",
      "Iteration 50\n",
      "Iteration 51\n",
      "Iteration 52\n",
      "Iteration 53\n"
     ]
    }
   ],
   "source": [
    "model = train_model(0.1, 0.1, 100, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ixd5mrwmeLlB"
   },
   "source": [
    "###Most comman, total frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1-bldQQJlP5A",
    "outputId": "d063f705-e768-4423-9d6a-aead1ecf7986"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(',', 4606), ('.', 1679), (\"'\", 1533), ('!', 858), ('\"', 761), ('-', 581), ('--', 446), (';', 347), ('?', 345), ('ye', 339)]\n",
      "[(';', 422), ('\"', 241), ('captain', 198), ('said', 125), ('ahab', 118), ('ship', 109), (',\"', 101), ('.\"', 96), ('ye', 93), ('?\"', 91)]\n",
      "[(';', 336), ('captain', 106), ('ship', 105), (\"'\", 94), ('\"\\'', 61), ('ships', 46), ('two', 44), ('men', 43), ('steelkilt', 40), ('gentlemen', 35)]\n",
      "[(';', 925), ('!', 232), ('god', 116), ('thou', 83), ('yet', 79), ('white', 78), ('jonah', 55), ('old', 54), ('sailor', 54), ('would', 53)]\n",
      "[('whale', 626), ('.', 563), (';', 489), ('whales', 211), ('sperm', 182), ('leviathan', 93), ('(', 87), ('oil', 69), ('found', 53), ('fish', 52)]\n",
      "[(';', 582), ('boat', 262), ('ahab', 257), ('whale', 236), ('seemed', 155), ('white', 153), ('sea', 132), ('ship', 117), ('starbuck', 116), ('crew', 114)]\n",
      "[('\"', 326), ('.\"', 188), ('whale', 170), ('--', 167), (\"'\", 166), ('.', 97), ('-', 84), (';', 83), ('fish', 67), ('right', 41)]\n",
      "[(';', 472), ('-', 278), ('whale', 140), ('head', 127), ('two', 90), ('side', 88), ('.', 80), ('away', 67), ('sperm', 62), ('hands', 53)]\n",
      "[(',', 13661), ('.', 4037), ('-', 1517), (\"'\", 728), ('one', 647), ('upon', 452), ('like', 444), ('--', 430), ('though', 346), ('would', 329)]\n",
      "[(';', 294), ('.', 262), ('queequeg', 225), ('said', 77), (\"'\", 74), (',\"', 66), ('bed', 66), ('seemed', 58), ('night', 54), ('cook', 50)]\n"
     ]
    }
   ],
   "source": [
    "counters = model.evaluate(relative = False)\n",
    "for counter in counters:\n",
    "  print(counter.most_common(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s6fASBxGeQZG"
   },
   "source": [
    "###Most common, relative frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Vfb8Opoo093C",
    "outputId": "87693db0-d33b-4b6d-995a-c594a9623439"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('moving', 1.0), ('ha', 1.0), ('yon', 1.0), ('hurrah', 1.0), ('kick', 1.0), ('using', 1.0), ('tambourine', 1.0), ('pip', 0.9594594594594594), ('.)', 0.9545454545454546), ('guess', 0.9444444444444444)]\n",
      "[('peleg', 1.0), ('quaker', 1.0), ('dost', 1.0), ('bildad', 1.0), ('bye', 1.0), ('guernsey', 1.0), ('elijah', 0.9333333333333333), ('spirits', 0.9), ('steward', 0.8888888888888888), ('seven', 0.875)]\n",
      "[('gabriel', 1.0), ('commodore', 1.0), (\"!'\", 1.0), (\"?'\", 1.0), (\",'\", 1.0), ('priest', 1.0), ('leak', 1.0), ('radney', 1.0), ('steelkilt', 1.0), ('lakeman', 1.0)]\n",
      "[('mild', 1.0), ('pulpit', 1.0), ('naught', 1.0), ('woe', 1.0), ('soil', 1.0), ('whiteness', 1.0), ('vice', 1.0), ('doubloon', 1.0), ('um', 1.0), ('lesson', 0.9230769230769231)]\n",
      "[('cetology', 1.0), ('greenland', 1.0), ('baleen', 1.0), ('magnitude', 1.0), ('cuvier', 1.0), ('scoresby', 1.0), ('species', 1.0), ('century', 1.0), ('folio', 1.0), ('ii', 1.0)]\n",
      "[('wave', 1.0), ('gradually', 1.0), ('fate', 1.0), ('bowed', 1.0), ('speed', 1.0), ('floats', 1.0), ('silently', 1.0), ('commander', 1.0), ('moments', 1.0), ('flank', 1.0)]\n",
      "[('...', 1.0), ('bunger', 1.0), ('ginger', 0.9375), ('possession', 0.9166666666666666), ('money', 0.9166666666666666), ('law', 0.8095238095238095), ('please', 0.7692307692307693), ('gentleman', 0.75), ('dry', 0.7333333333333333), ('swallow', 0.7272727272727273)]\n",
      "[('block', 1.0), ('strain', 1.0), ('simultaneously', 1.0), ('summit', 1.0), ('tackle', 1.0), ('bucket', 1.0), ('crotch', 1.0), ('tun', 1.0), ('pole', 0.9259259259259259), ('rib', 0.9230769230769231)]\n",
      "[('supplied', 1.0), ('find', 1.0), ('bird', 1.0), ('besides', 1.0), ('appeared', 1.0), ('taken', 1.0), ('certain', 1.0), ('fail', 1.0), ('dread', 1.0), ('ordinary', 1.0)]\n",
      "[('corner', 1.0), ('clam', 1.0), ('tomahawk', 1.0), ('mrs', 1.0), ('ramadan', 1.0), ('fleece', 1.0), ('dat', 1.0), ('landlord', 0.9705882352941176), ('streets', 0.9333333333333333), ('steak', 0.9333333333333333)]\n"
     ]
    }
   ],
   "source": [
    "counters = model.evaluate()\n",
    "for counter in counters:\n",
    "  print(counter.most_common(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bpeBwvbpeUzR"
   },
   "source": [
    "##LDA 2: α = β = 0.01, 100 iterations 10 categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zVQfRocNwwQJ"
   },
   "outputs": [],
   "source": [
    "model2 = train_model(0.01, 0.01, 100, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9pReUF0XejOC"
   },
   "source": [
    "###Most common, total frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0bzfjlQ_07u5",
    "outputId": "1f3af834-476c-48df-ccd4-bb2b4a7235ae"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('whales', 160), ('like', 143), ('whaling', 107), ('though', 97), ('water', 86), ('three', 85), ('nantucket', 78), ('ship', 76), ('may', 76), ('peleg', 74)]\n",
      "[('.', 494), ('little', 148), ('queequeg', 147), ('like', 111), (',\"', 95), ('good', 89), ('come', 84), ('harpooneer', 81), ('away', 78), ('sort', 74)]\n",
      "[('white', 98), ('?', 68), ('still', 66), ('times', 64), ('yet', 50), ('without', 33), ('waters', 32), ('wild', 30), ('day', 30), ('seas', 28)]\n",
      "[('ahab', 470), ('ship', 252), ('thou', 236), ('man', 218), ('boat', 205), ('captain', 171), ('deck', 162), ('like', 131), ('men', 125), ('round', 109)]\n",
      "[(',', 18249), ('.', 5756), (';', 3946), ('-', 2493), (\"'\", 2150), ('\"', 1168), ('--', 1045), ('whale', 963), ('one', 889), ('upon', 547)]\n",
      "[(\"'\", 456), ('\"', 266), ('ye', 110), ('.', 100), ('?', 88), ('\"\\'', 66), ('said', 52), ('stubb', 46), ('steelkilt', 40), ('cook', 39)]\n",
      "[('.', 368), ('whale', 230), ('sperm', 115), ('?', 98), ('fish', 86), ('(', 82), ('whales', 78), ('oil', 71), ('great', 58), ('like', 50)]\n",
      "[(',\"', 178), ('ship', 168), ('captain', 150), ('?\"', 145), ('said', 144), ('queequeg', 103), ('pequod', 93), ('well', 77), ('bildad', 76), ('see', 71)]\n",
      "[('like', 199), ('seemed', 195), ('three', 151), ('still', 147), ('though', 146), ('stubb', 131), ('boats', 112), ('flask', 104), ('air', 98), ('starbuck', 98)]\n",
      "[('!', 938), ('?', 362), ('ye', 279), ('!\"', 243), ('.\"', 215), ('sir', 155), ('oh', 121), ('white', 119), ('!--', 119), ('old', 115)]\n"
     ]
    }
   ],
   "source": [
    "counters = model2.evaluate(relative = False)\n",
    "for counter in counters:\n",
    "  print(counter.most_common(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dURuhEfZexSQ"
   },
   "source": [
    "###Most common, relative frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8QQ4uP2y1CJf",
    "outputId": "24d92268-65d9-457c-f956-2308e28cae7a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('french', 1.0), ('street', 1.0), ('monstrous', 1.0), ('whereas', 1.0), ('learned', 1.0), ('spouts', 1.0), ('voyages', 1.0), ('frequently', 1.0), ('southern', 1.0), ('doubtless', 1.0)]\n",
      "[('harpooneer', 1.0), ('money', 1.0), ('image', 1.0), ('begin', 1.0), ('lungs', 1.0), ('difference', 1.0), ('considering', 1.0), ('unaccountable', 1.0), ('answer', 1.0), ('social', 1.0)]\n",
      "[('leagues', 1.0), ('picture', 1.0), ('palms', 1.0), ('seek', 1.0), ('faith', 1.0), ('spiritual', 1.0), ('straits', 1.0), ('george', 1.0), ('canoe', 1.0), ('subtle', 1.0)]\n",
      "[('dust', 1.0), ('waves', 1.0), ('tow', 1.0), ('masts', 1.0), ('sing', 1.0), ('bringing', 1.0), ('seated', 1.0), ('rigging', 1.0), ('compasses', 1.0), ('thither', 1.0)]\n",
      "[('supplied', 1.0), ('late', 1.0), ('school', 1.0), ('--', 1.0), ('coat', 1.0), ('heart', 1.0), ('body', 1.0), ('ever', 1.0), ('world', 1.0), ('somehow', 1.0)]\n",
      "[(\"!'\", 1.0), ('shipmate', 1.0), ('kick', 1.0), ('wharf', 1.0), (\"?'\", 1.0), (\",'\", 1.0), (\".'\", 1.0), ('radney', 1.0), ('steelkilt', 1.0), ('lakeman', 1.0)]\n",
      "[('cetology', 1.0), ('fat', 1.0), ('baleen', 1.0), ('folio', 1.0), ('ii', 1.0), ('octavo', 1.0), ('iii', 1.0), ('porpoise', 1.0), (').--', 1.0), ('ambergris', 1.0)]\n",
      "[('towing', 1.0), ('letters', 1.0), ('destroyed', 1.0), ('replied', 1.0), ('upright', 1.0), ('\"--', 1.0), ('service', 1.0), ('trying', 1.0), ('spare', 1.0), ('canvas', 1.0)]\n",
      "[('wound', 1.0), ('dart', 1.0), ('stroke', 1.0), ('points', 1.0), ('rushed', 1.0), ('leaping', 1.0), ('striving', 1.0), ('falling', 1.0), ('invisible', 1.0), ('swung', 1.0)]\n",
      "[('glancing', 1.0), ('moving', 1.0), ('.)', 1.0), ('tis', 1.0), ('nailed', 1.0), ('tiger', 1.0), ('pay', 1.0), ('ha', 1.0), ('yon', 1.0), ('hurrah', 1.0)]\n"
     ]
    }
   ],
   "source": [
    "counters = model2.evaluate()\n",
    "for counter in counters:\n",
    "  print(counter.most_common(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MvUIZxNAe1J2"
   },
   "source": [
    "##LDA 3: α = β = 0.1, 100 iterations, 50 cetgories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wTCUfeFjGcwO"
   },
   "outputs": [],
   "source": [
    "model3 = train_model(0.1, 0.1, 100, 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9kc1-4aifIHt"
   },
   "source": [
    "###Most common.\n",
    "First we see the 50 most common in total frequency and then we see the 50 highest relative frequencies for this models assigned classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0tEXYChTG4hT",
    "outputId": "6e14b93b-eb07-40d3-fee7-1f533fdc0bb5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('watches', 0.75), ('helm', 0.7407407407407407), ('parsee', 0.6666666666666666), ('closed', 0.6), ('latitude', 0.5384615384615384), ('cast', 0.5333333333333333), ('unseen', 0.5), ('apart', 0.5), ('spoken', 0.46153846153846156), ('clouds', 0.45454545454545453)]\n",
      "[('school', 0.8), ('discovered', 0.7), ('remains', 0.625), ('utterly', 0.6153846153846154), ('host', 0.6), ('magnitude', 0.5909090909090909), ('creatures', 0.5882352941176471), ('flood', 0.5714285714285714), ('marked', 0.5555555555555556), ('subsequent', 0.5384615384615384)]\n",
      "[('breakfast', 0.5714285714285714), ('troubled', 0.5333333333333333), ('sat', 0.5111111111111111), ('pagan', 0.5), ('boots', 0.5), ('mortals', 0.4), ('clock', 0.375), ('softly', 0.36363636363636365), ('fellows', 0.36363636363636365), ('ages', 0.35714285714285715)]\n",
      "[('tackles', 0.9411764705882353), ('tub', 0.9230769230769231), ('tackle', 0.8), ('carefully', 0.7272727272727273), ('strip', 0.7), ('bucket', 0.6666666666666666), ('block', 0.6363636363636364), ('attached', 0.6190476190476191), ('hook', 0.6111111111111112), ('windlass', 0.5909090909090909)]\n",
      "[('george', 1.0), ('perseus', 0.9090909090909091), ('harbor', 0.5625), ('idol', 0.5384615384615384), ('gives', 0.5), ('hints', 0.5), ('kings', 0.5), ('bible', 0.45454545454545453), ('leaves', 0.4444444444444444), ('evinced', 0.42857142857142855)]\n",
      "[('dignity', 0.6), ('marble', 0.5714285714285714), ('declared', 0.5), ('endure', 0.5), ('brow', 0.475), ('regarded', 0.4166666666666667), ('contrary', 0.36363636363636365), ('pick', 0.36363636363636365), ('birds', 0.36363636363636365), ('cask', 0.36363636363636365)]\n",
      "[('ginger', 1.0), ('spades', 0.9), ('monkey', 0.5), ('sharks', 0.4146341463414634), ('handed', 0.4), ('usage', 0.3333333333333333), ('spirits', 0.3), ('murder', 0.3), ('stock', 0.3), ('dough', 0.29411764705882354)]\n",
      "[('absent', 0.5), ('social', 0.5), ('infallibly', 0.45454545454545453), ('slave', 0.45454545454545453), ('ends', 0.4444444444444444), ('greatest', 0.4166666666666667), ('seated', 0.4074074074074074), ('cod', 0.38461538461538464), ('grim', 0.36363636363636365), ('pine', 0.36363636363636365)]\n",
      "[('instances', 0.6774193548387096), ('oarsman', 0.5714285714285714), ('scenes', 0.5454545454545454), ('perils', 0.5416666666666666), ('extreme', 0.5384615384615384), ('experience', 0.5), ('utmost', 0.5), ('headsman', 0.5), ('shock', 0.4666666666666667), ('fishery', 0.4461538461538462)]\n",
      "[('herd', 0.8461538461538461), ('lake', 0.8), ('straits', 0.75), ('circles', 0.7272727272727273), ('forms', 0.5), ('stump', 0.5), ('backs', 0.46153846153846156), ('closely', 0.42857142857142855), ('captured', 0.4117647058823529), ('evinced', 0.38095238095238093)]\n",
      "[('whiteness', 1.0), ('hue', 0.6428571428571429), ('aught', 0.6363636363636364), ('phantom', 0.6153846153846154), ('terror', 0.42857142857142855), ('horror', 0.4166666666666667), ('dumb', 0.4166666666666667), ('smells', 0.4166666666666667), ('fowl', 0.4), ('mountains', 0.38461538461538464)]\n",
      "[('bildad', 1.0), ('peleg', 0.9864864864864865), ('quaker', 0.9), ('yojo', 0.6470588235294118), ('service', 0.5454545454545454), ('dost', 0.5238095238095238), ('pious', 0.5), ('seven', 0.5), ('papers', 0.4), ('bye', 0.4)]\n",
      "[('interval', 0.7083333333333334), ('encounter', 0.6666666666666666), ('gliding', 0.6428571428571429), ('food', 0.6153846153846154), ('birth', 0.6), ('superstitious', 0.6), (');', 0.5714285714285714), ('encountered', 0.5), ('naturally', 0.5), ('mountain', 0.5)]\n",
      "[('dinner', 0.6666666666666666), ('dough', 0.6470588235294118), ('officer', 0.6), ('steward', 0.5555555555555556), ('harpooneers', 0.48148148148148145), ('lived', 0.46153846153846156), ('usage', 0.4166666666666667), ('mates', 0.39473684210526316), ('table', 0.3783783783783784), ('butter', 0.36363636363636365)]\n",
      "[('fit', 0.5333333333333333), ('thine', 0.5), ('mild', 0.5), ('hill', 0.47058823529411764), ('sooner', 0.46153846153846156), ('wife', 0.46153846153846156), ('died', 0.45), ('weary', 0.4375), ('countenance', 0.4166666666666667), ('yon', 0.4)]\n",
      "[(',', 0.9962243502051984), ('.', 0.9417981542125633), ('one', 0.9401330376940134), (';', 0.929873417721519), ('whole', 0.9083969465648855), ('might', 0.9010989010989011), ('upon', 0.8994515539305301), ('like', 0.8974763406940063), ('must', 0.8953068592057761), ('striking', 0.875)]\n",
      "[('eastward', 0.9), ('leagues', 0.6), ('fowls', 0.6), ('leaping', 0.5714285714285714), ('ocean', 0.49382716049382713), ('descried', 0.45), ('ran', 0.45), ('slowly', 0.4489795918367347), ('beheld', 0.42857142857142855), ('calm', 0.42105263157894735)]\n",
      "[('gabriel', 1.0), ('jeroboam', 1.0), ('bedford', 0.8333333333333334), ('letter', 0.7857142857142857), ('street', 0.6153846153846154), ('streets', 0.6), ('ishmael', 0.45), ('city', 0.42857142857142855), ('money', 0.4166666666666667), ('individual', 0.38461538461538464)]\n",
      "[('clam', 1.0), ('mrs', 1.0), ('hussey', 0.9411764705882353), ('cod', 0.5384615384615384), ('supper', 0.4), ('pots', 0.3888888888888889), ('clothes', 0.36363636363636365), ('ramadan', 0.36363636363636365), ('key', 0.3333333333333333), ('sake', 0.3333333333333333)]\n",
      "[('whalers', 0.7894736842105263), ('000', 0.75), ('dutch', 0.6923076923076923), ('samuel', 0.6363636363636364), ('named', 0.6363636363636364), ('merchant', 0.631578947368421), ('whaling', 0.6259541984732825), ('beef', 0.6111111111111112), ('discovery', 0.5454545454545454), ('quantity', 0.5384615384615384)]\n",
      "[('landlord', 0.9411764705882353), ('bed', 0.6933333333333334), ('tomahawk', 0.631578947368421), ('bar', 0.5454545454545454), ('blanket', 0.5384615384615384), ('harpooneer', 0.5308641975308642), ('room', 0.5263157894736842), ('cannibal', 0.47368421052631576), ('bench', 0.47058823529411764), ('window', 0.4666666666666667)]\n",
      "[('forge', 1.0), ('perth', 0.9444444444444444), ('blacksmith', 0.8571428571428571), ('canst', 0.8333333333333334), ('thou', 0.8228346456692913), ('thyself', 0.8181818181818182), ('hast', 0.7857142857142857), ('beware', 0.7142857142857143), ('thee', 0.711864406779661), ('art', 0.6585365853658537)]\n",
      "[('sleet', 1.0), ('nest', 0.9090909090909091), ('fingers', 0.6428571428571429), ('headed', 0.64), ('crow', 0.5714285714285714), ('afloat', 0.5454545454545454), ('vice', 0.5454545454545454), ('manned', 0.5384615384615384), ('isles', 0.5294117647058824), ('served', 0.5)]\n",
      "[('halloa', 1.0), ('tis', 0.8260869565217391), ('hark', 0.75), ('split', 0.6923076923076923), ('ah', 0.6086956521739131), ('bell', 0.6), ('crack', 0.6), ('tied', 0.5454545454545454), ('stay', 0.5384615384615384), ('ask', 0.5)]\n",
      "[('ambergris', 1.0), ('observe', 0.7272727272727273), ('milk', 0.6363636363636364), ('descending', 0.6), ('feels', 0.5384615384615384), ('colour', 0.4375), ('nature', 0.38095238095238093), ('pleasant', 0.375), ('ill', 0.3684210526315789), ('taste', 0.36363636363636365)]\n",
      "[('compasses', 0.8), ('needle', 0.6363636363636364), ('trunk', 0.625), ('motions', 0.5833333333333334), ('elephant', 0.5789473684210527), ('east', 0.5238095238095238), ('binnacle', 0.4375), ('flukes', 0.40540540540540543), ('tail', 0.38461538461538464), ('needles', 0.36363636363636365)]\n",
      "[('news', 0.6363636363636364), ('thither', 0.6), ('vessels', 0.5769230769230769), ('cruising', 0.5769230769230769), ('hither', 0.5384615384615384), ('sailed', 0.5), ('conduct', 0.5), ('ignorant', 0.5), ('captains', 0.4782608695652174), ('covered', 0.45454545454545453)]\n",
      "[('skeleton', 0.7941176470588235), ('skull', 0.7083333333333334), ('spine', 0.7058823529411765), ('ribs', 0.6190476190476191), ('size', 0.5), ('rib', 0.46153846153846156), ('temple', 0.45454545454545453), ('grown', 0.3888888888888889), ('inches', 0.3684210526315789), ('naked', 0.36363636363636365)]\n",
      "[('vapour', 0.6), ('spouting', 0.5833333333333334), ('contact', 0.45454545454545453), ('ordinary', 0.42105263157894735), ('supplied', 0.4166666666666667), ('surface', 0.41025641025641024), ('occurred', 0.4), ('canal', 0.4), ('ponderous', 0.38461538461538464), ('wherefore', 0.38461538461538464)]\n",
      "[('.\"', 0.9104166666666667), ('?\"', 0.8), ('devil', 0.7857142857142857), ('guess', 0.7777777777777778), ('\"', 0.7670850767085077), (',\"', 0.7525083612040134), ('tow', 0.7142857142857143), ('chap', 0.6153846153846154), ('flung', 0.6), ('mean', 0.6)]\n",
      "[('summit', 0.8), ('tun', 0.7857142857142857), ('external', 0.6666666666666666), ('front', 0.6), ('blinds', 0.6), ('filled', 0.5909090909090909), ('jaw', 0.5), ('lip', 0.5), ('objects', 0.5), ('ear', 0.5)]\n",
      "[('heavens', 0.8125), ('door', 0.7777777777777778), ('corner', 0.75), ('marvellous', 0.6666666666666666), ('ramadan', 0.6363636363636364), ('deal', 0.5833333333333334), ('bag', 0.5454545454545454), ('sitting', 0.5454545454545454), ('savage', 0.5283018867924528), ('unaccountable', 0.5)]\n",
      "[('guernsey', 1.0), ('elijah', 0.8666666666666667), ('bye', 0.6), ('rose', 0.5142857142857142), ('owners', 0.5), ('jack', 0.5), ('shipped', 0.5), ('trying', 0.5), ('articles', 0.45454545454545453), ('nights', 0.45454545454545453)]\n",
      "[('chapel', 0.9), ('pulpit', 0.875), ('ladder', 0.5833333333333334), ('memory', 0.5454545454545454), ('peter', 0.4), ('marks', 0.4), ('skin', 0.3888888888888889), ('methinks', 0.3), ('ropes', 0.2727272727272727), ('walls', 0.2727272727272727)]\n",
      "[('jonah', 0.8588235294117647), ('lesson', 0.8461538461538461), ('delight', 0.7222222222222222), ('shipmates', 0.6785714285714286), ('woe', 0.4838709677419355), ('berth', 0.4), ('feels', 0.38461538461538464), ('wharf', 0.36363636363636365), ('tide', 0.3333333333333333), ('god', 0.3310344827586207)]\n",
      "[('works', 0.8387096774193549), ('pots', 0.5555555555555556), ('tiller', 0.5384615384615384), ('oil', 0.5222222222222223), ('casks', 0.45), ('lamp', 0.4482758620689655), ('try', 0.4074074074074074), ('pot', 0.4), ('aboard', 0.3333333333333333), ('smells', 0.3333333333333333)]\n",
      "[('blows', 0.875), ('sword', 0.6), ('afternoon', 0.6), ('leaned', 0.5833333333333334), ('missing', 0.5714285714285714), ('warp', 0.5384615384615384), ('motion', 0.5294117647058824), ('skill', 0.5), ('dropped', 0.48), ('reached', 0.47058823529411764)]\n",
      "[('pacing', 0.8181818181818182), ('swear', 0.75), ('starbuck', 0.7336956521739131), ('continued', 0.7142857142857143), ('heading', 0.7), ('sideways', 0.6923076923076923), ('instant', 0.6911764705882353), ('whispered', 0.6363636363636364), ('enchanted', 0.6363636363636364), ('shouted', 0.625)]\n",
      "[('pulling', 0.8695652173913043), ('pull', 0.8222222222222222), ('swiftly', 0.7333333333333333), ('eager', 0.6923076923076923), ('oars', 0.6129032258064516), ('tiger', 0.6), ('ahead', 0.5555555555555556), ('yellow', 0.5454545454545454), ('event', 0.5454545454545454), ('oarsmen', 0.5454545454545454)]\n",
      "[('pains', 0.6666666666666666), ('doubloon', 0.631578947368421), ('sand', 0.6), ('signs', 0.6), ('nine', 0.5), ('foolish', 0.47058823529411764), ('bare', 0.4), ('mainmast', 0.4), ('nailed', 0.36363636363636365), ('virtue', 0.35)]\n",
      "[('wreck', 0.75), ('derick', 0.75), ('flank', 0.7), ('rapid', 0.6666666666666666), ('haul', 0.6666666666666666), ('lance', 0.6363636363636364), ('floats', 0.6363636363636364), ('german', 0.6), ('sinking', 0.5833333333333334), ('occasionally', 0.5714285714285714)]\n",
      "[('folio', 1.0), ('octavo', 1.0), ('iii', 1.0), (').--', 1.0), ('porpoise', 0.9473684210526315), ('ii', 0.9333333333333333), ('baleen', 0.9090909090909091), ('system', 0.75), ('cetology', 0.7), ('narwhale', 0.6363636363636364)]\n",
      "[('priest', 0.5), ('canoe', 0.45454545454545453), ('queequeg', 0.448), ('asked', 0.42857142857142855), ('aboard', 0.42857142857142855), ('dying', 0.42857142857142855), ('toil', 0.4166666666666667), ('e', 0.41379310344827586), ('mildly', 0.4), ('island', 0.36363636363636365)]\n",
      "[('radney', 1.0), ('steelkilt', 1.0), ('lakeman', 1.0), (\"?'\", 0.96875), ('\"\\'', 0.9411764705882353), ('gentlemen', 0.9210526315789473), (\"!'\", 0.8666666666666667), ('pumps', 0.8666666666666667), (\".'\", 0.8611111111111112), ('leak', 0.7142857142857143)]\n",
      "[('steak', 1.0), ('dat', 1.0), ('de', 0.9473684210526315), ('fleece', 0.9166666666666666), ('dish', 0.8), ('cook', 0.7636363636363637), ('eat', 0.6363636363636364), ('meat', 0.5833333333333334), ('den', 0.5384615384615384), ('dam', 0.5263157894736842)]\n",
      "[('...', 1.0), ('cuvier', 1.0), ('greenland', 0.9428571428571428), ('fat', 0.9285714285714286), ('naturalists', 0.9090909090909091), ('pursued', 0.9), ('scoresby', 0.8), ('derived', 0.8), ('pictures', 0.75), ('animal', 0.7058823529411765)]\n",
      "[('kick', 1.0), ('bunger', 1.0), ('carpenter', 0.7142857142857143), ('insult', 0.5833333333333334), ('legs', 0.5588235294117647), ('ivory', 0.5), ('leg', 0.4772727272727273), ('wise', 0.46153846153846156), ('bone', 0.43137254901960786), ('rage', 0.4)]\n",
      "[('pip', 0.8378378378378378), ('log', 0.7272727272727273), ('coffin', 0.6938775510204082), ('buoy', 0.6666666666666666), ('tambourine', 0.6363636363636364), ('manxman', 0.5833333333333334), ('jumped', 0.5454545454545454), ('coward', 0.5), ('happened', 0.5), ('indifferent', 0.36363636363636365)]\n",
      "[('possession', 0.8333333333333334), ('law', 0.6190476190476191), ('anybody', 0.6), ('queen', 0.5555555555555556), ('loose', 0.5428571428571428), ('gentleman', 0.45), ('upright', 0.4166666666666667), ('fast', 0.4032258064516129), ('lights', 0.4), ('royal', 0.3793103448275862)]\n",
      "[('um', 0.8333333333333334), ('.)', 0.7272727272727273), ('sailor', 0.5061728395061729), ('spanish', 0.5), ('row', 0.47058823529411764), ('leap', 0.46153846153846156), (').', 0.4), ('lads', 0.4), ('yon', 0.4), ('hurrah', 0.4)]\n",
      "[('-', 77), ('seemed', 60), ('night', 52), ('deck', 43), ('every', 27), ('still', 27), ('ahab', 24), ('till', 20), ('face', 20), ('upon', 20)]\n",
      "[('-', 345), ('sperm', 58), ('waters', 31), ('years', 29), ('present', 28), ('may', 26), ('three', 25), ('vast', 24), ('whales', 22), ('leviathan', 22)]\n",
      "[('queequeg', 57), ('arm', 26), ('--', 26), ('sat', 23), ('soon', 16), ('felt', 14), ('knew', 13), ('went', 12), ('strong', 12), ('pagan', 11)]\n",
      "[('-', 117), ('line', 68), ('one', 37), ('whale', 34), ('end', 30), ('sperm', 28), ('hands', 24), ('cutting', 21), ('part', 19), ('tackles', 16)]\n",
      "[('whaleman', 16), ('story', 15), ('considering', 13), ('st', 12), ('let', 10), ('george', 10), ('times', 10), ('perseus', 10), ('gives', 9), ('harbor', 9)]\n",
      "[('man', 26), ('brow', 19), ('god', 17), ('face', 14), ('dignity', 12), ('-', 12), ('regarded', 10), ('shall', 10), ('snow', 9), ('peculiar', 9)]\n",
      "[('queequeg', 21), ('?', 18), ('sharks', 17), ('ginger', 16), ('rope', 11), ('spades', 9), ('poor', 9), ('harpooneer', 8), ('monkey', 8), ('seemed', 7)]\n",
      "[('-', 145), ('would', 36), ('go', 27), ('little', 23), ('never', 22), ('ships', 19), ('perhaps', 19), ('cannot', 18), ('rather', 14), ('hard', 12)]\n",
      "[('boat', 70), ('whale', 49), ('fishery', 29), ('instances', 21), (':', 21), ('boats', 18), ('times', 16), (',', 14), ('know', 14), ('perils', 13)]\n",
      "[('whales', 70), ('us', 21), ('seemed', 20), ('young', 17), ('pequod', 16), ('straits', 12), ('herd', 11), ('broken', 9), ('known', 8), ('evinced', 8)]\n",
      "[('white', 55), ('whiteness', 27), ('though', 24), ('?', 18), ('even', 15), ('\"', 10), ('things', 10), ('whose', 9), ('bear', 9), ('name', 9)]\n",
      "[('bildad', 76), ('peleg', 73), ('ye', 43), ('captain', 28), ('queequeg', 22), ('thou', 20), ('never', 20), ('think', 17), ('lay', 16), ('well', 15)]\n",
      "[('ahab', 85), ('would', 45), ('white', 43), ('yet', 38), ('dick', 31), ('even', 29), ('crew', 28), ('moby', 28), ('times', 21), ('could', 20)]\n",
      "[('ahab', 30), ('flask', 29), ('cabin', 28), ('harpooneers', 26), ('deck', 21), ('stubb', 15), ('mates', 15), ('table', 14), ('dinner', 12), ('boy', 12)]\n",
      "[('!', 98), ('heart', 23), ('oh', 20), ('god', 18), ('mild', 14), ('!--', 14), ('away', 13), ('years', 13), ('wife', 12), ('sky', 11)]\n",
      "[(',', 18206), ('.', 6327), (';', 3673), (\"'\", 1232), ('one', 848), ('like', 569), ('upon', 492), ('--', 440), ('man', 349), ('though', 305)]\n",
      "[('sea', 73), ('ocean', 40), ('slowly', 22), ('calm', 16), ('waves', 15), ('thus', 14), ('ships', 13), ('arms', 13), ('like', 12), ('pacific', 12)]\n",
      "[('land', 25), ('gabriel', 20), ('new', 19), ('bedford', 15), ('letter', 11), ('green', 11), ('town', 11), ('-', 10), ('jeroboam', 10), ('passage', 9)]\n",
      "[('queequeg', 22), ('hussey', 16), ('mrs', 13), ('supper', 12), ('harpoon', 11), ('clam', 11), ('till', 8), ('us', 7), ('pots', 7), ('cod', 7)]\n",
      "[('whaling', 82), ('nantucket', 48), ('ship', 43), ('whalemen', 31), ('first', 29), ('many', 25), ('voyage', 24), ('fishery', 24), ('world', 20), ('ever', 19)]\n",
      "[('bed', 52), ('harpooneer', 43), ('landlord', 32), ('room', 30), ('-', 17), ('sleep', 17), ('.', 14), ('never', 14), ('table', 13), ('night', 13)]\n",
      "[('thou', 209), ('thee', 84), ('sir', 79), ('ahab', 69), ('thy', 68), ('oh', 33), ('st', 28), ('art', 27), ('eyes', 25), ('fire', 23)]\n",
      "[('-', 431), ('mast', 45), (';', 44), ('life', 31), ('sailor', 26), ('young', 22), ('new', 21), ('heads', 18), ('headed', 16), ('(', 16)]\n",
      "[(\"'\", 471), ('!', 448), ('?', 214), ('--', 135), ('ye', 114), ('old', 87), ('-', 79), (';', 61), ('stubb', 50), ('aye', 43)]\n",
      "[('certain', 16), ('thing', 16), ('nature', 16), ('though', 15), ('always', 14), ('ambergris', 13), ('times', 12), ('used', 11), ('found', 11), ('considering', 10)]\n",
      "[('tail', 30), ('flukes', 15), ('elephant', 11), ('sun', 11), ('east', 11), ('trunk', 10), ('middle', 9), ('else', 8), ('compasses', 8), ('upper', 7)]\n",
      "[('ship', 124), ('captain', 122), ('board', 32), ('two', 32), ('home', 24), ('-', 23), ('years', 23), ('ships', 21), ('sail', 21), ('--', 20)]\n",
      "[('feet', 36), ('skeleton', 27), ('skull', 17), ('length', 16), ('bones', 14), ('ribs', 13), ('spine', 12), ('living', 10), ('view', 9), ('brain', 9)]\n",
      "[('water', 31), ('air', 24), ('whether', 23), ('spout', 20), ('surface', 16), ('pipe', 15), ('say', 10), ('beneath', 9), ('vapour', 9), ('cannot', 9)]\n",
      "[('\"', 1100), (\"'\", 745), ('-', 483), ('.\"', 437), ('--', 329), (',\"', 225), ('?\"', 196), ('said', 172), ('?', 148), ('!', 134)]\n",
      "[('head', 88), ('whale', 70), ('side', 57), ('-', 52), ('sperm', 48), ('two', 38), ('jaw', 27), ('?', 26), ('lower', 23), ('right', 22)]\n",
      "[('little', 87), ('seemed', 78), ('-', 67), ('thought', 65), ('could', 45), ('night', 36), ('door', 35), ('towards', 33), ('good', 29), ('looked', 29)]\n",
      "[('captain', 94), (\"'\", 76), ('man', 59), ('know', 58), ('pequod', 48), ('ship', 46), ('good', 39), ('ye', 36), ('see', 35), ('old', 33)]\n",
      "[('pulpit', 14), ('skin', 14), ('yes', 9), ('chapel', 9), ('storm', 8), ('ladder', 7), ('black', 6), ('memory', 6), ('father', 6), ('marks', 6)]\n",
      "[('jonah', 73), ('god', 48), ('shipmates', 19), ('sea', 16), ('still', 16), ('woe', 15), ('delight', 13), ('ship', 12), ('lesson', 11), ('\"', 10)]\n",
      "[('oil', 47), ('works', 26), ('try', 22), ('ship', 19), ('-', 19), ('fire', 18), ('pequod', 15), ('lamp', 13), ('back', 12), ('pots', 10)]\n",
      "[('--', 77), ('whale', 26), ('blows', 21), ('air', 21), ('boats', 19), ('head', 18), ('leviathan', 15), ('sword', 15), ('!--', 15), ('rope', 14)]\n",
      "[('-', 312), ('\"', 283), ('ahab', 196), ('!', 167), ('!\"', 154), (';', 148), ('starbuck', 135), ('ye', 120), ('white', 103), ('cried', 77)]\n",
      "[('boat', 139), ('stubb', 102), ('boats', 64), ('-', 57), ('water', 40), ('like', 38), ('pull', 37), ('crew', 30), ('!\"', 29), ('flask', 29)]\n",
      "[('sun', 31), ('look', 26), ('comes', 12), ('doubloon', 12), ('pains', 10), ('gold', 9), ('poor', 9), ('signs', 9), ('read', 8), ('foolish', 8)]\n",
      "[('whale', 132), ('cried', 37), ('still', 36), ('lance', 28), ('blood', 24), ('chase', 24), (\"'\", 24), ('air', 21), ('fish', 19), ('vast', 19)]\n",
      "[('.', 107), ('-', 38), ('(', 35), ('whales', 34), ('fish', 27), ('book', 25), ('back', 18), ('porpoise', 18), ('),', 16), ('folio', 16)]\n",
      "[('queequeg', 112), ('going', 19), ('lay', 17), ('harpoon', 16), ('captain', 13), ('island', 12), ('e', 12), ('last', 11), ('house', 10), ('people', 10)]\n",
      "[('\"\\'', 64), ('steelkilt', 40), ('gentlemen', 35), (\"?'\", 31), (\".'\", 31), (\"!'\", 26), ('lakeman', 24), ('radney', 22), ('mate', 17), (\",'\", 16)]\n",
      "[('cook', 42), ('stubb', 36), ('de', 36), ('whale', 28), ('dat', 20), ('go', 19), ('steak', 15), ('sharks', 14), ('fleece', 11), ('supper', 10)]\n",
      "[('whale', 544), ('.', 148), ('whales', 97), ('sperm', 94), ('great', 63), ('leviathan', 49), ('(', 38), ('sea', 33), ('greenland', 33), ('right', 33)]\n",
      "[('leg', 42), ('carpenter', 35), ('(', 28), ('ivory', 28), (\"'\", 27), ('bone', 22), ('legs', 19), ('man', 17), ('queer', 17), ('says', 17)]\n",
      "[('pip', 62), ('coffin', 34), ('line', 24), ('poor', 16), ('log', 16), ('boy', 11), ('hammock', 10), ('happened', 8), ('buoy', 8), ('carpenter', 7)]\n",
      "[('fish', 60), ('-', 56), ('?', 34), ('fast', 25), ('lord', 19), ('loose', 19), ('law', 13), ('whale', 12), ('royal', 11), ('king', 10)]\n",
      "[('.', 102), ('!', 80), ('-', 46), ('sailor', 41), ('(', 36), ('.)', 16), (\"'\", 14), ('white', 12), ('thou', 11), (')', 10)]\n"
     ]
    }
   ],
   "source": [
    "counters = model3.evaluate()\n",
    "for counter in counters:\n",
    "  print(counter.most_common(10))\n",
    "\n",
    "counters = model3.evaluate(relative = False)\n",
    "for counter in counters:\n",
    "  print(counter.most_common(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cjzMSvEbff4A"
   },
   "source": [
    "##LDA 4: α = β = 0.01, 100 iterations, 50 categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sjrEFSWOGrQl"
   },
   "outputs": [],
   "source": [
    "model4 = train_model(0.01, 0.01, 100, 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n-q24eAffoeO"
   },
   "source": [
    "###Same as for previous model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tp4PwfB8HJXr",
    "outputId": "5257c10a-e071-4c1f-9273-df88416dd92b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('queen', 0.7777777777777778), ('law', 0.7619047619047619), ('possession', 0.75), ('gentleman', 0.75), ('ambergris', 0.6923076923076923), ('loose', 0.6285714285714286), ('humor', 0.6), ('blubber', 0.5882352941176471), ('hoisting', 0.5833333333333334), ('originally', 0.5294117647058824)]\n",
      "[('son', 0.75), ('customary', 0.6428571428571429), ('knowing', 0.625), ('receiving', 0.6153846153846154), ('thereby', 0.6111111111111112), ('earnest', 0.6), ('sweeping', 0.5833333333333334), ('cannibals', 0.5454545454545454), ('possible', 0.5333333333333333), ('quietly', 0.5263157894736842)]\n",
      "[('landlord', 0.9705882352941176), ('throwing', 0.9), ('bench', 0.8823529411764706), ('bed', 0.8266666666666667), ('crow', 0.7142857142857143), ('tomahawk', 0.6842105263157895), ('streets', 0.6), ('peter', 0.6), ('landed', 0.5833333333333334), ('idol', 0.5384615384615384)]\n",
      "[('baleen', 1.0), ('cuvier', 1.0), ('scoresby', 1.0), ('folio', 1.0), ('octavo', 1.0), ('iii', 1.0), (').--', 1.0), ('ribs', 0.9047619047619048), ('porpoise', 0.8421052631578947), ('greenland', 0.8)]\n",
      "[('tongue', 0.8181818181818182), ('seven', 0.75), ('forge', 0.75), ('crown', 0.7222222222222222), ('fashion', 0.6363636363636364), ('mother', 0.5882352941176471), ('guess', 0.5), ('crossed', 0.4666666666666667), ('swallow', 0.45454545454545453), ('flesh', 0.45)]\n",
      "[('sick', 1.0), ('monkey', 1.0), ('fleet', 1.0), ('civilized', 0.9375), ('deal', 0.8333333333333334), ('jumped', 0.8181818181818182), ('clean', 0.8125), ('heavens', 0.8125), ('church', 0.7857142857142857), ('connected', 0.75)]\n",
      "[('dat', 1.0), ('steak', 0.9333333333333333), ('fleece', 0.9166666666666666), ('spades', 0.9), ('dish', 0.8), ('cook', 0.7454545454545455), ('sharks', 0.7073170731707317), ('strip', 0.7), ('heaving', 0.6666666666666666), ('meat', 0.6666666666666666)]\n",
      "[('elijah', 0.8666666666666667), ('capstan', 0.8333333333333334), ('perth', 0.8333333333333334), ('haul', 0.75), ('buoy', 0.75), ('pointing', 0.7222222222222222), ('finger', 0.6923076923076923), ('barbs', 0.6666666666666666), ('chains', 0.6666666666666666), ('shouted', 0.625)]\n",
      "[('ramadan', 1.0), ('guernsey', 1.0), ('gabriel', 0.95), ('jeroboam', 0.9), ('letter', 0.6428571428571429), ('foul', 0.6), ('hast', 0.5714285714285714), ('replied', 0.5454545454545454), ('seized', 0.5294117647058824), ('happened', 0.5)]\n",
      "[('needle', 0.9090909090909091), ('compasses', 0.9), ('loud', 0.8333333333333334), ('meeting', 0.8333333333333334), ('needles', 0.8181818181818182), ('murmured', 0.8), ('limbs', 0.7), ('gay', 0.6666666666666666), ('using', 0.6363636363636364), ('trowsers', 0.6153846153846154)]\n",
      "[('forming', 1.0), ('scientific', 1.0), ('system', 0.9166666666666666), ('simply', 0.9), ('rock', 0.875), ('surface', 0.8717948717948718), ('hitherto', 0.8235294117647058), ('monstrous', 0.8181818181818182), ('written', 0.8181818181818182), ('fins', 0.8)]\n",
      "[('smells', 0.75), ('hemp', 0.7272727272727273), ('terror', 0.7142857142857143), ('skies', 0.7), ('pip', 0.6891891891891891), ('knowledge', 0.6666666666666666), ('drowned', 0.5833333333333334), ('exclaimed', 0.5333333333333333), ('fain', 0.5), ('bare', 0.5)]\n",
      "[('owners', 0.85), ('officers', 0.8), ('moody', 0.6923076923076923), ('watches', 0.6666666666666666), ('boy', 0.6451612903225806), ('beware', 0.6428571428571429), ('contrary', 0.6363636363636364), ('fiery', 0.6086956521739131), ('japanese', 0.6), ('hearted', 0.5833333333333334)]\n",
      "[('radney', 1.0), ('steelkilt', 1.0), ('lakeman', 1.0), (\"?'\", 0.90625), (\",'\", 0.8125), (\".'\", 0.8055555555555556), (\"!'\", 0.8), ('kick', 0.7272727272727273), ('lima', 0.7), ('shipmate', 0.6666666666666666)]\n",
      "[('yon', 1.0), ('doubloon', 1.0), ('heat', 0.8666666666666667), ('equator', 0.8181818181818182), ('gold', 0.717948717948718), ('manxman', 0.6666666666666666), ('poles', 0.6363636363636364), ('nine', 0.6), ('calls', 0.5833333333333334), ('tis', 0.5652173913043478)]\n",
      "[('prairie', 1.0), ('hue', 0.8571428571428571), ('experience', 0.8), ('figure', 0.7368421052631579), ('milk', 0.7272727272727273), ('vice', 0.6363636363636364), ('nameless', 0.6111111111111112), ('nearest', 0.6), ('murderous', 0.6), ('faith', 0.6)]\n",
      "[('depth', 1.0), ('mountain', 1.0), ('double', 0.8666666666666667), ('enormous', 0.8461538461538461), ('tub', 0.8461538461538461), ('immediately', 0.8235294117647058), ('valiant', 0.8), ('divided', 0.8), ('utmost', 0.7857142857142857), ('ball', 0.75)]\n",
      "[('jonah', 0.9294117647058824), ('shipmates', 0.8571428571428571), ('lesson', 0.8461538461538461), ('delight', 0.6111111111111112), ('berth', 0.6), ('prophet', 0.4375), ('wharf', 0.36363636363636365), ('feels', 0.3076923076923077), ('friends', 0.29411764705882354), ('concluding', 0.2727272727272727)]\n",
      "[('hurrah', 0.9), ('boys', 0.6857142857142857), ('row', 0.6470588235294118), ('cries', 0.625), ('squall', 0.6), ('tiger', 0.6), ('beat', 0.5454545454545454), ('afraid', 0.5333333333333333), ('lads', 0.5333333333333333), ('big', 0.46153846153846156)]\n",
      "[('\"\\'', 0.9264705882352942), ('gentlemen', 0.9210526315789473), ('knives', 0.8), ('leak', 0.7857142857142857), ('pictures', 0.6875), ('keeping', 0.6153846153846154), ('seaman', 0.6153846153846154), ('concern', 0.6), ('murder', 0.6), ('affair', 0.5882352941176471)]\n",
      "[('hunt', 0.7083333333333334), ('front', 0.68), ('external', 0.5833333333333334), ('parsee', 0.5416666666666666), ('vague', 0.5), ('moby', 0.4936708860759494), ('morrow', 0.45454545454545453), ('ear', 0.4166666666666667), ('threw', 0.4), ('charge', 0.4)]\n",
      "[('need', 0.7692307692307693), ('notice', 0.7272727272727273), ('aboard', 0.6666666666666666), ('knows', 0.6428571428571429), ('smell', 0.6363636363636364), ('carefully', 0.6363636363636364), ('father', 0.5925925925925926), ('arrived', 0.5714285714285714), ('thinking', 0.5555555555555556), ('upright', 0.5)]\n",
      "[('hussey', 1.0), ('mrs', 1.0), ('clam', 0.9090909090909091), ('perseus', 0.9090909090909091), ('school', 0.9), ('george', 0.9), ('cod', 0.6923076923076923), ('venerable', 0.6), ('entitled', 0.6), ('derick', 0.5833333333333334)]\n",
      "[('fowls', 1.0), ('commotion', 1.0), ('erect', 0.9230769230769231), ('outs', 0.9090909090909091), ('summit', 0.9), ('firmly', 0.8333333333333334), ('continually', 0.8125), ('wound', 0.7647058823529411), ('simultaneously', 0.75), ('rapid', 0.75)]\n",
      "[('islands', 1.0), ('hoops', 1.0), ('quiet', 1.0), ('wish', 0.9230769230769231), ('noise', 0.9166666666666666), ('trouble', 0.9090909090909091), ('midst', 0.9), ('terms', 0.8333333333333334), ('damn', 0.8), ('fastened', 0.8)]\n",
      "[('commodore', 1.0), ('endless', 0.7857142857142857), ('narrative', 0.75), ('whaler', 0.7368421052631579), ('glory', 0.7222222222222222), ('ignorant', 0.7), ('howling', 0.6666666666666666), ('news', 0.6363636363636364), ('absent', 0.6), ('miles', 0.59375)]\n",
      "[('observe', 0.7272727272727273), ('devil', 0.6428571428571429), ('fetch', 0.6363636363636364), ('considering', 0.6129032258064516), ('suppose', 0.5675675675675675), ('stuff', 0.5555555555555556), ('stubb', 0.5060728744939271), ('descending', 0.5), ('halloa', 0.5), ('nearer', 0.5)]\n",
      "[('shadows', 0.8), ('beholding', 0.8), ('brow', 0.6), ('methinks', 0.5), ('blinds', 0.5), ('wrinkles', 0.4666666666666667), ('snow', 0.4666666666666667), ('critical', 0.46153846153846156), ('birds', 0.45454545454545453), ('whispered', 0.45454545454545453)]\n",
      "[('london', 1.0), ('host', 0.9), ('straits', 0.875), ('herd', 0.8461538461538461), ('nations', 0.8333333333333334), ('temple', 0.8181818181818182), ('spouts', 0.8125), ('john', 0.7692307692307693), ('vessels', 0.7692307692307693), ('roll', 0.7647058823529411)]\n",
      "[('thine', 0.8333333333333334), ('split', 0.7692307692307693), ('canst', 0.75), ('rod', 0.7333333333333333), ('greater', 0.6923076923076923), ('lightning', 0.6785714285714286), ('fate', 0.6666666666666666), ('memory', 0.6363636363636364), ('sing', 0.6363636363636364), ('swear', 0.5833333333333334)]\n",
      "[('sleet', 0.9333333333333333), ('early', 0.7272727272727273), ('appeared', 0.7), ('image', 0.6363636363636364), ('worship', 0.6363636363636364), ('pay', 0.5882352941176471), ('coat', 0.5714285714285714), ('taste', 0.5454545454545454), ('silent', 0.5384615384615384), ('thither', 0.5)]\n",
      "[('bedford', 0.8888888888888888), ('um', 0.8333333333333334), ('.)', 0.6818181818181818), (').', 0.6666666666666666), ('summer', 0.6), ('bell', 0.6), ('spanish', 0.5833333333333334), ('ladies', 0.5384615384615384), ('country', 0.48), ('sailor', 0.4691358024691358)]\n",
      "[('accursed', 1.0), ('enchanted', 1.0), ('torn', 0.9230769230769231), ('sank', 0.9230769230769231), ('windward', 0.9047619047619048), ('lake', 0.9), ('narrow', 0.75), ('paused', 0.75), ('pacing', 0.7272727272727273), ('lo', 0.7058823529411765)]\n",
      "[('pull', 1.0), ('irons', 1.0), ('wreck', 1.0), ('gunwale', 1.0), ('oarsmen', 1.0), ('leeward', 0.96), ('bunger', 0.9230769230769231), ('pulling', 0.9130434782608695), ('oars', 0.8709677419354839), ('continued', 0.8571428571428571)]\n",
      "[('warp', 0.9230769230769231), ('throughout', 0.8), ('forced', 0.6666666666666666), ('nights', 0.6363636363636364), ('contact', 0.6363636363636364), ('heaved', 0.5833333333333334), ('care', 0.5714285714285714), ('devils', 0.5294117647058824), ('slightest', 0.5), ('crossing', 0.5)]\n",
      "[('ginger', 1.0), ('headsman', 0.9), ('indifferent', 0.8181818181818182), ('nest', 0.8181818181818182), ('leaped', 0.75), ('event', 0.7272727272727273), ('usage', 0.6666666666666666), ('tambourine', 0.6363636363636364), ('steward', 0.6111111111111112), ('handed', 0.6)]\n",
      "[('clothes', 0.8181818181818182), ('coffin', 0.7346938775510204), ('dim', 0.7222222222222222), ('grey', 0.7), ('queequeg', 0.676), ('hearing', 0.6666666666666666), ('troubled', 0.6666666666666666), ('hammock', 0.5454545454545454), ('seeing', 0.5), ('surprise', 0.5)]\n",
      "[('tiller', 0.9230769230769231), ('binnacle', 0.8125), ('works', 0.8064516129032258), ('bringing', 0.8), ('sit', 0.75), ('compass', 0.7058823529411765), ('removed', 0.7), ('east', 0.6666666666666666), ('additional', 0.6666666666666666), ('sunrise', 0.6363636363636364)]\n",
      "[('dying', 1.0), ('circles', 1.0), ('flew', 0.8), ('showed', 0.8), ('desired', 0.7272727272727273), ('height', 0.7222222222222222), ('spite', 0.7058823529411765), ('gradually', 0.7), ('closed', 0.7), ('higher', 0.6956521739130435)]\n",
      "[('whiteness', 1.0), ('encounter', 0.8666666666666667), ('instances', 0.7096774193548387), ('probably', 0.6666666666666666), ('hidden', 0.6521739130434783), ('firm', 0.6428571428571429), ('herds', 0.6363636363636364), ('perils', 0.625), ('forty', 0.6060606060606061), ('leagues', 0.6)]\n",
      "[('fat', 1.0), ('tun', 1.0), ('samuel', 0.9090909090909091), ('000', 0.9), ('spermaceti', 0.8421052631578947), ('dutch', 0.7692307692307693), ('street', 0.6923076923076923), ('eating', 0.6666666666666666), ('city', 0.6428571428571429), ('named', 0.6363636363636364)]\n",
      "[('trunk', 0.875), ('window', 0.8666666666666667), ('floor', 0.8), ('element', 0.7272727272727273), ('pity', 0.7), ('blanket', 0.6923076923076923), ('marvellous', 0.6666666666666666), ('neck', 0.625), ('hanging', 0.6), ('motions', 0.5833333333333334)]\n",
      "[('rib', 0.9230769230769231), ('chanced', 0.8181818181818182), ('picked', 0.8), ('flask', 0.7735849056603774), ('dinner', 0.7222222222222222), ('skull', 0.7083333333333334), ('flukes', 0.7027027027027027), ('mildly', 0.7), ('dough', 0.6470588235294118), ('skeleton', 0.6470588235294118)]\n",
      "[('quest', 1.0), ('bull', 0.9285714285714286), ('naturally', 0.9166666666666666), ('stock', 0.9), ('birth', 0.9), ('kings', 0.8333333333333334), ('northern', 0.8181818181818182), ('needs', 0.8), ('consternation', 0.8), ('minds', 0.7857142857142857)]\n",
      "[('monomaniac', 0.9090909090909091), ('glad', 0.8333333333333334), ('intent', 0.75), ('served', 0.7), ('blacksmith', 0.6666666666666666), ('unseen', 0.6428571428571429), ('spoken', 0.6153846153846154), ('souls', 0.5454545454545454), ('carpenter', 0.5306122448979592), ('lances', 0.47368421052631576)]\n",
      "[('naturalists', 1.0), ('ii', 1.0), ('mostly', 0.9090909090909091), ('feeding', 0.9), ('food', 0.8461538461538461), ('scenes', 0.8181818181818182), ('pursued', 0.8), ('content', 0.7777777777777778), ('grounds', 0.7692307692307693), ('object', 0.7435897435897436)]\n",
      "[('...', 1.0), ('entered', 0.7857142857142857), ('fashioned', 0.75), ('jack', 0.6428571428571429), ('voyages', 0.5555555555555556), ('covered', 0.5454545454545454), ('armed', 0.5333333333333333), ('space', 0.5333333333333333), ('thinks', 0.5263157894736842), ('afternoon', 0.5)]\n",
      "[('signs', 0.8666666666666667), ('spiritual', 0.8181818181818182), ('aught', 0.8181818181818182), ('pulpit', 0.8125), ('chapel', 0.8), ('woe', 0.7096774193548387), ('fierce', 0.7), ('terrors', 0.6666666666666666), ('sacred', 0.6), ('ladder', 0.5833333333333334)]\n",
      "[('bucket', 1.0), ('eastward', 1.0), ('tashtego', 0.6538461538461539), ('lively', 0.6428571428571429), ('breadth', 0.6), ('reaching', 0.5833333333333334), ('unearthly', 0.5833333333333334), ('filled', 0.5454545454545454), ('swiftly', 0.5333333333333333), ('daggoo', 0.5142857142857142)]\n",
      "[('bildad', 1.0), ('peleg', 0.9864864864864865), ('quaker', 0.9), ('service', 0.8181818181818182), ('bye', 0.8), ('papers', 0.7), ('eh', 0.6842105263157895), ('yojo', 0.6470588235294118), ('pious', 0.6), ('trying', 0.5625)]\n",
      "[('-', 112), ('?', 55), ('fish', 52), ('whale', 33), ('two', 30), ('fast', 29), ('line', 29), (\"'\", 27), ('loose', 22), ('english', 21)]\n",
      "[('seemed', 48), ('queequeg', 43), ('water', 33), ('harpoon', 28), ('seen', 25), ('still', 21), ('hands', 20), (',--', 17), ('table', 17), ('say', 16)]\n",
      "[(\"'\", 73), ('bed', 62), ('landlord', 33), ('thought', 32), ('harpooneer', 30), ('little', 26), ('room', 25), ('much', 23), ('sleep', 23), ('back', 20)]\n",
      "[('whale', 392), ('whales', 77), ('(', 70), ('sea', 60), ('leviathan', 42), ('-', 39), ('right', 39), ('book', 37), ('back', 37), ('among', 31)]\n",
      "[('ye', 143), ('?', 96), ('old', 81), ('thou', 80), ('see', 40), ('well', 39), ('three', 38), ('look', 35), ('right', 33), ('hard', 30)]\n",
      "[('good', 55), ('many', 51), ('way', 45), ('tell', 43), ('harpooneer', 39), ('boat', 35), ('wild', 33), ('think', 31), ('come', 31), ('turn', 31)]\n",
      "[('cook', 41), ('stubb', 32), ('sharks', 29), ('hands', 27), ('go', 27), ('one', 26), ('de', 25), ('dat', 20), ('night', 18), ('say', 18)]\n",
      "[('\"', 552), ('.\"', 238), ('?\"', 141), (',\"', 105), ('said', 101), ('!\"', 69), ('sir', 35), ('one', 28), ('near', 26), ('half', 24)]\n",
      "[('\"', 74), ('captain', 56), ('man', 51), ('arm', 38), ('cried', 30), ('well', 29), ('time', 25), ('pequod', 24), ('hand', 23), ('aye', 20)]\n",
      "[('ahab', 99), ('seemed', 73), ('every', 51), ('crew', 49), ('long', 47), ('yet', 46), ('would', 45), ('starbuck', 45), ('thought', 36), ('pequod', 36)]\n",
      "[(';', 261), ('great', 86), ('may', 84), (':', 49), ('life', 44), ('part', 41), ('shall', 40), ('far', 39), ('surface', 34), ('deep', 33)]\n",
      "[(';', 417), (\"'\", 155), ('.', 83), ('though', 66), ('pip', 51), ('!', 37), ('first', 34), ('ship', 33), ('line', 29), ('god', 28)]\n",
      "[('ahab', 146), ('captain', 95), ('-', 92), ('one', 65), ('ship', 51), ('cabin', 47), ('little', 46), ('come', 46), ('pequod', 42), ('never', 40)]\n",
      "[('ye', 53), ('steelkilt', 40), (\".'\", 29), (\"?'\", 29), (\",'\", 26), ('--', 24), (\"!'\", 24), ('lakeman', 24), ('radney', 22), ('captain', 20)]\n",
      "[('!', 272), ('sir', 72), ('look', 54), ('.\"', 51), ('thy', 46), ('sun', 44), ('oh', 42), ('old', 32), ('life', 31), ('let', 29)]\n",
      "[(',', 4783), ('.', 1221), (';', 510), ('-', 342), ('like', 160), ('one', 131), ('upon', 107), ('whale', 76), ('must', 69), ('round', 60)]\n",
      "[('whale', 244), ('two', 115), ('head', 68), ('line', 56), ('three', 46), ('would', 43), ('end', 41), ('feet', 36), ('thus', 32), ('entire', 31)]\n",
      "[('jonah', 79), ('god', 35), ('shipmates', 24), ('lesson', 11), ('us', 11), ('lord', 11), ('delight', 11), ('goes', 9), ('\"', 9), ('deep', 7)]\n",
      "[('white', 82), ('ye', 81), ('!', 60), ('--', 51), ('boys', 24), ('eyes', 23), ('captain', 21), ('(', 19), ('?', 18), ('darted', 13)]\n",
      "[(';', 101), ('\"\\'', 63), ('would', 37), ('gentlemen', 35), ('mate', 24), ('day', 21), ('though', 20), ('night', 18), ('take', 17), ('heart', 16)]\n",
      "[(\"'\", 344), ('\"', 80), ('.\"', 50), ('moby', 39), (',--', 27), ('thing', 27), (',\"', 25), ('--', 24), ('dick', 22), ('mast', 21)]\n",
      "[(';', 600), (\"'\", 589), ('--', 202), ('.', 160), ('?', 111), ('!', 51), ('say', 50), ('look', 44), ('let', 41), ('ship', 41)]\n",
      "[('great', 30), ('us', 23), ('st', 18), ('hussey', 17), ('leviathan', 16), ('bones', 13), ('mrs', 13), ('young', 12), ('whaleman', 11), ('story', 10)]\n",
      "[('-', 114), ('air', 80), ('us', 78), ('water', 78), ('long', 76), ('sperm', 76), ('side', 67), ('every', 63), ('spout', 40), ('body', 33)]\n",
      "[(',', 9264), ('.', 3185), (';', 1506), (\"'\", 1326), ('-', 961), ('like', 283), ('\"', 260), ('!', 240), ('upon', 217), ('one', 183)]\n",
      "[('ship', 113), ('whale', 84), ('captain', 66), ('ships', 47), ('world', 41), ('even', 39), ('voyage', 37), ('fishery', 37), ('whaling', 35), ('men', 30)]\n",
      "[('stubb', 125), ('man', 107), ('whale', 107), ('head', 57), (',\"', 54), ('though', 53), ('sort', 42), ('devil', 36), ('see', 32), ('said', 32)]\n",
      "[('!', 76), ('starbuck', 51), ('\"', 29), ('man', 28), ('brow', 24), ('god', 23), ('thou', 20), ('water', 18), ('snow', 14), ('see', 14)]\n",
      "[('whales', 149), ('one', 99), ('time', 75), ('three', 59), ('years', 48), ('(', 39), ('last', 37), ('present', 33), ('also', 31), ('say', 31)]\n",
      "[('?', 117), ('thou', 114), ('ye', 107), ('!', 98), ('aye', 78), ('thee', 63), ('sea', 48), ('oh', 48), ('deck', 42), ('ahab', 42)]\n",
      "[('time', 36), ('go', 35), ('take', 33), ('strange', 30), ('heads', 26), ('head', 24), ('sight', 22), ('reason', 22), ('old', 20), ('part', 20)]\n",
      "[('!', 186), ('-', 119), (\"'\", 93), ('(', 44), ('sailor', 38), ('?', 27), ('--', 24), ('!--', 23), ('queer', 19), ('see', 19)]\n",
      "[('ahab', 175), ('--', 152), ('!', 144), ('\"', 125), ('!\"', 108), ('white', 86), ('starbuck', 77), ('!--', 67), ('stubb', 58), ('seen', 51)]\n",
      "[('boat', 242), ('whale', 139), ('boats', 109), ('cried', 84), ('ship', 73), ('men', 68), ('\"', 63), ('crew', 61), ('pull', 45), ('pequod', 44)]\n",
      "[(',', 4219), ('.', 1127), ('-', 446), ('like', 130), ('upon', 129), ('must', 71), ('round', 55), ('whale', 49), ('without', 39), ('still', 38)]\n",
      "[('-', 213), ('little', 45), ('poor', 43), ('mast', 40), ('sea', 39), ('hand', 31), ('last', 29), ('head', 27), ('almost', 26), ('half', 25)]\n",
      "[('queequeg', 169), ('us', 45), ('coffin', 36), ('going', 34), ('last', 28), ('nothing', 28), ('lay', 27), ('strange', 26), ('morning', 25), ('little', 24)]\n",
      "[('.', 125), ('thing', 36), ('try', 32), ('man', 30), ('night', 29), ('every', 26), ('works', 25), ('fire', 23), ('would', 22), ('pipe', 21)]\n",
      "[('sea', 91), ('still', 80), ('last', 74), ('seemed', 63), ('yet', 52), ('side', 40), ('away', 37), ('deck', 37), ('face', 36), ('whose', 25)]\n",
      "[('yet', 73), ('?', 68), ('sea', 52), ('man', 44), ('much', 31), ('waters', 30), ('far', 29), ('even', 29), ('may', 28), ('whiteness', 27)]\n",
      "[('whale', 67), ('--', 65), ('say', 39), ('whalemen', 28), ('head', 27), ('sperm', 27), ('great', 23), ('fine', 22), ('english', 21), ('dutch', 20)]\n",
      "[('.', 223), ('one', 136), ('head', 84), ('man', 81), ('would', 62), ('-', 55), ('though', 51), ('could', 47), ('night', 47), ('never', 44)]\n",
      "[('flask', 82), ('--', 49), ('tail', 47), ('time', 36), ('away', 30), ('flukes', 26), ('say', 25), ('air', 24), ('skeleton', 22), ('king', 19)]\n",
      "[('though', 98), ('ever', 70), ('yet', 67), ('whaling', 64), ('among', 61), ('long', 60), ('many', 60), ('might', 53), ('would', 51), ('could', 44)]\n",
      "[(';', 367), ('man', 40), ('day', 31), ('see', 26), ('carpenter', 26), ('great', 23), ('ship', 23), ('beneath', 21), ('things', 19), ('whole', 19)]\n",
      "[('.', 558), (';', 187), ('sperm', 141), ('would', 67), ('one', 62), ('oil', 57), ('still', 46), ('even', 44), ('almost', 43), ('thing', 41)]\n",
      "[('--', 386), ('\"', 112), ('sea', 76), ('one', 60), ('saw', 53), ('ever', 50), ('came', 43), ('said', 41), ('two', 39), ('first', 36)]\n",
      "[('upon', 41), ('man', 40), ('still', 37), ('old', 32), ('certain', 28), ('life', 25), ('death', 24), ('woe', 22), ('whose', 21), ('body', 21)]\n",
      "[('like', 55), ('sea', 41), ('tashtego', 34), ('fish', 32), ('indian', 27), ('ships', 22), ('three', 21), ('seamen', 20), ('slowly', 20), ('queequeg', 18)]\n",
      "[('\"', 118), ('bildad', 76), ('peleg', 73), ('captain', 65), ('?\"', 53), ('.\"', 47), ('ship', 46), ('good', 25), ('nothing', 21), ('say', 21)]\n"
     ]
    }
   ],
   "source": [
    "counters = model4.evaluate()\n",
    "for counter in counters:\n",
    "  print(counter.most_common(10))\n",
    "\n",
    "  counters = model4.evaluate(relative = False)\n",
    "for counter in counters:\n",
    "  print(counter.most_common(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2wyyaf1Gf1WX"
   },
   "source": [
    "##Investigating model 1 again but now with 200 iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lGUslKrL28e_"
   },
   "outputs": [],
   "source": [
    "model_200it = train_model(0.1, 0.1, 200, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fmZnsGMagGEC"
   },
   "source": [
    "###Relative frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "14yaEtfRVdOk",
    "outputId": "c3ec7948-603f-4953-9d1f-ef9c74135e45"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('stripped', 1.0), ('throughout', 1.0), ('external', 1.0), ('depth', 1.0), ('tun', 1.0), ('bulk', 0.96875), ('degree', 0.9615384615384616), ('surface', 0.9487179487179487), ('spine', 0.9411764705882353), ('immense', 0.9375)]\n",
      "[(\",'\", 1.0), (\".'\", 1.0), ('leak', 1.0), ('radney', 1.0), ('steelkilt', 1.0), ('lakeman', 1.0), (\"?'\", 0.96875), (\"!'\", 0.9666666666666667), ('pumps', 0.9333333333333333), ('jonah', 0.9294117647058824)]\n",
      "[('english', 1.0), ('cetology', 1.0), ('greenland', 1.0), ('baleen', 1.0), ('southern', 1.0), ('london', 1.0), ('john', 1.0), ('cuvier', 1.0), ('scoresby', 1.0), ('species', 1.0)]\n",
      "[('hussey', 1.0), ('sick', 1.0), ('throwing', 1.0), ('landlord', 1.0), ('clam', 1.0), ('tomahawk', 1.0), ('chapel', 1.0), ('notice', 1.0), ('mrs', 1.0), ('yojo', 1.0)]\n",
      "[('compasses', 1.0), ('phantom', 1.0), ('dream', 1.0), ('sleet', 1.0), ('fate', 1.0), ('spiritual', 1.0), ('continual', 1.0), ('leaned', 1.0), ('binnacle', 1.0), ('milky', 1.0)]\n",
      "[('towing', 1.0), ('instantly', 1.0), ('glided', 1.0), ('block', 1.0), ('oars', 1.0), ('wreck', 1.0), ('gunwale', 1.0), ('oarsmen', 1.0), ('haul', 1.0), ('derick', 1.0)]\n",
      "[('yon', 1.0), ('methinks', 1.0), ('pip', 1.0), ('whiteness', 0.9629629629629629), ('tambourine', 0.9090909090909091), ('mild', 0.8928571428571429), ('naught', 0.8571428571428571), ('dying', 0.8571428571428571), ('mountains', 0.8461538461538461), ('bedford', 0.8333333333333334)]\n",
      "[('eat', 1.0), ('steak', 1.0), ('dat', 1.0), ('guernsey', 1.0), ('dinner', 0.9444444444444444), ('ginger', 0.9375), ('fleece', 0.9166666666666666), ('outward', 0.9090909090909091), ('headsman', 0.9), ('dish', 0.9)]\n",
      "[('kick', 1.0), ('eh', 1.0), ('capstan', 1.0), ('forge', 1.0), ('um', 1.0), ('perth', 1.0), ('blacksmith', 0.9523809523809523), ('lightning', 0.9285714285714286), ('buoy', 0.9166666666666666), ('thyself', 0.9090909090909091)]\n",
      "[('sit', 1.0), ('empty', 1.0), ('clear', 1.0), ('thing', 1.0), ('received', 1.0), ('opening', 1.0), ('twelve', 1.0), ('visit', 1.0), ('everything', 1.0), ('ball', 1.0)]\n"
     ]
    }
   ],
   "source": [
    "counters_200 = model_200it.evaluate()\n",
    "for counter in counters_200:\n",
    "  print(counter.most_common(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ItpUJOs4gJMw"
   },
   "source": [
    "###Total frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IwN4v_SxkbG5",
    "outputId": "60a90c5e-68da-4be0-903e-e9ab5aec7224"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('whale', 221), ('sperm', 117), ('.', 101), ('head', 88), ('feet', 62), ('great', 58), ('side', 45), ('tail', 44), ('much', 43), ('two', 42)]\n",
      "[('jonah', 79), (\"'\", 72), ('\"\\'', 63), ('captain', 62), ('steelkilt', 40), (\".'\", 36), ('gentlemen', 33), (\",'\", 32), (\"?'\", 31), (\"!'\", 29)]\n",
      "[('whale', 605), ('.', 255), ('whales', 216), ('fish', 121), ('(', 110), ('-', 103), ('sperm', 99), ('?', 98), ('great', 80), ('\"', 80)]\n",
      "[('queequeg', 206), ('\"', 169), ('said', 120), ('captain', 113), ('thought', 90), ('little', 80), ('bildad', 76), ('good', 75), ('peleg', 74), (',\"', 72)]\n",
      "[('ahab', 314), (';', 284), ('white', 168), ('seemed', 143), ('starbuck', 135), (',', 123), ('sea', 110), ('men', 99), ('deck', 85), ('still', 84)]\n",
      "[('boat', 267), ('whale', 212), ('!\"', 136), ('line', 119), ('boats', 117), ('\"', 99), ('one', 96), ('!', 94), ('stubb', 91), ('cried', 83)]\n",
      "[('!', 511), ('?', 116), ('.', 86), ('pip', 74), ('oh', 68), ('sailor', 59), ('look', 53), ('--', 49), ('god', 47), ('ye', 45)]\n",
      "[('-', 131), ('stubb', 57), ('cook', 49), ('get', 46), ('whale', 44), ('deck', 38), ('flask', 37), ('hands', 35), ('de', 33), ('pequod', 31)]\n",
      "[(\"'\", 1145), ('\"', 1063), ('!', 418), ('.\"', 417), ('--', 334), ('ye', 334), ('?', 300), ('?\"', 216), ('thou', 214), (';', 161)]\n",
      "[(',', 18097), ('.', 6273), (';', 3480), ('-', 2132), (\"'\", 1322), ('one', 792), ('--', 572), ('like', 531), ('upon', 510), ('would', 412)]\n"
     ]
    }
   ],
   "source": [
    "counters_200_abs = model_200it.evaluate(relative=False)\n",
    "for counter in counters_200_abs:\n",
    "  print(counter.most_common(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r-MZjv_vgM8D"
   },
   "source": [
    "#Hidden markov transition model\n",
    "This class definition is similar to the one used for our LDA model, but some tweaks have been made and more logic has been added."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "azF0hsdHKgr1"
   },
   "outputs": [],
   "source": [
    "class HMTM:\n",
    "\n",
    "    def __init__(self, docs, cats, n_classes, voc, alpha, beta, pi):\n",
    "        # The document we are analysing\n",
    "        self.docs = docs\n",
    "        # All the tokens stored in a array of lists\n",
    "        self.w = np.array(docs)\n",
    "        # The category for each word\n",
    "        self.z = np.array(cats)\n",
    "        # Number of categories we are looking for\n",
    "        self.K = n_classes\n",
    "        # The words in our corpus\n",
    "        self.voc = voc\n",
    "        # Number of words in our corpus\n",
    "        self.V = len(voc)\n",
    "        # Hyper parameters used for Gibb's sampling\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.pi = pi\n",
    "\n",
    "        \"\"\"\n",
    "        Instead of counting these we just store the value in a matrix, this\n",
    "        makes to code a bit more optimized since we do not have to allocate or\n",
    "        do this computation over and over agian.\n",
    "        \"\"\"\n",
    "        # Category transition distribution for each doc (chapter)\n",
    "        self.n_trans = np.zeros( [len(docs), n_classes, n_classes] )\n",
    "        # Category distribution for starting words over all docs\n",
    "        self.n_start = np.zeros([n_classes])\n",
    "        # Category distribution for each word\n",
    "        self.m = np.zeros( [len(voc), n_classes] )\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    This function fits our model to the data by iterating a collapsed\n",
    "    Gibb's sampling n times\n",
    "    \"\"\"\n",
    "    def fit(self, n_iteration):\n",
    "        \n",
    "        print(\"Fitting the model\")\n",
    "        for it in range(n_iteration):\n",
    "            print(\"Iteration %i\" %(it+1))\n",
    "            self.iterate()\n",
    "\n",
    "    \"\"\"\n",
    "    This function performs a single iteration in our algorithm\n",
    "    \"\"\"\n",
    "    def iterate(self):\n",
    "        \n",
    "        # Allocating these now to optimize the code\n",
    "        q = np.zeros(self.K)\n",
    "        p = np.zeros(self.K)\n",
    "\n",
    "        for d, chapter in enumerate(self.docs):\n",
    "            for j, word in enumerate(chapter):\n",
    "                word_ind = self.voc.index(word)\n",
    "                \n",
    "                if j == 0:\n",
    "                    #Remove current token, update m, n_trans, n_start\n",
    "                    k_next = self.z[d][j+1]\n",
    "                    if self.z[d][j] != -1:\n",
    "                        k = self.z[d][j]\n",
    "                        self.n_trans[d, k, k_next] -= 1\n",
    "                        self.n_start[k] -= 1\n",
    "                        self.m[word_ind, k] -= 1\n",
    "                        self.z[d][j] = -1 \n",
    "                \n",
    "                    # computes the posterior distribution for the category distribution\n",
    "                    for k in range(self.K):\n",
    "                        sum_m_k = np.sum(self.m[:, k]) \n",
    "                        q[k] = ( self.alpha + self.n_start[k] ) * \\\n",
    "                                ( self.beta + self.m[word_ind, k] ) / \\\n",
    "                                (self.V * self.beta + sum_m_k ) \n",
    "                        # Normalizing\n",
    "                        p = q / np.sum(q)\n",
    "                \n",
    "                elif j == len(chapter)-1:\n",
    "                    #Remove current token, update m, n_trans\n",
    "                    k_prev = self.z[d][j-1]\n",
    "                    if self.z[d][j] != -1:\n",
    "                        k = self.z[d][j]\n",
    "                        self.n_trans[d, k_prev, k] -= 1\n",
    "                        self.m[word_ind, k] -= 1\n",
    "                        self.z[d][j] = -1 \n",
    "                    \n",
    "                    # computes the posterior distribution for the category distribution\n",
    "                    for k in range(self.K):\n",
    "                        sum_m_k = np.sum(self.m[:, k]) \n",
    "                        q[k] = ( self.alpha + self.n_trans[d, k_prev, k] ) * \\\n",
    "                                ( self.beta + self.m[word_ind, k] ) / \\\n",
    "                                (self.V * self.beta + sum_m_k ) \n",
    "                        # Normalizing\n",
    "                        p = q / np.sum(q)\n",
    "                    \n",
    "                else:\n",
    "                    # Remove current token, update m, \n",
    "                    k_prev = self.z[d][j-1]\n",
    "                    k_next = self.z[d][j+1]\n",
    "                    if self.z[d][j] != -1:\n",
    "                        k = self.z[d][j]\n",
    "                        self.n_trans[d, k_prev, k] -= 1\n",
    "                        self.n_trans[d, k, k_next] -= 1\n",
    "                        self.m[word_ind, k] -= 1\n",
    "                        self.z[d][j] = -1 \n",
    "\n",
    "                    # computes the posterior distribution for the category distribution\n",
    "                    for k in range(self.K):\n",
    "                        sum_m_k = np.sum(self.m[:, k]) \n",
    "                        q[k] = ( self.alpha + self.n_trans[d, k_prev, k] ) * \\\n",
    "                                ( self.beta + self.m[word_ind, k] ) / \\\n",
    "                                (self.V * self.beta + sum_m_k ) \n",
    "                        # Normalizing\n",
    "                        p = q / np.sum(q)\n",
    "\n",
    "                # Assigning a new category\n",
    "                assigned_category = np.random.choice(self.K, 1, p = p)[0]\n",
    "                \n",
    "                # Updating the model object accordingly\n",
    "                self.z[d][j] = assigned_category\n",
    "                self.m[word_ind, assigned_category] += 1\n",
    "                if j==0:\n",
    "                    self.n_start[assigned_category] += 1\n",
    "                    self.n_trans[d, assigned_category, k_next] += 1\n",
    "                elif j== len(chapter)-1:\n",
    "                    self.n_trans[d, k_prev, assigned_category] += 1\n",
    "                else:\n",
    "                    self.n_trans[d, k_prev, assigned_category] += 1\n",
    "                    self.n_trans[d, assigned_category, k_next] += 1\n",
    "                \n",
    "\n",
    "    \"\"\"\n",
    "    To evaluate our model we take a look at the highest values in the either the\n",
    "    relative frequency or the total frequency.\n",
    "    This function returns thoose counters.\n",
    "    \"\"\"\n",
    "    def evaluate(self, relative=True):\n",
    "        counters = self.count_occurances()\n",
    "\n",
    "        if relative:\n",
    "            for key in self.voc:\n",
    "                tot_freq = sum([counters[it][key] for it in range(self.K)])\n",
    "                \n",
    "                for jt in range(self.K):\n",
    "                    counters[jt][key] /= tot_freq\n",
    "\n",
    "        return counters\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    This is a helper function for our evaluation function. Here we count the frequencies \n",
    "    each appeared in each category and save it in a list of counter objects.\n",
    "    \"\"\"\n",
    "    def count_occurances(self):\n",
    "        counters = [Counter() for _ in range(self.K)]\n",
    "\n",
    "        for d, chapter in enumerate(self.docs):\n",
    "            for j, word in enumerate(chapter):\n",
    "                counters[self.z[d][j]][word] += 1\n",
    "\n",
    "        return counters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OrJ4BRbpKl5o"
   },
   "outputs": [],
   "source": [
    "def train_hmtm_model(alpha, beta, pi, n_iteration, K):\n",
    "    # Moby Dick - Herman Melville\n",
    "    print(\"Loading text document and formatting\")\n",
    "    moby_dick = gutenberg.words('melville-moby_dick.txt')\n",
    "\n",
    "    threshold = 10 \n",
    "    docs = preprocess(moby_dick, threshold)\n",
    "    freq = get_counter(docs)\n",
    "    cats = get_categories(docs)\n",
    "    voc = list(freq.keys())\n",
    "\n",
    "    model = HMTM(docs, cats, K, voc, alpha, beta, pi)\n",
    "    model.fit(n_iteration)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZbCJY8cGhOSD"
   },
   "source": [
    "##HMTM 1\n",
    "Training and viewing results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w2cL4ZQ9Kqzl"
   },
   "outputs": [],
   "source": [
    "model_hmtm = train_hmtm_model(0.1, 0.1, 1, 100, 10)\n",
    "\n",
    "counters_hmtm = model_hmtm.evaluate(relative = False)\n",
    "for counter in counters_hmtm:\n",
    "  print(counter.most_common(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1diPB2DoKsx4"
   },
   "source": [
    "Printout of above code:\n",
    "\n",
    "[('\"\\'', 62), ('steelkilt', 40), (\".'\", 36), (\"?'\", 32), (\",'\", 32), ('gentlemen', 30), (\"!'\", 27), ('lakeman', 24), ('radney', 22), ('mate', 15)]\n",
    "\n",
    "[('ginger', 16), ('bucket', 15), ('tun', 14), ('blubber', 13), ('strip', 9), ('blanket', 8), ('spades', 8), ('cutting', 8), ('mass', 8), ('tackles', 7)]\n",
    "\n",
    "[('fish', 41), ('fast', 20), ('law', 18), ('loose', 15), ('possession', 10), ('queen', 10), ('herd', 7), ('lake', 6), ('chase', 6), ('circles', 6)]\n",
    "\n",
    "[('white', 39), ('whiteness', 27), ('pulpit', 12), ('um', 11), ('terror', 6), ('chapel', 5), ('holy', 5), ('shark', 5), ('marble', 4), ('father', 4)]\n",
    "\n",
    "[('.', 148), ('whale', 78), ('whales', 46), ('(', 37), ('book', 24), ('),', 22), ('porpoise', 19), ('fish', 18), ('greenland', 17), ('folio', 16)]\n",
    "\n",
    "[('skull', 21), ('blacksmith', 20), ('skeleton', 15), ('perth', 14), ('feet', 13), ('spine', 12), ('brain', 12), ('ribs', 12), ('forge', 11), ('temple', 8)]\n",
    "\n",
    "[('!', 95), ('pip', 73), ('sailor', 32), ('(', 22), ('gabriel', 19), ('hussey', 17), ('.)', 16), ('mrs', 13), ('tambourine', 11), ('clam', 10)]\n",
    "\n",
    "[('bildad', 76), ('peleg', 74), ('cook', 38), ('dat', 20), ('de', 20), ('steak', 15), ('dam', 11), ('fleece', 11), ('quaker', 9), ('dost', 9)]\n",
    "\n",
    "[('jonah', 75), ('bed', 47), ('landlord', 33), ('harpooneer', 32), ('room', 28), ('god', 26), ('shipmates', 22), ('says', 12), ('thinks', 12), ('lesson', 11)]\n",
    "\n",
    "[(',', 18275), ('.', 6570), (';', 3949), (\"'\", 2606), ('-', 2492), ('\"', 1433), ('whale', 1115), ('!', 1080), ('--', 1045), ('one', 900)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fSVxtap-p5hF"
   },
   "source": [
    "Printout of above model, relative frequencies:\n",
    "\n",
    "[('peleg', 1.0), ('quaker', 1.0), ('bildad', 1.0), ('fleece', 1.0), ('steak', 1.0), ('dat', 1.0), ('whiteness', 0.9629629629629629), ('de', 0.7105263157894737), ('den', 0.6923076923076923), ('cook', 0.6909090909090909)]\n",
    "\n",
    "[('spine', 0.7058823529411765), ('skull', 0.6666666666666666), ('canal', 0.6666666666666666), ('ribs', 0.6190476190476191), ('skeleton', 0.5294117647058824), ('elephant', 0.5263157894736842), ('trunk', 0.4375), ('depth', 0.4), ('inches', 0.3684210526315789), ('flukes', 0.35135135135135137)]\n",
    "\n",
    "[('radney', 1.0), ('steelkilt', 1.0), ('lakeman', 1.0), ('\"\\'', 0.8970588235294118), (\"?'\", 0.875), (\".'\", 0.8611111111111112), (\"!'\", 0.8333333333333334), (\",'\", 0.8125), ('gentlemen', 0.7631578947368421), ('pumps', 0.7333333333333333)]\n",
    "\n",
    "[('cetology', 1.0), ('baleen', 1.0), ('narwhale', 1.0), ('folio', 1.0), ('ii', 1.0), ('iii', 1.0), ('porpoise', 1.0), (').--', 1.0), ('octavo', 0.9166666666666666), ('possession', 0.8333333333333334)]\n",
    "\n",
    "[('forge', 1.0), ('blacksmith', 1.0), ('ginger', 0.9375), ('guernsey', 0.9090909090909091), ('spades', 0.9), ('needle', 0.7272727272727273), ('compasses', 0.7), ('perth', 0.6111111111111112), ('monkey', 0.5), ('needles', 0.45454545454545453)]\n",
    "\n",
    "[('gabriel', 1.0), ('sinking', 1.0), ('bucket', 1.0), ('tun', 1.0), ('derick', 0.9166666666666666), ('jeroboam', 0.9), ('letter', 0.6428571428571429), ('noise', 0.4166666666666667), ('floats', 0.2727272727272727), ('says', 0.2608695652173913)]\n",
    "\n",
    "[('jonah', 1.0), ('george', 0.9), ('lesson', 0.8461538461538461), ('shipmates', 0.8214285714285714), ('perseus', 0.8181818181818182), ('prophet', 0.625), ('delight', 0.6111111111111112), ('berth', 0.4), ('feels', 0.38461538461538464), ('woe', 0.3548387096774194)]\n",
    "\n",
    "[('tub', 0.8461538461538461), ('herd', 0.8461538461538461), ('circles', 0.8181818181818182), ('straits', 0.8125), ('lake', 0.7), ('host', 0.5), ('hemp', 0.45454545454545453), ('block', 0.45454545454545453), ('strip', 0.4), ('windlass', 0.36363636363636365)]\n",
    "\n",
    "[('mrs', 1.0), ('bunger', 1.0), ('hussey', 0.9411764705882353), ('um', 0.9166666666666666), ('clam', 0.9090909090909091), ('ramadan', 0.8181818181818182), ('ambergris', 0.7692307692307693), ('pulpit', 0.75), ('ladder', 0.75), ('cod', 0.5384615384615384)]\n",
    "\n",
    "[('dick', 1.0), ('supplied', 1.0), ('late', 1.0), ('school', 1.0), (')', 1.0), ('pale', 1.0), ('--', 1.0), ('coat', 1.0), (',', 1.0), ('heart', 1.0)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y2bxo04hheUU"
   },
   "source": [
    "##HMTM 2\n",
    "Training and viewing result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NPTHYC5GK18b",
    "outputId": "d2ed6530-bf0c-4dfd-d27f-9f7a1dbab821"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading text document and formatting\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  import sys\n",
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:9: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting the model\n",
      "Iteration 1\n",
      "Iteration 2\n",
      "Iteration 3\n",
      "Iteration 4\n",
      "Iteration 5\n",
      "Iteration 6\n",
      "Iteration 7\n",
      "Iteration 8\n",
      "Iteration 9\n",
      "Iteration 10\n",
      "Iteration 11\n",
      "Iteration 12\n",
      "Iteration 13\n",
      "Iteration 14\n",
      "Iteration 15\n",
      "Iteration 16\n",
      "Iteration 17\n",
      "Iteration 18\n",
      "Iteration 19\n",
      "Iteration 20\n",
      "Iteration 21\n",
      "Iteration 22\n",
      "Iteration 23\n",
      "Iteration 24\n",
      "Iteration 25\n",
      "Iteration 26\n",
      "Iteration 27\n",
      "Iteration 28\n",
      "Iteration 29\n",
      "Iteration 30\n",
      "Iteration 31\n",
      "Iteration 32\n",
      "Iteration 33\n",
      "Iteration 34\n",
      "Iteration 35\n",
      "Iteration 36\n",
      "Iteration 37\n",
      "Iteration 38\n",
      "Iteration 39\n",
      "Iteration 40\n",
      "Iteration 41\n",
      "Iteration 42\n",
      "Iteration 43\n",
      "Iteration 44\n",
      "Iteration 45\n",
      "Iteration 46\n",
      "Iteration 47\n",
      "Iteration 48\n",
      "Iteration 49\n",
      "Iteration 50\n",
      "Iteration 51\n",
      "Iteration 52\n",
      "Iteration 53\n",
      "Iteration 54\n",
      "Iteration 55\n",
      "Iteration 56\n",
      "Iteration 57\n",
      "Iteration 58\n",
      "Iteration 59\n",
      "Iteration 60\n",
      "Iteration 61\n",
      "Iteration 62\n",
      "Iteration 63\n",
      "Iteration 64\n",
      "Iteration 65\n",
      "Iteration 66\n",
      "Iteration 67\n",
      "Iteration 68\n",
      "Iteration 69\n",
      "Iteration 70\n",
      "Iteration 71\n",
      "Iteration 72\n",
      "Iteration 73\n",
      "Iteration 74\n",
      "Iteration 75\n",
      "Iteration 76\n",
      "Iteration 77\n",
      "Iteration 78\n",
      "Iteration 79\n",
      "Iteration 80\n",
      "Iteration 81\n",
      "Iteration 82\n",
      "Iteration 83\n",
      "Iteration 84\n",
      "Iteration 85\n",
      "Iteration 86\n",
      "Iteration 87\n",
      "Iteration 88\n",
      "Iteration 89\n",
      "Iteration 90\n",
      "Iteration 91\n",
      "Iteration 92\n",
      "Iteration 93\n",
      "Iteration 94\n",
      "Iteration 95\n",
      "Iteration 96\n",
      "Iteration 97\n",
      "Iteration 98\n",
      "Iteration 99\n",
      "Iteration 100\n",
      "Iteration 101\n",
      "Iteration 102\n",
      "Iteration 103\n",
      "Iteration 104\n",
      "Iteration 105\n",
      "Iteration 106\n",
      "Iteration 107\n",
      "Iteration 108\n",
      "Iteration 109\n",
      "Iteration 110\n",
      "Iteration 111\n",
      "Iteration 112\n",
      "Iteration 113\n",
      "Iteration 114\n",
      "Iteration 115\n",
      "Iteration 116\n",
      "Iteration 117\n",
      "Iteration 118\n",
      "Iteration 119\n",
      "Iteration 120\n",
      "Iteration 121\n",
      "Iteration 122\n",
      "Iteration 123\n",
      "Iteration 124\n",
      "Iteration 125\n",
      "Iteration 126\n",
      "Iteration 127\n",
      "Iteration 128\n",
      "Iteration 129\n",
      "Iteration 130\n",
      "Iteration 131\n",
      "Iteration 132\n",
      "Iteration 133\n",
      "Iteration 134\n",
      "Iteration 135\n",
      "Iteration 136\n",
      "Iteration 137\n",
      "Iteration 138\n",
      "Iteration 139\n",
      "Iteration 140\n",
      "Iteration 141\n",
      "Iteration 142\n",
      "Iteration 143\n",
      "Iteration 144\n",
      "Iteration 145\n",
      "Iteration 146\n",
      "Iteration 147\n",
      "Iteration 148\n",
      "Iteration 149\n",
      "Iteration 150\n",
      "Iteration 151\n",
      "Iteration 152\n",
      "Iteration 153\n",
      "Iteration 154\n",
      "Iteration 155\n",
      "Iteration 156\n",
      "Iteration 157\n",
      "Iteration 158\n",
      "Iteration 159\n",
      "Iteration 160\n",
      "Iteration 161\n",
      "Iteration 162\n",
      "Iteration 163\n",
      "Iteration 164\n",
      "Iteration 165\n",
      "Iteration 166\n",
      "Iteration 167\n",
      "Iteration 168\n",
      "Iteration 169\n",
      "Iteration 170\n",
      "Iteration 171\n",
      "Iteration 172\n",
      "Iteration 173\n",
      "Iteration 174\n",
      "Iteration 175\n",
      "Iteration 176\n",
      "Iteration 177\n",
      "Iteration 178\n",
      "Iteration 179\n",
      "Iteration 180\n",
      "Iteration 181\n",
      "Iteration 182\n",
      "Iteration 183\n",
      "Iteration 184\n",
      "Iteration 185\n",
      "Iteration 186\n",
      "Iteration 187\n",
      "Iteration 188\n",
      "Iteration 189\n",
      "Iteration 190\n",
      "Iteration 191\n",
      "Iteration 192\n",
      "Iteration 193\n",
      "Iteration 194\n",
      "Iteration 195\n",
      "Iteration 196\n",
      "Iteration 197\n",
      "Iteration 198\n",
      "Iteration 199\n",
      "Iteration 200\n",
      "[('head', 15), ('bucket', 15), ('tun', 12), ('tackles', 10), ('windlass', 8), ('tashtego', 8), ('hole', 6), ('blubber', 6), ('tackle', 6), ('strip', 6)]\n",
      "[('new', 13), ('bedford', 13), ('oil', 11), ('streets', 8), ('town', 7), ('street', 6), ('country', 5), ('green', 4), ('glory', 3), ('milk', 3)]\n",
      "[('night', 6), ('towards', 3), ('jolly', 3), ('let', 3), ('mass', 2), ('made', 2), ('look', 2), ('horses', 2), ('temporary', 2), ('ribs', 1)]\n",
      "[('pip', 64), ('log', 11), ('manxman', 9), ('boy', 8), ('happened', 5), ('haul', 5), ('negro', 4), ('poor', 4), ('coward', 4), ('art', 4)]\n",
      "[('nose', 13), ('spout', 13), ('sperm', 8), ('surface', 8), ('vapour', 8), ('one', 7), ('canal', 7), ('water', 6), ('mixed', 5), ('cannot', 5)]\n",
      "[('stubb', 30), ('pull', 26), ('rose', 11), ('boys', 9), ('guernsey', 9), ('nose', 8), ('pulling', 8), ('oil', 7), ('vapour', 6), ('pulled', 6)]\n",
      "[('whaling', 42), ('fishery', 18), ('whalers', 18), ('harpooneer', 17), ('english', 17), ('000', 15), ('dutch', 12), ('voyage', 10), ('.', 10), ('ambergris', 8)]\n",
      "[('.\"', 32), ('...', 11), ('sperm', 11), ('ocean', 7), ('.', 6), ('either', 5), ('london', 4), ('still', 4), ('jaws', 3), ('king', 3)]\n",
      "[('(', 13), ('bunger', 13), ('narwhale', 9), ('horn', 9), ('porpoise', 5), ('ye', 5), ('well', 4), ('tackle', 4), ('hast', 3), ('feet', 3)]\n",
      "[('whale', 158), ('whales', 63), ('greenland', 23), ('book', 22), ('leviathan', 20), ('),', 18), ('folio', 15), ('found', 14), ('fin', 14), ('english', 13)]\n",
      "[('jonah', 82), ('god', 32), ('shipmates', 18), ('woe', 11), ('delight', 11), ('lesson', 10), ('prophet', 7), (\",'\", 6), ('truth', 6), (\"!'\", 5)]\n",
      "[('blacksmith', 19), ('perth', 16), ('sleet', 14), ('mast', 9), ('st', 9), ('forge', 9), ('hammer', 8), ('nest', 8), ('life', 7), ('crow', 7)]\n",
      "[('pulpit', 12), ('father', 7), ('ladder', 7), ('storm', 6), ('like', 6), ('idol', 5), ('memory', 5), ('chapel', 4), ('sacred', 4), ('marble', 3)]\n",
      "[(',', 44), (';', 22), (\"'\", 4), ('first', 3), ('northern', 3), ('(', 3), ('summer', 2), ('ice', 2), ('often', 2), ('seen', 2)]\n",
      "[('gabriel', 20), ('letter', 10), ('jeroboam', 10), (',', 4), (\"'\", 4), ('hark', 3), ('letters', 3), ('pequod', 3), ('dead', 2), ('wide', 2)]\n",
      "[('dart', 12), ('derick', 11), ('german', 10), ('sinking', 8), ('oh', 7), ('lamp', 7), ('rapid', 6), ('haul', 6), ('rush', 5), ('start', 5)]\n",
      "[('cook', 47), ('de', 24), ('stubb', 21), ('dat', 19), ('steak', 14), ('sharks', 14), ('dam', 11), ('meat', 10), ('fleece', 10), ('supper', 8)]\n",
      "[(',', 87), ('--', 6), ('seems', 3), ('wood', 3), ('striking', 2), ('sky', 2), ('person', 2), ('chiefly', 2), ('tashtego', 2), ('silence', 2)]\n",
      "[('.', 165), ('whale', 12), ('ahab', 10), ('ii', 9), ('iii', 9), ('-', 6), ('pequod', 5), ('right', 4), ('stubb', 4), (':', 4)]\n",
      "[(';', 12), ('followed', 4), ('light', 3), ('.', 3), ('stands', 2), ('oil', 2), ('watch', 2), ('valuable', 2), ('many', 2), ('gain', 2)]\n",
      "[(';', 10), ('um', 9), ('thunder', 3), ('pushed', 2), ('died', 2), ('instant', 2), ('concerning', 2), ('saw', 2), ('ahab', 2), ('main', 2)]\n",
      "[('bildad', 76), ('peleg', 74), ('captain', 32), ('thou', 28), ('yojo', 12), ('dost', 11), ('looked', 10), ('queequeg', 10), ('quaker', 9), ('want', 9)]\n",
      "[('ginger', 15), ('queequeg', 13), ('sharks', 10), ('.', 9), ('monkey', 8), ('spades', 7), ('rope', 7), ('harpooneer', 5), ('inserted', 4), ('watching', 4)]\n",
      "[('hussey', 17), ('pots', 14), ('mrs', 13), ('clam', 10), ('supper', 8), ('cod', 7), ('valuable', 6), ('first', 4), ('pot', 3), ('starboard', 3)]\n",
      "[('boat', 31), ('line', 25), ('warp', 9), ('tub', 8), ('whale', 5), ('hemp', 5), ('makes', 5), ('rope', 5), ('seated', 5), ('lance', 5)]\n",
      "[(',', 48), ('!', 3), ('distance', 3), ('real', 2), ('great', 2), ('oars', 2), ('bottom', 2), ('reach', 2), ('floats', 2), ('signs', 2)]\n",
      "[('flask', 16), ('cabin', 13), ('dough', 10), ('dinner', 10), ('harpooneers', 7), ('boy', 7), ('steward', 5), ('table', 5), ('beef', 5), ('lived', 5)]\n",
      "[('coffin', 31), ('life', 11), ('buoy', 10), ('dost', 7), ('dying', 7), ('hammock', 6), ('cask', 6), ('die', 6), ('carpenter', 6), ('faith', 6)]\n",
      "[('-', 78), (',', 44), ('?', 25), (';', 13), ('trouble', 3), ('goes', 3), ('lying', 2), ('thing', 2), ('hundred', 2), ('three', 2)]\n",
      "[('fish', 56), ('fast', 16), ('loose', 15), ('perseus', 10), ('law', 10), ('possession', 9), ('gentleman', 9), ('george', 8), ('queen', 8), ('sea', 7)]\n",
      "[('queequeg', 126), ('bed', 53), ('harpooneer', 39), ('room', 36), ('landlord', 32), ('tomahawk', 17), ('door', 15), ('night', 12), ('kill', 12), ('cold', 12)]\n",
      "[('man', 7), ('right', 4), ('deck', 3), ('erect', 3), ('nantucket', 2), ('black', 2), ('answered', 2), ('aloft', 2), ('back', 2), ('hunters', 2)]\n",
      "[('fire', 20), ('works', 12), ('flames', 12), ('sun', 11), ('try', 10), ('doubloon', 10), ('smoke', 7), ('signs', 7), ('darkness', 7), ('pitch', 6)]\n",
      "[('\"\\'', 61), ('steelkilt', 40), ('gentlemen', 28), (\".'\", 26), ('lakeman', 24), (\"!'\", 23), ('radney', 22), (\"?'\", 22), ('mate', 19), (\",'\", 13)]\n",
      "[('whale', 5), ('waves', 3), ('long', 3), ('points', 3), ('still', 3), ('manner', 2), ('outer', 2), ('send', 2), ('like', 2), ('ships', 2)]\n",
      "[('east', 6), (',', 6), ('compasses', 6), ('needle', 6), ('needles', 5), ('steel', 5), ('virtue', 5), ('binnacle', 5), ('city', 3), ('image', 3)]\n",
      "[('feet', 21), ('skeleton', 20), ('whale', 17), ('skull', 16), ('ribs', 14), ('brain', 13), ('bones', 9), ('inches', 9), ('spine', 9), ('length', 8)]\n",
      "[(',', 28), ('-', 7), ('owners', 4), ('sperm', 2), ('.--', 2), (';', 2), ('brow', 2), ('aloft', 2), ('might', 1), ('upon', 1)]\n",
      "[('!', 145), ('.', 72), (\"'\", 59), ('sailor', 36), ('(', 23), ('?', 12), ('.)', 12), (',', 9), ('pip', 8), ('old', 7)]\n",
      "[(',', 12), ('-', 5), ('another', 4), ('fins', 3), ('worse', 2), ('ship', 2), (').--', 2), ('law', 2), ('lips', 2), ('tail', 2)]\n",
      "[('tail', 22), ('flukes', 11), ('elephant', 11), (',', 6), ('trunk', 6), ('power', 6), ('two', 4), ('motions', 4), ('often', 4), ('scenes', 3)]\n",
      "[('white', 48), ('whiteness', 26), ('-', 8), ('hue', 8), ('terror', 6), ('bear', 5), ('aspect', 5), ('colour', 5), ('aught', 5), ('spiritual', 5)]\n",
      "[('.', 12), (';', 9), ('full', 5), ('wood', 3), ('anybody', 3), ('sea', 2), ('),', 2), (').--', 2), ('found', 2), ('alike', 2)]\n",
      "[('.', 68), (',', 15), ('near', 5), ('eat', 4), ('pequod', 4), ('port', 3), ('fully', 3), ('among', 2), ('fine', 2), ('oar', 2)]\n",
      "[(',', 81), ('.', 26), ('porpoise', 6), ('),', 3), ('manner', 2), ('christian', 2), ('however', 2), ('nose', 2), ('tossing', 2), ('oil', 2)]\n",
      "[('blubber', 13), ('skin', 13), ('substance', 10), ('body', 8), ('blanket', 7), ('greenland', 6), ('marks', 5), ('whale', 5), ('rocks', 4), ('rare', 4)]\n",
      "[('tongue', 6), ('blinds', 6), ('vast', 4), ('shore', 3), ('green', 3), ('coast', 2), ('kill', 2), ('back', 2), ('according', 2), ('numerous', 2)]\n",
      "[('carpenter', 27), ('leg', 22), (\"'\", 21), ('sir', 17), ('says', 15), ('(', 14), ('queer', 14), (\",'\", 13), ('wise', 11), ('flask', 10)]\n",
      "[('whales', 31), ('straits', 14), ('herd', 11), ('circles', 8), ('host', 7), ('lake', 6), ('swimming', 5), ('evinced', 5), ('floating', 5), ('thousands', 5)]\n",
      "[(',', 17889), ('.', 6339), (';', 3878), (\"'\", 2509), ('-', 2379), ('\"', 1424), ('--', 1032), ('!', 1018), ('whale', 987), ('one', 889)]\n"
     ]
    }
   ],
   "source": [
    "model_hmtm_2 = train_hmtm_model(0.1, 0.1, 1, 200, 50)\n",
    "\n",
    "counters_hmtm_2 = model_hmtm_2.evaluate(relative = False)\n",
    "for counter in counters_hmtm_2:\n",
    "  print(counter.most_common(10))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Assignment2_NLP.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
