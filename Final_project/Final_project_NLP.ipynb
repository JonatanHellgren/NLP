{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Final project NLP.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "##Idea based on: A neural interlingua for multilingual machine translation.\n",
        "\n",
        "## Basic idea\n",
        "* Implement a multilingual encoder-interlingua-decoder model and train it on three languages, for example en, sv and de.\n",
        "\n",
        "* Train model on translating the following language pairs:\n",
        "    * en - de\n",
        "    * de - sv\n",
        "    * en - en\n",
        "    * sv - sv\n",
        "    * de - de.\n",
        "\n",
        "* Train a bilingual model only consisting of a encoder-decoder pair and train it on the language pair: en-sv. This will be used for reference when evaluating the model.\n",
        "\n",
        "* Evaluate the multilingual model by letting the first model zero-shot translate en-sv, then compare results with the bilingual model. Hopefully they wont be to far apart. \n",
        "\n",
        "###Possible improvements\n",
        "* Use Bert embedding, this would make the model larger, but might yield a higher score as we have seen previously. I belive this is a bit harder to implement and thus I wont start of with it.\n",
        "\n",
        "* Include more languages in the interlingua model. This will make the training take more time but how it will affect the results I find unclear. Three languages is the bare minimum for a zero-shot translation to be possible. \n",
        "\n",
        "###Implementation\n",
        "* Use mt_util.py for dataloading as in machine translation colab notebook.\n",
        "* Use code from the same notebook for the bilingual model with BahdanauAttention.\n",
        "* Modify the bilingual model to include a second interlingua attention.\n",
        "* Write a script that trains the multilingual model. \n"
      ],
      "metadata": {
        "id": "zec8PZNWvzcm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive/')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bC-fvaD25LwN",
        "outputId": "4c980f21-f01c-4884-fdff-6ffacdffc4cd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2VyOchesUAqY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "176c683f-3ecb-414b-ecde-9b8fbc344a2d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting sacrebleu\n",
            "  Downloading sacrebleu-2.0.0-py3-none-any.whl (90 kB)\n",
            "\u001b[?25l\r\u001b[K     |███▋                            | 10 kB 22.0 MB/s eta 0:00:01\r\u001b[K     |███████▏                        | 20 kB 10.0 MB/s eta 0:00:01\r\u001b[K     |██████████▉                     | 30 kB 8.5 MB/s eta 0:00:01\r\u001b[K     |██████████████▍                 | 40 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 51 kB 5.5 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▋          | 61 kB 5.7 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▎      | 71 kB 5.5 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 81 kB 6.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 90 kB 4.2 MB/s \n",
            "\u001b[?25hCollecting colorama\n",
            "  Downloading colorama-0.4.4-py2.py3-none-any.whl (16 kB)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.7/dist-packages (from sacrebleu) (0.8.9)\n",
            "Collecting portalocker\n",
            "  Downloading portalocker-2.3.2-py2.py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from sacrebleu) (1.19.5)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from sacrebleu) (2019.12.20)\n",
            "Installing collected packages: portalocker, colorama, sacrebleu\n",
            "Successfully installed colorama-0.4.4 portalocker-2.3.2 sacrebleu-2.0.0\n",
            "--2022-01-10 12:29:56--  https://raw.githubusercontent.com/liu-nlp/dl4nlp/master/exercise3_1/mt_util.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.111.133, 185.199.109.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 7686 (7.5K) [text/plain]\n",
            "Saving to: ‘mt_util.py’\n",
            "\n",
            "mt_util.py          100%[===================>]   7.51K  --.-KB/s    in 0s      \n",
            "\n",
            "2022-01-10 12:29:56 (49.1 MB/s) - ‘mt_util.py’ saved [7686/7686]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Install the library to compute the BLEU score.\n",
        "!pip install sacrebleu\n",
        "\n",
        "# Get the Python file that contains the auxiliary functions for this exercise.\n",
        "!wget https://raw.githubusercontent.com/liu-nlp/dl4nlp/master/exercise3_1/mt_util.py\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "import time\n",
        "import numpy as np\n",
        "import sys\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "import sacrebleu\n",
        "from collections import defaultdict\n",
        "\n",
        "import mt_util"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Bilingual model\n",
        "Using code from week 5 machine translation notebook and tweaking it a bit to fit our specific purpose."
      ],
      "metadata": {
        "id": "nZIf4me9jpe5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderDecoder(nn.Module):\n",
        "    def __init__(self, encoder, decoder):\n",
        "        super().__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        \n",
        "    # S is a batch of source-language sentences, shape (n_sentences, n_source_words).\n",
        "    # T_shifted contains the corresponding target-language sentences, shifted one step\n",
        "    #   to the right for teacher forcing, shape (n_sentences, n_target_words).\n",
        "    def forward(self, S, T_shifted):        \n",
        "        # Apply the encoder, we get token representations in the source language.\n",
        "        S_enc = self.encoder(S)\n",
        "\n",
        "        # Compute a mask that we'll use to get rid of padding tokens when we compute\n",
        "        # the attention. (We hardcode that the padding token has index 0.)\n",
        "        S_mask = (S != 0)\n",
        "        \n",
        "        # Apply the decoder in teacher forcing mode.\n",
        "        return self.decoder(S_enc, S_mask, T_shifted)\n",
        "    \n",
        "    # S is a batch of source-language sentences, shape (n_sentences, n_source_words).\n",
        "    # max_len is the maximally allowed sentence length.\n",
        "    def greedy_decode(self, S, max_len):\n",
        "        S_enc = self.encoder(S)    \n",
        "        S_mask = (S != 0)\n",
        "        return self.decoder.greedy_decode(S_enc, S_mask, max_len)\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, emb_layer, param):\n",
        "        super().__init__()\n",
        "        _, emb_dim = emb_layer.weight.shape       \n",
        "        self.emb_layer = emb_layer\n",
        "        self.rnn = nn.GRU(emb_dim, param.enc_rnn_dim, param.enc_rnn_depth, \n",
        "                          batch_first=True, bidirectional=True)\n",
        "        \n",
        "    def forward(self, S):\n",
        "        embedded = self.emb_layer(S)\n",
        "        output, _ = self.rnn(embedded)\n",
        "        return output\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "\n",
        "    def __init__(self, emb_layer, param):\n",
        "        super().__init__()\n",
        "        voc_size, emb_dim = emb_layer.weight.shape\n",
        "\n",
        "        # Word embeddings for the target language. \n",
        "        self.emb_layer = emb_layer\n",
        "\n",
        "        attn_dim = 2*param.enc_rnn_dim\n",
        "\n",
        "        # The decoder's state is represented using a GRU, called f in Bahdanau's paper.\n",
        "        # The input of this GRU is a combination of the embedding of the previous word\n",
        "        # and the \"context\" computed by the attention model.\n",
        "        self.rnn = nn.GRU(emb_dim + attn_dim, param.dec_rnn_dim, param.dec_rnn_depth, \n",
        "                          batch_first=True, bidirectional=False)\n",
        "        \n",
        "        # The output layer (called g in the paper) is a bit simpler than in the paper. \n",
        "        # We just use a linear model.\n",
        "        self.output_layer = nn.Linear(param.dec_rnn_dim + attn_dim + emb_dim,\n",
        "                                      voc_size, bias=False)\n",
        "                \n",
        "        self.rnn_dim = param.dec_rnn_dim\n",
        "\n",
        "        # The attention model, which computes a context or \"summary\" of the source sentence.\n",
        "        # Right now, we use a simplified attention model that doesn't depend on the decoder state.\n",
        "        # See below for the implementation.\n",
        "        # self.attention = MeanAttention()\n",
        "        self.attention = BahdanauAttention(param.enc_rnn_dim*2, param.enc_rnn_dim, param.hidden_dim)\n",
        "\n",
        "        \n",
        "    # This method applies the attention model, the RNN and the output model at one\n",
        "    # position in the decoded sentence.\n",
        "    def forward_step(self, prev_embed, enc_out, src_mask, precomputed_key, decoder_hidden):        \n",
        "\n",
        "        # The \"query\" for attention is the hidden state of the decoder.\n",
        "        query = decoder_hidden[-1].unsqueeze(1)  \n",
        "        \n",
        "        # Apply the attention model to compute a \"context\", summary of the encoder output\n",
        "        # based on the current query.\n",
        "        # Also returns the attention weights, which we might want to visualize or inspect.\n",
        "        context, attn_probs = self.attention(\n",
        "            query=query, precomputed_key=precomputed_key,\n",
        "            value=enc_out, mask=src_mask)\n",
        "        \n",
        "        # Run the RNN one step.\n",
        "        rnn_input = torch.cat([prev_embed, context], dim=2)\n",
        "        rnn_output, decoder_hidden = self.rnn(rnn_input, decoder_hidden)\n",
        "        \n",
        "        # The word output model (called g in Bahdanau's paper) uses the embedding \n",
        "        # of the previous word, the RNN output, and the context computed by the attention model.\n",
        "        pre_output = torch.cat([prev_embed, rnn_output, context], dim=2)\n",
        "        T_output = self.output_layer(pre_output)\n",
        "\n",
        "        return attn_probs, decoder_hidden, T_output\n",
        "        \n",
        "        \n",
        "    # Apply the decoder in teacher forcing mode.\n",
        "    def forward(self, enc_out, src_mask, T_shifted):\n",
        "\n",
        "        n_sen, n_words = T_shifted.shape\n",
        "        \n",
        "        # Word embeddings for the shifted word, representing the previous step.\n",
        "        T_emb = self.emb_layer(T_shifted)\n",
        "        \n",
        "        # Precompute attention keys (if needed).\n",
        "        precomputed_key = self.attention.precompute_key(enc_out)\n",
        "        \n",
        "        # Initialize the hidden state of the GRU.\n",
        "        decoder_hidden = torch.zeros(1, n_sen, self.rnn_dim, device=src_mask.device)\n",
        "        \n",
        "        all_out = []\n",
        "        \n",
        "        # For each position in the target sentence:\n",
        "        for i in range(n_words):\n",
        "            \n",
        "            # Embedding for the previous word.\n",
        "            prev_embed = T_emb[:, i].unsqueeze(1)\n",
        "            \n",
        "            # Run the decoder one step.\n",
        "            # This returns a new hidden state, and the output \n",
        "            # scores (over the target vocabulary) at this position.\n",
        "            _, decoder_hidden, T_output = self.forward_step(prev_embed, enc_out, \n",
        "                                                            src_mask, precomputed_key, decoder_hidden)\n",
        "            all_out.append(T_output)\n",
        " \n",
        "        # Combine the output scores for all positions.\n",
        "        return torch.cat(all_out, dim=1)\n",
        "            \n",
        "    # Apply the decoder in a greedy step-by-step fashion, at each step selecting the\n",
        "    # highest-scoring word.\n",
        "    def greedy_decode(self, enc_out, src_mask, max_len):\n",
        "        n_sen, _ = src_mask.shape\n",
        "\n",
        "        precomputed_key = self.attention.precompute_key(enc_out)\n",
        "        decoder_hidden = torch.zeros(1, n_sen, self.rnn_dim, device=src_mask.device)\n",
        "        \n",
        "        prev_tokens = torch.zeros(src_mask.size(0), 1, dtype=torch.long, \n",
        "                                  device=src_mask.device)\n",
        "        all_out = []\n",
        "        all_attn = []\n",
        "\n",
        "        for i in range(max_len):\n",
        "            prev_embed = self.emb_layer(prev_tokens)\n",
        "            attn_probs, decoder_hidden, T_output = self.forward_step(prev_embed, enc_out, src_mask, \n",
        "                                                                     precomputed_key, decoder_hidden)\n",
        "            # Select the highest-scoring word.\n",
        "            prev_tokens = T_output.argmax(-1)\n",
        "            all_out.append(prev_tokens)\n",
        "            all_attn.append(attn_probs)\n",
        "        \n",
        "        return torch.cat(all_out, dim=1), torch.stack(all_attn)\n",
        "\n"
      ],
      "metadata": {
        "id": "tjfGgYD8jr12"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Bahdanau attention\n",
        "For opur attention mechanism we will use the one defined in the paper by Bahdanau et al."
      ],
      "metadata": {
        "id": "ybAqqfoGkEmP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BahdanauAttention(nn.Module):\n",
        "    \n",
        "    def __init__(self, input_dim, query_dim, hidden_dim):\n",
        "        super().__init__()\n",
        "        self.K = nn.Linear(input_dim, hidden_dim)\n",
        "        self.Q = nn.Linear(query_dim, hidden_dim)\n",
        "        self.v_a = nn.Linear(hidden_dim, 1)\n",
        "        self.f = nn.Tanh() # Activation function\n",
        "                   \n",
        "    # When we implement the full attention mechanisms, we'll want to precompute the \"key\"\n",
        "    # representations. For now, we do nothing here.\n",
        "    def precompute_key(self, encoder_out):\n",
        "        return self.K(encoder_out)\n",
        "            \n",
        "    def forward(self, query, precomputed_key, value, mask):\n",
        "        # n_sen, n_words, enc_rnn_dim = value.shape\n",
        "        \n",
        "        Qs = self.Q(query)\n",
        "        summarized = torch.add(Qs, precomputed_key)\n",
        "        scores = self.v_a( self.f(summarized) )\n",
        "        scores = torch.squeeze(scores, dim=2)\n",
        "\n",
        "        # Mask out the energy scores for the padding tokens.\n",
        "        # We set them to -infinity so that they will be 0 after applying the softmax.\n",
        "        if mask != None:\n",
        "            scores.data.masked_fill_(~mask, -float('inf'))\n",
        "                \n",
        "        # Turn scores to probabilities.\n",
        "        alphas = F.softmax(scores, dim=1)        \n",
        "        \n",
        "        # The context vector is the weighted sum of the values.\n",
        "        context = torch.bmm(alphas.unsqueeze(1), value)\n",
        "        \n",
        "        return context, alphas\n"
      ],
      "metadata": {
        "id": "f0Gojkhyj8_y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Parameters and training loop"
      ],
      "metadata": {
        "id": "z7y6X9gxkYjU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TranslatorParameters:\n",
        "    device = 'cuda'\n",
        "    \n",
        "    n_batches_print = 32\n",
        "    \n",
        "    src_voc_size = 10000\n",
        "    tgt_voc_size = 10000\n",
        "    \n",
        "    random_seed = 0\n",
        "            \n",
        "    n_epochs = 37\n",
        "    \n",
        "    batch_size = 64\n",
        "    \n",
        "    learning_rate = 2e-4\n",
        "    weight_decay = 0\n",
        "        \n",
        "    emb_dim = 128\n",
        "    \n",
        "    enc_rnn_dim = 128 \n",
        "    enc_rnn_depth = 1\n",
        "\n",
        "    dec_rnn_dim = 128 \n",
        "    dec_rnn_depth = 1 \n",
        "\n",
        "    hidden_dim = 64\n",
        "\n",
        "    max_train_sentences = 200000\n",
        "    max_valid_sentences = 1000\n",
        "    \n",
        "    src_train = './data/de-en-train.de'\n",
        "    tgt_train = './data/de-en-train.en'\n",
        "    src_valid = './data/de-en-test.de'\n",
        "    tgt_valid = './data/de-en-test.en'\n",
        "      \n",
        "    \n",
        "class Translator:\n",
        "    \n",
        "    def __init__(self, params):\n",
        "        self.params = params\n",
        "        \n",
        "    def train(self):\n",
        "        \n",
        "        p = self.params\n",
        "        \n",
        "        # Setting a fixed seed for reproducibility.\n",
        "        torch.manual_seed(p.random_seed)\n",
        "        random.seed(p.random_seed)\n",
        "        \n",
        "        print('Preparing data...', end='')\n",
        "        sys.stdout.flush()\n",
        "        \n",
        "        # Read the source-language data.\n",
        "        S_train = mt_util.read_lines(p.src_train, max_lines=p.max_train_sentences)\n",
        "        self.S_voc = mt_util.Vocabulary(include_unknown=True, lower=True, max_voc_size=p.src_voc_size)\n",
        "        self.S_voc.build(S_train)\n",
        "        S_valid = mt_util.read_lines(p.src_valid, max_lines=p.max_valid_sentences)\n",
        "\n",
        "        # Read the target-language data.\n",
        "        T_train = mt_util.read_lines(p.tgt_train, max_lines=p.max_train_sentences)\n",
        "        self.T_voc = mt_util.Vocabulary(include_unknown=True, lower=True, max_voc_size=p.tgt_voc_size)\n",
        "        self.T_voc.build(T_train)\n",
        "        T_valid = mt_util.read_lines(p.tgt_valid, max_lines=p.max_valid_sentences)\n",
        "                \n",
        "        # Batching for the training and validation sets.\n",
        "        self.batcher = mt_util.SequenceBatcher(p.device)        \n",
        "        train_dataset = mt_util.SequenceDataset(self.S_voc.encode(S_train), self.T_voc.encode(T_train))\n",
        "        train_loader = DataLoader(train_dataset, p.batch_size, shuffle=True, collate_fn=self.batcher)\n",
        "\n",
        "        valid_dataset = mt_util.SequenceDataset(self.S_voc.encode(S_valid), self.T_voc.encode(T_valid))\n",
        "        valid_loader = DataLoader(valid_dataset, p.batch_size, shuffle=True, collate_fn=self.batcher)\n",
        "        \n",
        "        print(' done.')\n",
        "        \n",
        "        print('Building model...', end='')\n",
        "        sys.stdout.flush()        \n",
        "        \n",
        "        # We use a utility to build the embedding layer, if we would like to \n",
        "        # use a pre-trained model (which we don't do here).\n",
        "        S_emb = self.S_voc.make_embedding_layer(finetune=True, emb_dim=p.emb_dim)\n",
        "        T_emb = self.T_voc.make_embedding_layer(finetune=True, emb_dim=p.emb_dim)\n",
        "        \n",
        "        # Build the encoder and decoder.\n",
        "        encoder = Encoder(S_emb, p)\n",
        "        decoder = Decoder(T_emb, p)\n",
        "        self.model = EncoderDecoder(encoder, decoder)\n",
        "        \n",
        "        self.model.to(p.device)\n",
        "        print(' done.')\n",
        "                \n",
        "        optimizer = torch.optim.Adam(self.model.parameters(), \n",
        "                                     lr=p.learning_rate, weight_decay=p.weight_decay)\n",
        "\n",
        "        # The loss function is a cross-entropy loss at the token level.\n",
        "        # We don't include padding tokens when computing the loss.\n",
        "        loss_func = torch.nn.CrossEntropyLoss(ignore_index=self.T_voc.get_pad_idx())\n",
        "\n",
        "        self.history = defaultdict(list)\n",
        "        for epoch in range(1, p.n_epochs+1):\n",
        "            \n",
        "            t0 = time.time()\n",
        "\n",
        "            loss_sum = 0\n",
        "            for i, (Sbatch, Tbatch) in enumerate(train_loader, 1):\n",
        "                \n",
        "                # We use teacher forcing to train the decoder.\n",
        "                # This means that the input at each decoding step will be the\n",
        "                # *gold-standard* word at the previous position.\n",
        "                # We create a tensor Tbatch_shifted that contains the previous words.                \n",
        "                batch_size, sen_len = Tbatch.shape\n",
        "                zero_pad = torch.zeros(batch_size, 1, dtype=torch.long, device=Tbatch.device)\n",
        "                Tbatch_shifted = torch.cat([zero_pad, Tbatch[:, :-1]], dim=1)\n",
        "                \n",
        "                self.model.train()\n",
        "                scores = self.model(Sbatch, Tbatch_shifted)\n",
        "\n",
        "                loss = loss_func(scores.view(-1, len(self.T_voc)), Tbatch.view(-1))\n",
        "\n",
        "                optimizer.zero_grad()            \n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                loss_sum += loss.item()\n",
        "\n",
        "                print('.', end='')\n",
        "                sys.stdout.flush()\n",
        "                \n",
        "                # We periodically print some diagnostics: loss, BLEU score on the validation\n",
        "                # set, and the translation of a test sentence.\n",
        "                if i % p.n_batches_print == 0:\n",
        "                    # test_out = ' '.join(self.translate([p.test_sen.split()])[0])\n",
        "                    # bleu = self.eval_bleu(S_valid, T_valid)\n",
        "                    print(f' {i*p.batch_size}')#: loss={loss_sum/i:.4f} BLEU={bleu:.4f} | {p.test_sen} -> {test_out}')\n",
        "                    \n",
        "            print()\n",
        "            t1 = time.time()\n",
        "            \n",
        "            train_loss = loss_sum / len(train_loader)\n",
        "                                    \n",
        "            print(f'Epoch {epoch}: train loss = {train_loss:.4f}, time = {t1-t0:.4f}')\n",
        "\n",
        "            bleu = self.eval_bleu(S_valid, T_valid)\n",
        "            self.history['bleu'].append(bleu)\n",
        "\n",
        "            # Plot history\n",
        "            plt.figure(figsize=(6, 5))\n",
        "            plt.style.use('seaborn')\n",
        "            x = range(1, epoch+1)\n",
        "            # lines\n",
        "            plt.plot(x, self.history['bleu'])\n",
        "            # text\n",
        "            plt.suptitle('Bilingual model', fontsize=20)\n",
        "            plt.ylabel('BLEU', fontsize=14)\n",
        "            plt.xlabel('epoch', fontsize=14)\n",
        "            # save\n",
        "            plt.savefig('fig1.png', dpi=150)\n",
        "            plt.clf()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # Utility function that runs the encoder and decoder for a batch.\n",
        "    # We then map the target-language word indices to the corresponding strings,\n",
        "    # and cut off the sentences after the end-of-sentence marker.\n",
        "    # For visualization, we'll also return all the attention scores.\n",
        "    def translate_batch(self, src):\n",
        "        with torch.no_grad():\n",
        "            encoded = self.S_voc.encode(src)\n",
        "            max_len = max(len(s) for s in encoded)\n",
        "            pad = self.S_voc.get_pad_idx()\n",
        "            for s in encoded:\n",
        "                s.extend([pad]*(max_len-len(s)))\n",
        "            enc_tensor = torch.tensor(encoded, device=self.params.device)\n",
        "            tgt, attn = self.model.greedy_decode(enc_tensor, max_len=2*max_len)\n",
        "            tgt_dec = self.T_voc.decode(tgt[:,1:].cpu().numpy())\n",
        "            for s in tgt_dec:\n",
        "                try:\n",
        "                    eos_ix = s.index(mt_util.EOS)\n",
        "                    del s[eos_ix:]\n",
        "                except:\n",
        "                    pass\n",
        "            return tgt_dec, attn.cpu().numpy()\n",
        "        \n",
        "    # Translates a set of sentences in the source language.\n",
        "    def translate(self, src, batch_size=512):\n",
        "        self.model.eval()\n",
        "        out = []\n",
        "        batch = []\n",
        "        for sen in src:\n",
        "            batch.append(sen)\n",
        "            if len(batch) == batch_size:\n",
        "                translated_batch, _ = self.translate_batch(batch)\n",
        "                out.extend(translated_batch)\n",
        "                batch = []\n",
        "        if len(batch) > 0:\n",
        "            translated_batch, _ = self.translate_batch(batch)\n",
        "            out.extend(translated_batch)\n",
        "        return out\n",
        "         \n",
        "    # Translates a single sentence and returns the translation and attention scores.\n",
        "    def translate_with_attention(self, sentence):\n",
        "        translated, attn = self.translate_batch([sentence])\n",
        "        attn = attn.squeeze(1)\n",
        "        return translated[0], attn\n",
        "        \n",
        "    # Runs the translator, and then uses the SacreBLEU evaluator to compute a BLEU score.\n",
        "    def eval_bleu(self, src, ref):\n",
        "        translated = self.translate(src)\n",
        "        translated = [' '.join(s) for s in translated]\n",
        "        ref = [' '.join(s).lower() for s in ref]\n",
        "        return sacrebleu.raw_corpus_bleu(translated, [ref], 0.01).score\n",
        "        \n"
      ],
      "metadata": {
        "id": "NRbk_l6ukX4m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "translator = Translator(TranslatorParameters())\n",
        "translator.train()"
      ],
      "metadata": {
        "id": "G3i55E4lkqLy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Multilingual model"
      ],
      "metadata": {
        "id": "9SMRPC0Fk1Cx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultilingualModel(nn.Module):\n",
        "    \"\"\"\n",
        "    Most of the code in this model is from the week 5 notebook on neural\n",
        "    machine translation. A mayor difference is that a interlingua representation \n",
        "    has been added between the encoder and decoder, this implementation was done\n",
        "    by following the model desciption described in Yichao Lu et al., however \n",
        "    we are using GRU instead of LSTM.\n",
        "    \"\"\"\n",
        "    def __init__(self, encoder, interlingua, decoder):\n",
        "        super().__init__()\n",
        "        self.encoder = encoder\n",
        "        self.interlingua = interlingua\n",
        "        self.decoder = decoder\n",
        "        \n",
        "    def forward(self, S, T_shifted):        \n",
        "        \"\"\" a normal forward pass with teacher enforcing \"\"\"\n",
        "        # encoding the data\n",
        "        S_enc = self.encoder(S)\n",
        "\n",
        "        # mask that contains information on the positions of the padding tokens\n",
        "        S_mask = (S != 0)\n",
        "\n",
        "        # computing interlingua representation\n",
        "        S_interlingua = self.interlingua(S_enc, S_mask)\n",
        "\n",
        "        # returning the decoded sentance as output\n",
        "        return self.decoder(S_interlingua, S_mask, T_shifted)\n",
        "    \n",
        "    def greedy_decode(self, S, max_len):\n",
        "        \"\"\" when predicting greedily we will not use teacher enforcing \"\"\"\n",
        "        S_enc = self.encoder(S)    \n",
        "        S_mask = (S != 0)\n",
        "        S_interlingua = self.interlingua(S_enc, S_mask)\n",
        "        return self.decoder.greedy_decode(S_interlingua, S_mask, max_len)\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, emb_layer, param):\n",
        "        super().__init__()\n",
        "        _, emb_dim = emb_layer.weight.shape       \n",
        "        self.emb_layer = emb_layer\n",
        "        self.rnn = nn.GRU(emb_dim, param.enc_rnn_dim, param.enc_rnn_depth, \n",
        "                          batch_first=True, bidirectional=True)\n",
        "    def forward(self, S):\n",
        "        embedded = self.emb_layer(S)\n",
        "        output, _ = self.rnn(embedded)\n",
        "        return output\n",
        "\n",
        "class Interlingua(nn.Module):\n",
        "\n",
        "    def __init__(self, param):\n",
        "        super().__init__()\n",
        "\n",
        "        self.rnn_dim = param.inter_rnn_dim\n",
        "        self.max_sent_len = param.max_sent_len\n",
        "\n",
        "        attn_dim = 2*param.enc_rnn_dim # twice the size since encoder is bidirectional\n",
        "        self.rnn = nn.GRU(attn_dim, param.inter_rnn_dim, param.inter_rnn_depth,\n",
        "                          batch_first=True, bidirectional=False)\n",
        "        self.attention = BahdanauAttention(attn_dim, param.inter_rnn_dim, param.hidden_dim)\n",
        "        self.output_layer = nn.Linear(param.inter_rnn_dim, param.inter_rnn_dim)\n",
        "\n",
        "\n",
        "    def forward_step(self, inter_out, src_mask, precomputed_key, inter_hidden):        \n",
        "\n",
        "        # The \"query\" for attention is the hidden state of the decoder.\n",
        "        query = inter_hidden[-1].unsqueeze(1)  \n",
        "        \n",
        "        # Compute attention\n",
        "        context, attn_probs = self.attention(\n",
        "            query=query, precomputed_key=precomputed_key,\n",
        "            value=inter_out, mask=src_mask)\n",
        "        \n",
        "        # Run the RNN one step.\n",
        "        rnn_output, inter_hidden = self.rnn(context, inter_hidden)\n",
        "        output = self.output_layer(rnn_output)\n",
        "        \n",
        "        return attn_probs, inter_hidden, output\n",
        "\n",
        "    def forward(self, enc_out, src_mask):\n",
        "\n",
        "        # n_sen, n_words = T_shifted.shape\n",
        "        n_sen = enc_out.shape[0]\n",
        "        \n",
        "        # Precompute attention keys \n",
        "        precomputed_key = self.attention.precompute_key(enc_out)\n",
        "        \n",
        "        # Initialize the hidden state of the GRU.\n",
        "        inter_hidden = torch.zeros(1, n_sen, self.rnn_dim, device=src_mask.device)\n",
        "        \n",
        "        all_out = []\n",
        "        for _ in range(self.max_sent_len*2): # for each interlingua representation\n",
        "            \n",
        "            # Run the decoder one step.\n",
        "            # This returns a new hidden state, and the output \n",
        "            # scores (over the target vocabulary) at this position.\n",
        "            _, inter_hidden, output = self.forward_step(enc_out, src_mask, precomputed_key, inter_hidden)\n",
        "            all_out.append(output)\n",
        " \n",
        "        # Combine the output scores for all positions.\n",
        "        return torch.cat(all_out, dim=1)\n",
        "            \n",
        "\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "\n",
        "    def __init__(self, emb_layer, param):\n",
        "        super().__init__()\n",
        "        voc_size, emb_dim = emb_layer.weight.shape\n",
        "\n",
        "        # Word embeddings for the target language. \n",
        "        self.emb_layer = emb_layer\n",
        "\n",
        "        self.rnn_dim = param.dec_rnn_dim\n",
        "\n",
        "        # The decoder's state is represented using a GRU, called f in Bahdanau's paper.\n",
        "        # The input of this GRU is a combination of the embedding of the previous word\n",
        "        # and the \"context\" computed by the attention model.\n",
        "        self.rnn = nn.GRU(emb_dim + param.dec_rnn_dim, param.dec_rnn_dim, param.dec_rnn_depth, \n",
        "                          batch_first=True, bidirectional=False)\n",
        "        \n",
        "        # The output layer (called g in the paper) is a bit simpler than in the paper. \n",
        "        # We just use a linear model.\n",
        "        self.output_layer = nn.Linear(param.dec_rnn_dim * 2 + emb_dim,\n",
        "                                      voc_size, bias=False)\n",
        "                \n",
        "\n",
        "        # The attention model, which computes a context or \"summary\" of the source sentence.\n",
        "        # Right now, we use a simplified attention model that doesn't depend on the decoder state.\n",
        "        # See below for the implementation.\n",
        "        self.attention = BahdanauAttention(param.inter_rnn_dim, param.enc_rnn_dim, param.hidden_dim)\n",
        "\n",
        "        \n",
        "    # This method applies the attention model, the RNN and the output model at one\n",
        "    # position in the decoded sentence.\n",
        "    def forward_step(self, prev_embed, inter_out, precomputed_key, decoder_hidden):        \n",
        "\n",
        "        # The \"query\" for attention is the hidden state of the decoder.\n",
        "        query = decoder_hidden[-1].unsqueeze(1)  \n",
        "        \n",
        "        # Apply the attention model to compute a \"context\", summary of the encoder output\n",
        "        # based on the current query.\n",
        "        # Also returns the attention weights, which we might want to visualize or inspect.\n",
        "        context, attn_probs = self.attention(\n",
        "            query=query, precomputed_key=precomputed_key,\n",
        "            value=inter_out, mask=None)\n",
        "        \n",
        "        # Run the RNN one step.\n",
        "        rnn_input = torch.cat([prev_embed, context], dim=2)\n",
        "        rnn_output, decoder_hidden = self.rnn(rnn_input, decoder_hidden)\n",
        "        \n",
        "        # The word output model (called g in Bahdanau's paper) uses the embedding \n",
        "        # of the previous word, the RNN output, and the context computed by the attention model.\n",
        "        pre_output = torch.cat([prev_embed, rnn_output, context], dim=2)\n",
        "        T_output = self.output_layer(pre_output)\n",
        "\n",
        "        return attn_probs, decoder_hidden, T_output\n",
        "        \n",
        "        \n",
        "    # Apply the decoder in teacher forcing mode.\n",
        "    def forward(self, inter_out, src_mask, T_shifted):\n",
        "\n",
        "        n_sen, n_words = T_shifted.shape\n",
        "        \n",
        "        # Word embeddings for the shifted word, representing the previous step.\n",
        "        T_emb = self.emb_layer(T_shifted)\n",
        "        \n",
        "        # Precompute attention keys (if needed).\n",
        "        precomputed_key = self.attention.precompute_key(inter_out)\n",
        "        \n",
        "        # Initialize the hidden state of the GRU.\n",
        "        decoder_hidden = torch.zeros(1, n_sen, self.rnn_dim, device=src_mask.device)\n",
        "        \n",
        "        all_out = []\n",
        "        \n",
        "        # For each position in the target sentence:\n",
        "        for i in range(n_words):\n",
        "            \n",
        "            # Embedding for the previous word.\n",
        "            prev_embed = T_emb[:, i].unsqueeze(1)\n",
        "            \n",
        "            # Run the decoder one step.\n",
        "            # This returns a new hidden state, and the output \n",
        "            # scores (over the target vocabulary) at this position.\n",
        "            _, decoder_hidden, T_output = self.forward_step(prev_embed, inter_out, \n",
        "                                                            precomputed_key, decoder_hidden)\n",
        "            all_out.append(T_output)\n",
        " \n",
        "        # Combine the output scores for all positions.\n",
        "        return torch.cat(all_out, dim=1)\n",
        "            \n",
        "    # Apply the decoder in a greedy step-by-step fashion, at each step selecting the\n",
        "    # highest-scoring word.\n",
        "    def greedy_decode(self, enc_out, src_mask, max_len):\n",
        "        n_sen, _ = src_mask.shape\n",
        "\n",
        "        precomputed_key = self.attention.precompute_key(enc_out)\n",
        "        decoder_hidden = torch.zeros(1, n_sen, self.rnn_dim, device=src_mask.device)\n",
        "        \n",
        "        prev_tokens = torch.zeros(src_mask.size(0), 1, dtype=torch.long, \n",
        "                                  device=src_mask.device)\n",
        "        all_out = []\n",
        "        all_attn = []\n",
        "\n",
        "        for i in range(max_len):\n",
        "            prev_embed = self.emb_layer(prev_tokens)\n",
        "            attn_probs, decoder_hidden, T_output = self.forward_step(prev_embed, enc_out, \n",
        "                                                                     precomputed_key, decoder_hidden)\n",
        "            # Select the highest-scoring word.\n",
        "            prev_tokens = T_output.argmax(-1)\n",
        "            all_out.append(prev_tokens)\n",
        "            all_attn.append(attn_probs)\n",
        "        \n",
        "        return torch.cat(all_out, dim=1), torch.stack(all_attn)\n",
        "\n"
      ],
      "metadata": {
        "id": "xKXn6-bjk3q-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Parameters and training loop"
      ],
      "metadata": {
        "id": "-tFfkCFKlT98"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TranslatorParameters_milti:\n",
        "    device = 'cuda'\n",
        "    \n",
        "    n_batches_print = 32\n",
        "    \n",
        "    src_voc_size = 10000\n",
        "    tgt_voc_size = 10000\n",
        "    \n",
        "    random_seed = 0\n",
        "            \n",
        "    n_epochs = 100\n",
        "    \n",
        "    batch_size = 64\n",
        "\n",
        "    hidden_dim = 64# hidden dim used when computing attention\n",
        "\n",
        "    max_sent_len = 25\n",
        "    \n",
        "    learning_rate = 2e-4\n",
        "    weight_decay = 0\n",
        "        \n",
        "    emb_dim = 128\n",
        "    \n",
        "    enc_rnn_dim = 128\n",
        "    enc_rnn_depth = 2\n",
        "\n",
        "    inter_rnn_dim = 128\n",
        "    inter_rnn_depth = 1\n",
        "\n",
        "    dec_rnn_dim = 128\n",
        "    dec_rnn_depth = 1\n",
        "    \n",
        "    max_train_sentences = 200000\n",
        "    max_train_sentences_identity = 10000\n",
        "    max_valid_sentences = 1000\n",
        "\n",
        "    n_languages = 3\n",
        "\n",
        "    # Training datasets\n",
        "    de_en = {'src_train': './data/de-en-train.de',\n",
        "             'tgt_train': './data/de-en-train.en',\n",
        "             'src_valid': './data/de-en-val.de',\n",
        "             'tgt_valid': './data/de-en-val.en',\n",
        "             'name': 'de_en'}\n",
        "\n",
        "    en_de = {'src_train': './data/de-en-train.en',\n",
        "             'tgt_train': './data/de-en-train.de',\n",
        "             'src_valid': './data/de-en-val.en',\n",
        "             'tgt_valid': './data/de-en-val.de',\n",
        "             'name': 'en_de'}\n",
        "\n",
        "    sv_de = {'src_train': './data/de-sv-train.sv',\n",
        "             'tgt_train': './data/de-sv-train.de',\n",
        "             'src_valid': './data/de-sv-val.sv',\n",
        "             'tgt_valid': './data/de-sv-val.de',\n",
        "             'name': 'sv_de'}\n",
        "\n",
        "    de_sv = {'src_train': './data/de-sv-train.de',\n",
        "             'tgt_train': './data/de-sv-train.sv',\n",
        "             'src_valid': './data/de-sv-val.de',\n",
        "             'tgt_valid': './data/de-sv-val.sv',\n",
        "             'name': 'de_sv'}\n",
        "\n",
        "    en = {'src_train': './data/de-en-train.en',\n",
        "          'tgt_train': './data/de-en-train.en',\n",
        "          'src_valid': './data/de-en-val.en',\n",
        "          'tgt_valid': './data/de-en-val.en',\n",
        "          'name': 'en'}\n",
        "\n",
        "    de = {'src_train': './data/de-sv-train.de',\n",
        "          'tgt_train': './data/de-sv-train.de',\n",
        "          'src_valid': './data/de-sv-val.de',\n",
        "          'tgt_valid': './data/de-sv-val.de',\n",
        "          'name': 'de'}\n",
        "\n",
        "    sv = {'src_train': './data/de-sv-train.sv',\n",
        "          'tgt_train': './data/de-sv-train.sv',\n",
        "          'src_valid': './data/de-sv-val.sv',\n",
        "          'tgt_valid': './data/de-sv-val.sv',\n",
        "          'name': 'sv'}\n",
        "\n",
        "    # zero-shot translation\n",
        "    sv_en = {'src_train': './data/en-sv-train.sv',\n",
        "             'tgt_train': './data/en-sv-train.en',\n",
        "             'src_valid': './data/en-sv-val.sv',\n",
        "             'tgt_valid': './data/en-sv-val.en',\n",
        "             'name': 'en_sv'}\n",
        "\n",
        "    datafiles = [en, de, sv, en_de, de_en, de_sv, sv_de, sv_en]\n",
        "    \n",
        "\n",
        "\n",
        "    \n",
        "class Translator_multi:\n",
        "    \n",
        "    def __init__(self, params):\n",
        "        self.params = params\n",
        "        \n",
        "    def train(self):\n",
        "        \n",
        "        p = self.params\n",
        "        \n",
        "        # Setting a fixed seed for reproducibility.\n",
        "        torch.manual_seed(p.random_seed)\n",
        "        random.seed(p.random_seed)\n",
        "        \n",
        "        print('Preparing data...', end='')\n",
        "        sys.stdout.flush()\n",
        "\n",
        "        self.dataloaders = []\n",
        "        self.model_names = []\n",
        "        self.S_vocs = []\n",
        "        self.T_vocs = []\n",
        "        self.S_valids = []\n",
        "        self.T_valids = []\n",
        "\n",
        "        for ind, dataset in enumerate(p.datafiles):\n",
        "            self.model_names.append(dataset['name'])\n",
        "            # Read the source-language data.\n",
        "            if ind < 3:\n",
        "                max_train_sentences = p.max_train_sentences_identity\n",
        "            else:\n",
        "                max_train_sentences = p.max_train_sentences\n",
        "\n",
        "            S_train = mt_util.read_lines(dataset['src_train'], max_lines=max_train_sentences)\n",
        "            S_voc = mt_util.Vocabulary(include_unknown=True, lower=True, max_voc_size=p.src_voc_size)\n",
        "            S_voc.build(S_train)\n",
        "            S_valid = mt_util.read_lines(dataset['src_valid'], max_lines=p.max_valid_sentences)\n",
        "\n",
        "            # Read the target-language data.\n",
        "            T_train = mt_util.read_lines(dataset['tgt_train'], max_lines=max_train_sentences)\n",
        "            T_voc = mt_util.Vocabulary(include_unknown=True, lower=True, max_voc_size=p.tgt_voc_size)\n",
        "            T_voc.build(T_train)\n",
        "            T_valid = mt_util.read_lines(dataset['tgt_valid'], max_lines=p.max_valid_sentences)\n",
        "        \n",
        "            # Batching for the training and validation sets.\n",
        "            batcher = mt_util.SequenceBatcher(p.device)        \n",
        "            train_dataset = mt_util.SequenceDataset(S_voc.encode(S_train), T_voc.encode(T_train))\n",
        "            train_loader = DataLoader(train_dataset, p.batch_size, shuffle=True, collate_fn=batcher)\n",
        "\n",
        "            self.dataloaders.append(train_loader)\n",
        "            self.S_vocs.append(S_voc)\n",
        "            self.T_vocs.append(T_voc)\n",
        "            self.S_valids.append(S_valid)\n",
        "            self.T_valids.append(T_valid)\n",
        "\n",
        "            \n",
        "        print(' done.')\n",
        "        \n",
        "        print('Building model...', end='')\n",
        "        sys.stdout.flush() \n",
        "\n",
        "        # here we will create all three encoder-decoder pairs, the following indecies will be used:\n",
        "        # 0: en\n",
        "        # 1: de\n",
        "        # 2: sv\n",
        "        self.encoders = []\n",
        "        self.decoders = []\n",
        "        for it in range(p.n_languages):\n",
        "\n",
        "            S_emb = self.S_vocs[it].make_embedding_layer(finetune=True, emb_dim=p.emb_dim)\n",
        "            T_emb = self.T_vocs[it].make_embedding_layer(finetune=True, emb_dim=p.emb_dim)\n",
        "        \n",
        "            # Build the encoder and decoder.\n",
        "            self.encoders.append(Encoder(S_emb, p))\n",
        "            self.decoders.append(Decoder(T_emb, p))\n",
        "        \n",
        "        inter = Interlingua(p)\n",
        "\n",
        "        self.models = []\n",
        "        # en-de\n",
        "        self.models.append(MultilingualModel(self.encoders[0], inter, self.decoders[0]))\n",
        "        # de-en\n",
        "        self.models.append(MultilingualModel(self.encoders[1], inter, self.decoders[1]))\n",
        "        # sv-de\n",
        "        self.models.append(MultilingualModel(self.encoders[2], inter, self.decoders[2]))\n",
        "        # de-sv\n",
        "        self.models.append(MultilingualModel(self.encoders[0], inter, self.decoders[1]))\n",
        "        # en-en\n",
        "        self.models.append(MultilingualModel(self.encoders[1], inter, self.decoders[0]))\n",
        "        # de-de\n",
        "        self.models.append(MultilingualModel(self.encoders[1], inter, self.decoders[2]))\n",
        "        # sv-sv\n",
        "        self.models.append(MultilingualModel(self.encoders[2], inter, self.decoders[1]))\n",
        "        # en-sv (zero-shot model, won't be trained directly)\n",
        "        self.models.append(MultilingualModel(self.encoders[2], inter, self.decoders[0]))\n",
        "        print(' done.')\n",
        "                \n",
        "\n",
        "        self.history = defaultdict(list)\n",
        "        best_bleu = 0\n",
        "        for epoch in range(1, p.n_epochs+1):\n",
        "\n",
        "            for ind, model in enumerate(self.models[:-1]):\n",
        "                self.ind = ind\n",
        "                \n",
        "                self.model = model\n",
        "                # print(id(model.encoder))\n",
        "                # print(id(model.interlingua))\n",
        "                # print(id(model.decoder))\n",
        "                self.model.to(p.device)\n",
        "                optimizer = torch.optim.Adam(self.model.parameters(), \n",
        "                                             lr=p.learning_rate, weight_decay=p.weight_decay)\n",
        "\n",
        "                # The loss function is a cross-entropy loss at the token level.\n",
        "                # We don't include padding tokens when computing the loss.\n",
        "                loss_func = torch.nn.CrossEntropyLoss(ignore_index=self.T_vocs[ind].get_pad_idx())\n",
        "\n",
        "                t0 = time.time()\n",
        "\n",
        "                loss_sum = 0\n",
        "                for i, (Sbatch, Tbatch) in enumerate(self.dataloaders[ind], 1):\n",
        "                    \n",
        "                    # We use teacher forcing to train the decoder.\n",
        "                    # This means that the input at each decoding step will be the\n",
        "                    # *gold-standard* word at the previous position.\n",
        "                    # We create a tensor Tbatch_shifted that contains the previous words.                \n",
        "                    batch_size, sen_len = Tbatch.shape\n",
        "                    zero_pad = torch.zeros(batch_size, 1, dtype=torch.long, device=Tbatch.device)\n",
        "                    Tbatch_shifted = torch.cat([zero_pad, Tbatch[:, :-1]], dim=1)\n",
        "                    \n",
        "                    self.model.train()\n",
        "                    scores = self.model(Sbatch, Tbatch_shifted)\n",
        "\n",
        "                    loss = loss_func(scores.view(-1, len(self.T_vocs[ind])), Tbatch.view(-1))\n",
        "\n",
        "                    optimizer.zero_grad()            \n",
        "                    loss.backward()\n",
        "                    optimizer.step()\n",
        "                    loss_sum += loss.item()\n",
        "\n",
        "                    print('.', end='')\n",
        "                    sys.stdout.flush()\n",
        "                    \n",
        "                    # We periodically print some diagnostics: loss, BLEU score on the validation\n",
        "                    # set, and the translation of a test sentence.\n",
        "                    if i % p.n_batches_print == 0:\n",
        "                        print(f' {i*p.batch_size}') \n",
        "                        \n",
        "                t1 = time.time()\n",
        "                \n",
        "                # train_loss = loss_sum / len(self.dataloaders[ind])\n",
        "                bleu = self.eval_bleu(self.S_valids[ind], self.T_valids[ind])\n",
        "\n",
        "                self.history[self.model_names[ind]].append(bleu)\n",
        "                print(f'Epoch {epoch}, model {self.model_names[ind]}: time={t1-t0:.4f}, BLEU={bleu}\\n')\n",
        "\n",
        "                self.model.to('cpu')\n",
        "\n",
        "            # load sv-en model for evaluation\n",
        "            ind = -1\n",
        "            self.model = self.models[ind]\n",
        "            self.model.to(p.device)\n",
        "            self.ind = ind\n",
        "\n",
        "            # evaluate model, store in history and print epoch performance\n",
        "            bleu = self.eval_bleu(self.S_valids[ind], self.T_valids[ind])\n",
        "            self.history[self.model_names[ind]].append(bleu)\n",
        "            print(f'Epoch {epoch} results for {self.model_names[ind]}:\\n\\t BLEU={bleu:.4f}\\n')\n",
        "            self.model.to('cpu')\n",
        "        \n",
        "            # Plot history\n",
        "            plt.figure(figsize=(6, 5))\n",
        "            plt.style.use('seaborn')\n",
        "            x = range(1, epoch+1)\n",
        "            # lines\n",
        "            for mn in self.model_names:\n",
        "                plt.plot(x, self.history[mn], label=mn)\n",
        "            plt.legend()\n",
        "            # text\n",
        "            plt.suptitle('Multilingual model', fontsize=20)\n",
        "            plt.ylabel('BLEU', fontsize=14)\n",
        "            plt.xlabel('epoch', fontsize=14)\n",
        "            # save\n",
        "            plt.savefig('fig.png', dpi=150)\n",
        "            plt.clf()\n",
        "\n",
        "            if bleu > best_bleu:\n",
        "                names = ['en', 'de', 'sv']\n",
        "                for encoder, decoder, n in zip(self.encoders, self.decoders, names):\n",
        "                    torch.save(encoder.state_dict(), f\"encoder.{n}\")\n",
        "                    torch.save(decoder.state_dict(), f\"decoder.{n}\")\n",
        "                torch.save(inter.state_dict(), f\"inter\")\n",
        "\n",
        "    # Utility function that runs the encoder and decoder for a batch.\n",
        "    # We then map the target-language word indices to the corresponding strings,\n",
        "    # and cut off the sentences after the end-of-sentence marker.\n",
        "    # For visualization, we'll also return all the attention scores.\n",
        "    def translate_batch(self, src):\n",
        "        with torch.no_grad():\n",
        "            encoded = self.S_vocs[self.ind].encode(src)\n",
        "            max_len = max(len(s) for s in encoded)\n",
        "            pad = self.S_vocs[self.ind].get_pad_idx()\n",
        "            for s in encoded:\n",
        "                s.extend([pad]*(max_len-len(s)))\n",
        "            enc_tensor = torch.tensor(encoded, device=self.params.device)\n",
        "            tgt, attn = self.model.greedy_decode(enc_tensor, max_len=2*max_len)\n",
        "            tgt_dec = self.T_vocs[self.ind].decode(tgt[:,1:].cpu().numpy())\n",
        "            for s in tgt_dec:\n",
        "                try:\n",
        "                    eos_ix = s.index(mt_util.EOS)\n",
        "                    del s[eos_ix:]\n",
        "                except:\n",
        "                    pass\n",
        "            return tgt_dec, attn.cpu().numpy()\n",
        "        \n",
        "    # Translates a set of sentences in the source language.\n",
        "    def translate(self, src, batch_size=512):\n",
        "        self.model.eval()\n",
        "        out = []\n",
        "        batch = []\n",
        "        for sen in src:\n",
        "            batch.append(sen)\n",
        "            if len(batch) == batch_size:\n",
        "                translated_batch, _ = self.translate_batch(batch)\n",
        "                out.extend(translated_batch)\n",
        "                batch = []\n",
        "        if len(batch) > 0:\n",
        "            translated_batch, _ = self.translate_batch(batch)\n",
        "            out.extend(translated_batch)\n",
        "        return out\n",
        "         \n",
        "    # Translates a single sentence and returns the translation and attention scores.\n",
        "    def translate_with_attention(self, sentence):\n",
        "        translated, attn = self.translate_batch([sentence])\n",
        "        attn = attn.squeeze(1)\n",
        "        return translated[0], attn\n",
        "        \n",
        "    # Runs the translator, and then uses the SacreBLEU evaluator to compute a BLEU score.\n",
        "    def eval_bleu(self, src, ref):\n",
        "        translated = self.translate(src)\n",
        "        translated = [' '.join(s) for s in translated]\n",
        "        ref = [' '.join(s).lower() for s in ref]\n",
        "        return sacrebleu.raw_corpus_bleu(translated, [ref], 0.01).score\n",
        "        \n"
      ],
      "metadata": {
        "id": "OAb5SjcglODy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "translator_multi = Translator_multi(TranslatorParameters_multi())\n",
        "translator_multi.train()\n"
      ],
      "metadata": {
        "id": "8ni9ZT__lbAu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}